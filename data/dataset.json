[
    {
        "id": 1,
        "domain": "Abstract Algebra",
        "informal": "We prove that if $H\\preceq G$ and $K\\preceq G$, then $HK\\preceq G\\iff HK=KH$. \n\nFor the direction ($\\Rightarrow$): Since $HK$ is a subgroup, by the equivalent definition we have $(HK)^{-1}=HK$. But $(HK)^{-1}=K^{-1}H^{-1}$, and since $H$ and $K$ are subgroups, this equals $KH$. Hence, $HK=KH$.\n\nFor the direction ($\\Leftarrow$): $(HK)^{-1}=K^{-1}H^{-1}=KH=HK$. Also, $(HK)(HK)=H(KH)K=H(HK)K=(HH)(KK)=HK$. Therefore, $HK\\preceq G$.",
        "structure": []
    },
    {
        "id": 1,
        "domain": "Analysis",
        "informal": "**Theorem**\n\nLet ⟨$a_n(z)$⟩ and ⟨$b_n(z)$⟩ be sequences of complex functions on a compact set $K$. Let ⟨$a_n(z)$⟩ be such that:\n\n- ⟨$a_n(z)$⟩ is bounded in $K$\n\n- $\\sum |a_n(z) - a_{n+1}(z)|$ is convergent with a sum which is bounded in $K$\n\n- $\\sum b_n(z)$ is uniformly convergent in $K$.\n\nThen $\\sum a_n(z) b_n(z)$ is uniformly convergent on $K$.\n\n---\n\nFirst we modify the statement of Abel's Lemma\n\n**Lemma**\n\nSuppose $\\sum b_k$ converges. Let $B_k = b_k + b_{k+1} + b_{k+2} + \\cdots$\n\nThen:\n\n$a_n b_n + \\cdots + a_{n+k} b_{n+k} = B_n a_n + B_{n+1} (a_{n+1} - a_n) + \\cdots + B_{n+k} (a_{n+k} - a_{n+k-1}) - B_{n+k+1} a_{n+k}$\n\n**Proof of lemma**\n\n$a_n b_n + \\cdots + a_{n+k} b_{n+k} = a_n (B_n - B_{n+1}) + \\cdots + a_{n+k} (B_{n+k} - B_{n+k+1})$\n\n$= B_n a_n + B_{n+1} (a_{n+1} - a_n) + \\cdots + B_{n+k} (a_{n+k} - a_{n+k-1}) - B_{n+k+1} a_{n+k}$\n\n□\n\n**Proof of theorem**\n\nWe show that $\\sum_{j=n}^{n+k} a_j(z) b_j(z)$ is uniformly small if $n$ is large enough.\n\nUsing the notation of the lemma, since $B_n(z) \\to 0$ uniformly, let $n$ be so large that $|B_N(z)| \\leq \\epsilon$ in $K$ for all $N \\geq n$.\n\nSince ⟨$a_n(z)$⟩ is uniformly bounded in $K$, let $|a_n(z)| \\leq M$ for all $z \\in K$.\n\nSince $\\sum |a_n(z) - a_{n+1}(z)|$ is convergent with a sum which is bounded in $K$, let $n$ be so large that:\n\n$\\forall k \\in \\mathbb{N} : |a_{n+1} - a_n| + \\cdots + |a_{n+k} - a_{n+k-1}| \\leq \\epsilon$ for all $z \\in K$\n\nthen we have:\n\n$\\forall z \\in K : |B_{n+1} (a_{n+1} - a_n)| + \\cdots + |B_{n+k} (a_{n+k} - a_{n+k-1})| \\leq M \\epsilon$\n\nTherefore:\n\n$|a_n b_n + \\cdots + a_{n+k} b_{n+k}| = |B_n a_n + B_{n+1} (a_{n+1} - a_n) + \\cdots + B_{n+k} (a_{n+k} - a_{n+k-1}) + B_{n+k+1} a_{n+k}|$ (Lemma)\n\n$\\leq |B_n a_n| + |B_{n+1} (a_{n+1} - a_n)| + \\cdots + |B_{n+k} (a_{n+k} - a_{n+k-1})| + |B_{n+k+1} a_{n+k}|$ (Triangle Inequality for Complex Numbers)                            \n\n$\\leq M \\epsilon + M \\epsilon$\n\n$= 3M \\epsilon$\n\nTherefore, $\\sum a_n(z) b_n(z)$ is uniformly convergent on $K$. \n\n■\n",
        "structure": []
    },
    {
        "id": 1,
        "domain": "High School",
        "informal": "For a sequence of positive real numbers $\\{x_n\\}$ and any positive integer $n$, prove that:\n$$\n\\prod_{i=1}^{n}(1 + x_i) \\geq 1 + \\sum_{i=1}^{n} x_i.\n$$\n\n**Proof:**\n\nWe prove this by mathematical induction.\n\n**Base case:** When $n = 1$, we have $1+x_1 = 1+x_1$, so the inequality holds.\n\n**Inductive step:** Assume that for some positive integer $k$, the inequality holds:\n$$\n\\prod_{i=1}^{k}(1 + x_i) \\geq 1 + \\sum_{i=1}^{k} x_i.\n$$\nWe now prove it for $k+1$. Since $x_i > 0$ for all $1 \\leq i \\leq n$, we have $1 + x_i > 0$. Therefore,\n$$\n\\prod_{i=1}^{k+1}(1 + x_i) = \\left( \\prod_{i=1}^{k}(1 + x_i) \\right)(1 + x_{k+1}) \\geq \\left(1 + \\sum_{i=1}^{k} x_i\\right)(1 + x_{k+1}).\n$$\nExpanding the right-hand side:\n$$\n\\left(1 + \\sum_{i=1}^{k} x_i\\right)(1 + x_{k+1}) = 1 + \\sum_{i=1}^{k} x_i + x_{k+1} + \\sum_{i=1}^{k} x_i x_{k+1} = 1 + \\sum_{i=1}^{k+1} x_i + \\sum_{i=1}^{k} x_i x_{k+1}.\n$$\nSince $x_i x_{k+1} \\geq 0$ for all $i$, we have:\n$$\n\\prod_{i=1}^{k+1}(1 + x_i) \\geq 1 + \\sum_{i=1}^{k+1} x_i.\n$$\nThis completes the inductive step.\n\nTherefore, for any positive integer $n$, we have:\n$$\n\\prod_{i=1}^{n}(1 + x_i) \\geq 1 + \\sum_{i=1}^{n} x_i.\n$$\n\n**Q.E.D.**",
        "structure": []
    },
    {
        "id": 1,
        "domain": "IMO",
        "informal": "## Goal\n\nLet $(a_{n})_{n\\geqslant1}$ be a sequence of positive real numbers with the property that $(a_{n+1})^2+a_na_{n+2}\\leqslant a_n+a_{n+2}$ for all positive integers $n$. Prove that $a_{2022}\\leqslant1$.\n\n## Proof\n\nWe begin by observing that $(a_{n+1})^{2}-1\\leqslant a_{n}+a_{n+2}-a_{n}a_{n+2}-1$, which is equivalent to $$(a_{n+1})^2-1\\leqslant(1-a_n)(a_{n+2}-1).$$ Suppose now that there exists a positive integer $n$ such that $a_{n+1}>1$ and $a_{n+2}>1$. Since $( a_{n+ 1}) ^{2}- 1$ $\\leqslant$ $( 1- a_{n}) ( a_{n+ 2}- 1)$ ,we deduce that $0 < 1- a_{n} < 1 < 1 + a_{n+ 2}$ ，thus $(a_{n+1})^{2}-1<(a_{n+2}+1)(a_{n+2}-1)=(a_{n+2})^{2}-1$.\n\nOn the other hand, $(a_{n+2})^{2}-1\\leqslant(1-a_{n+3})(a_{n+1}-1)$$<(1+a_{n+1})(a_{n+1}-1)=(a_{n+1})^{2}-1$, a contradiction. We have shown that we cannot have two consecutive terms,except maybe $a_1$ and $a_2$ ,strictly greater than 1.\n\nFinally, suppose $a_{2022}>1$. This implies that $a_{2021}\\leqslant1$ and $a_{2023}\\leqslant1$ . Therefore $0<(a_{2022})^{2}-1\\leqslant(1-a_{2021})(a_{2023}-1)\\leqslant0$, a contradiction. We conclude that $a_{2022}\\leqslant 1$.\n\n$Qed.$",
        "structure": []
    },
    {
        "id": 1,
        "domain": "Linear Algebra",
        "informal": "2.6 *span is the smallest containing subspace*\n\nThe span of a list of vectors in $V$ is the smallest subspace of $V$ containing all vectors in the list.\n\n**Proof** Suppose $v_1, ..., v_m$ is a list of vectors in $V$.  \nFirst we show that $\\operatorname{span}(v_1, ..., v_m)$ is a subspace of $V$. The additive identity is in $\\operatorname{span}(v_1, ..., v_m)$ because\n\n$$0 = 0v_1 + \\cdots + 0v_m.$$\n\nAlso, $\\operatorname{span}(v_1, ..., v_m)$ is closed under addition because\n\n$$(a_1 v_1 + \\cdots + a_m v_m) + (c_1 v_1 + \\cdots + c_m v_m) = (a_1 + c_1) v_1 + \\cdots + (a_m + c_m) v_m.$$\n\nFurthermore, $\\operatorname{span}(v_1, ..., v_m)$ is closed under scalar multiplication because\n\n$$\\lambda (a_1 v_1 + \\cdots + a_m v_m) = \\lambda a_1 v_1 + \\cdots + \\lambda a_m v_m.$$\n\nThus $\\operatorname{span}(v_1, ..., v_m)$ is a subspace of $V$ (by 1.34).  \nEach $v_k$ is a linear combination of $v_1, ..., v_m$ (to show this, set $a_k = 1$ and let the other $a$'s in 2.2 equal 0). Thus $\\operatorname{span}(v_1, ..., v_m)$ contains each $v_k$. Conversely, because subspaces are closed under scalar multiplication and addition, every subspace of $V$ that contains each $v_k$ contains $\\operatorname{span}(v_1, ..., v_m)$. Thus $\\operatorname{span}(v_1, ..., v_m)$ is the smallest subspace of $V$ containing all the vectors $v_1, ..., v_m$.\n",
        "structure": []
    },
    {
        "id": 1,
        "domain": "Logic",
        "informal": "4.6 Coincidence Lemma. Let $\\mathfrak{I}_1 = (\\mathcal{A}_1, \\beta_1)$ be an $S_1$-interpretation and $\\mathfrak{I}_2 = (\\mathcal{A}_2, \\beta_2)$ be an $S_2$-interpretation, both with the same domain, i.e. $A_1 = A_2$. Put $S := S_1 \\cap S_2$.\n\n(a) Let $t$ be an $S$-term. If $\\mathfrak{I}_1$ and $\\mathfrak{I}_2$ agree on the $S$-symbols occurring in $t$ and on the variables occurring in $t$, then $\\mathfrak{I}_1(t) = \\mathfrak{I}_2(t)$. (Note: $\\mathfrak{I}_1$ and $\\mathfrak{I}_2$ agree on $k \\in S$ or on $x$ if $k^{\\mathcal{A}_1} = k^{\\mathcal{A}_2}$ or $\\beta_1(x) = \\beta_2(x)$, respectively.)\n\n(b) Let $\\varphi$ be an $S$-formula. If $\\mathfrak{I}_1$ and $\\mathfrak{I}_2$ agree on the $S$-symbols and on the variables occurring free in $\\varphi$, then $\\mathfrak{I}_1 \\models \\varphi$ iff $\\mathfrak{I}_2 \\models \\varphi$.\n\n**Proof.** (a) We use induction on $S$-terms.\n\n$t = x$: By hypothesis, $\\beta_1(x) = \\beta_2(x)$ and therefore\n\n$$\\mathfrak{I}_1(x) = \\beta_1(x) = \\beta_2(x) = \\mathfrak{I}_2(x).$$\n\n$t = c$: Similarly.\n\n$t = f t_1 \\ldots t_n$ ($f \\in S$ $n$-ary and $t_1, \\ldots, t_n \\in T^S$):\n\n$$\\mathfrak{I}_1(f t_1 \\ldots t_n) = f^{\\mathcal{A}_1}(\\mathfrak{I}_1(t_1), \\ldots, \\mathfrak{I}_1(t_n))$$\n\n$$= f^{\\mathcal{A}_1}(\\mathfrak{I}_2(t_1), \\ldots, \\mathfrak{I}_2(t_n)) \\quad \\text{(by induction hypothesis)}$$\n\n$$= f^{\\mathcal{A}_2}(\\mathfrak{I}_2(t_1), \\ldots, \\mathfrak{I}_2(t_n)) \\quad \\text{(by hypothesis, } f^{\\mathcal{A}_1} = f^{\\mathcal{A}_2})$$\n\n$$= \\mathfrak{I}_2(f t_1 \\ldots t_n).$$\n\n(b) We use induction on $S$-formulas.\n\n$\\varphi = R t_1 \\ldots t_n$ ($R \\in S$ $n$-ary, $t_1, \\ldots, t_n \\in T^S$):\n\n$$\\mathfrak{I}_1 \\models R t_1 \\ldots t_n \\iff R^{\\mathcal{A}_1} \\mathfrak{I}_1(t_1) \\ldots \\mathfrak{I}_1(t_n)$$\n\n$$\\iff R^{\\mathcal{A}_1} \\mathfrak{I}_2(t_1) \\ldots \\mathfrak{I}_2(t_n) \\quad \\text{(by (a))}$$\n\n$$\\iff R^{\\mathcal{A}_2} \\mathfrak{I}_2(t_1) \\ldots \\mathfrak{I}_2(t_n) \\quad \\text{(by hypothesis, } R^{\\mathcal{A}_1} = R^{\\mathcal{A}_2})$$\n\n$$\\iff \\mathfrak{I}_2 \\models R t_1 \\ldots t_n.$$\n\n$\\varphi = t_1 \\equiv t_2$: Similarly.\n\n$\\varphi = \\neg \\psi$: $\\mathfrak{I}_1 \\models \\neg \\psi$\n\niff not $\\mathfrak{I}_1 \\models \\psi$\n\niff not $\\mathfrak{I}_2 \\models \\psi$ (by induction hypothesis)\n\niff $\\mathfrak{I}_2 \\models \\neg \\psi$.\n\n$\\varphi = (\\psi \\lor \\chi)$: Similarly.\n\n$\\varphi = \\exists x \\psi$: $\\mathfrak{I}_1 \\models \\exists x \\psi$\n\niff there is an $a \\in A_1$ such that $\\mathfrak{I}_1 \\frac{a}{x} \\models \\psi$\n\niff there is an $a \\in A_2 (= A_1)$ such that $\\mathfrak{I}_2 \\frac{a}{x} \\models \\psi$\n\n(by induction hypothesis applied to $\\psi$, $\\mathfrak{I}_1 \\frac{a}{x}$ and $\\mathfrak{I}_2 \\frac{a}{x}$; note that, because $\\operatorname{free}(\\psi) \\subset \\operatorname{free}(\\varphi) \\cup \\{x\\}$, the interpretations $\\mathfrak{I}_1 \\frac{a}{x}$ and $\\mathfrak{I}_2 \\frac{a}{x}$ agree on all symbols occurring in $\\psi$ and all variables occurring free in $\\psi$)\n\niff $\\mathfrak{I}_2 \\models \\exists x \\psi$.\n",
        "structure": []
    },
    {
        "id": 1,
        "domain": "Measure Theory",
        "informal": "2.58  *countable subadditivity*\n\nSuppose $(X,S,\\mu)$ is a measure space and $E_1, E_2, \\ldots \\in S$. Then\n\n$$\n\\mu \\left( \\bigcup_{k=1}^\\infty E_k \\right) \\leq \\sum_{k=1}^\\infty \\mu(E_k).\n$$\n\n**Proof** Let $D_1 = \\varnothing$ and $D_k = E_1 \\cup \\cdots \\cup E_{k-1}$ for $k \\geq 2$. Then\n\n$$\nE_1 \\setminus D_1, E_2 \\setminus D_2, E_3 \\setminus D_3, \\ldots\n$$\n\nis a disjoint sequence of subsets of $X$ whose union equals $\\bigcup_{k=1}^\\infty E_k$. Thus\n\n$$\n\\mu \\left( \\bigcup_{k=1}^\\infty E_k \\right) = \\mu \\left( \\bigcup_{k=1}^\\infty (E_k \\setminus D_k) \\right)\n= \\sum_{k=1}^\\infty \\mu(E_k \\setminus D_k)\n\\leq \\sum_{k=1}^\\infty \\mu(E_k),\n$$\n\nwhere the second line above follows from the countable additivity of $\\mu$ and the last line above follows from 2.57(a).\n",
        "structure": []
    },
    {
        "id": 1,
        "domain": "Probability Theory",
        "informal": "**Proposition 2.8.** Let $X$ be a random variable, and let $A \\in \\mathcal{B}$. Then $\\{X \\in A\\}$ is an event, i.e., it is in $\\mathcal{F}$.\n\n**Proof.** We will use a trick here: we define the class $\\mathcal{G}$ of all $A \\subset \\mathbb{R}$ for which $\\{X \\in A\\} \\in \\mathcal{F}$. Then we show that $\\mathcal{G}$ is a $\\sigma$-field which contains the open intervals. This means that it contains the Borel sets since $\\mathcal{B}$ is the minimal such $\\sigma$-field; this implies the result. The tricky part is that we deal simultaneously with $\\mathcal{F}$, a class of subsets of $\\Omega$, and $\\mathcal{G}$, a class of subsets of $\\mathbb{R}$. We know that $\\mathcal{F}$ is a $\\sigma$-field. The problem is to show that $\\mathcal{G}$ is too. Let's get to it.\n\nSince $\\{a < X < b\\}$ is an event, $\\mathcal{G}$ contains open intervals. It clearly contains the empty set. If $A \\in \\mathcal{G}$, then $\\{X \\in A\\} \\in \\mathcal{F}$ so $\\{X \\in A^c\\} = \\{X \\in A\\}^c \\in \\mathcal{F}$, since $\\mathcal{F}$ is closed under complementation. Therefore, $A^c \\in \\mathcal{G}$. Finally, if $A_1, A_2, \\cdots \\in \\mathcal{G}$, then $\\{X \\in \\bigcup_n A_n\\} = \\bigcup_n \\{X \\in A_n\\} \\in \\mathcal{F}$ since $\\mathcal{F}$ is also closed under countable unions. Therefore, $\\bigcup_n A_n \\in \\mathcal{G}$. This verifies that $\\mathcal{G}$ is a $\\sigma$-field containing the open intervals, as claimed.  □",
        "structure": []
    },
    {
        "id": 1,
        "domain": "Set Theory",
        "informal": "## Goal\n\n$A = \\bigcup \\mathcal{P}(A)$\n\n## Proof\n\n  - ($\\subseteq$): Let $x \\in A$. The singleton set $\\{x\\}$ is a subset of $A$, so $\\{x\\} \\in \\mathcal{P}(A)$. Therefore, $x \\in \\bigcup \\mathcal{P}(A)$ since $x \\in \\{x\\}$ and $\\{x\\}$ is in $\\mathcal{P}(A)$.\n\n  - ($\\supseteq$): Let $x \\in \\bigcup \\mathcal{P}(A)$. By definition, there exists some $Y \\in \\mathcal{P}(A)$ such that $x \\in Y$. Since $Y \\in \\mathcal{P}(A)$, we know $Y \\subseteq A$. Therefore, $x \\in A$.\n\nHence, $A = \\bigcup \\mathcal{P}(A)$.\n\n",
        "structure": []
    },
    {
        "id": 1,
        "domain": "Topology Point Set",
        "informal": "**Lemma 13.2.**  *Let $X$ be a topological space. Suppose that $\\mathcal{C}$ is a collection of open sets of $X$ such that for each open set $U$ of $X$ and each $x$ in $U$, there is an element $C$ of $\\mathcal{C}$ such that $x \\in C \\subset U$. Then $\\mathcal{C}$ is a basis for the topology of $X$.*\n\n*Proof.* We must show that $\\mathcal{C}$ is a basis. The first condition for a basis is easy: Given $x \\in X$, since $X$ is itself an open set, there is by hypothesis an element $C$ of $\\mathcal{C}$ such that $x \\in C \\subset X$. To check the second condition, let $x$ belong to $C_1 \\cap C_2$, where $C_1$ and $C_2$ are elements of $\\mathcal{C}$. Since $C_1$ and $C_2$ are open, so is $C_1 \\cap C_2$. Therefore, there exists by hypothesis an element $C_3$ in $\\mathcal{C}$ such that $x \\in C_3 \\subset C_1 \\cap C_2$.\n\nLet $\\mathcal{T}$ be the collection of open sets of $X$; we must show that the topology $\\mathcal{T}'$ generated by $\\mathcal{C}$ equals the topology $\\mathcal{T}$. First, note that if $U$ belongs to $\\mathcal{T}$ and if $x \\in U$, then there is by hypothesis an element $C$ of $\\mathcal{C}$ such that $x \\in C \\subset U$. It follows that $U$ belongs to the topology $\\mathcal{T}'$, by definition. Conversely, if $W$ belongs to the topology $\\mathcal{T}'$, then $W$ equals a union of elements of $\\mathcal{C}$, by the preceding lemma. Since each element of $\\mathcal{C}$ belongs to $\\mathcal{T}$ and $\\mathcal{T}$ is a topology, $W$ also belongs to $\\mathcal{T}$. ■\n\n",
        "structure": []
    },
    {
        "id": 2,
        "domain": "Abstract Algebra",
        "informal": "**Theorem**\n\nLet $G$ be a group which acts on a finite set $X$.\n\nLet $x \\in X$.\n\nLet $\\text{Orb}(x)$ denote the orbit of $x$.\n\nLet $\\text{Stab}(x)$ denote the stabilizer of $x$ by $G$.\n\nLet $[G : \\text{Stab}(x)]$ denote the index of $\\text{Stab}(x)$ in $G$.\n\nThen:\n\n$$|\\text{Orb}(x)| = [G : \\text{Stab}(x)] = \\frac{|G|}{|\\text{Stab}(x)|}$$\n\n**Proof**\n\nLet us define the mapping:\n\n$a : G \\to \\text{Orb}(x)$\n\nsuch that:\n\n$a(g) = g * x$\n\nwhere $*$ denotes the group action.\n\nIt is clear that $a$ is surjective, because from the definition $x$ was acted on by all the elements of $G$.\n\nNext, from Stabilizer is Subgroup: Corollary:\n\n$a(g) = a(h) \\iff g^{-1}h \\in \\text{Stab}(x)$\n\nThis means:\n\n$g \\equiv h \\pmod{\\text{Stab}(x)}$\n\nThus there is a well-defined bijection:\n\n$G / \\text{Stab}(x) \\to \\text{Orb}(x)$\n\ngiven by:\n\n$g \\text{Stab}(x) \\mapsto g * x$\n\nSo $\\text{Orb}(x)$ has the same number of elements as $G / \\text{Stab}(x)$.\n\nThat is:\n\n$|\\text{Orb}(x)| = [G : \\text{Stab}(x)]$\n\nThe result follows.",
        "structure": []
    },
    {
        "id": 2,
        "domain": "Analysis",
        "informal": "Let $\\{x_n\\}$ be a sequence of real numbers defined by $x_n = n^{(-1)^n}$ for all $n \\geq 1$. Prove that $\\{x_n\\}$ is unbounded and that $\\lim\\limits_{n \\to \\infty} x_n = \\infty$ does not hold.\n\n**Proof:**\n\nFirst, for every positive integer $k$, we have:\n- $x_{2k} = 2k$\n- $x_{2k-1} = \\frac{1}{2k-1}$\n\n**(1) Proof that $\\{x_n\\}$ is unbounded:**\n\nConsider the subsequence $y_k = x_{2k}$. We show that $\\{y_k\\}$ is unbounded. For any given $M > 0$, choose $k_0 = \\lceil M/2 \\rceil$. Then for all $k \\geq k_0$, we have:\n\\[\ny_k = x_{2k} = 2k \\geq 2k_0 \\geq M.\n\\]\nThus, $\\{y_k\\}$ is unbounded. Since $\\{y_k\\}$ is a subsequence of $\\{x_n\\}$, it follows that $\\{x_n\\}$ is unbounded.\n\n**(2) Proof that $\\lim\\limits_{n \\to \\infty} x_n = \\infty$ does not hold:**\n\nConsider two subsequences:\n- $y_k = x_{2k}$ with $\\lim\\limits_{k \\to \\infty} y_k = \\lim\\limits_{k \\to \\infty} 2k = +\\infty$\n- $z_k = x_{2k-1}$ with $\\lim\\limits_{k \\to \\infty} z_k = \\lim\\limits_{k \\to \\infty} \\frac{1}{2k-1} = 0$\n\nSince the subsequence $\\{z_k\\}$ converges to $0$ (not $\\infty$), the limit $\\lim\\limits_{n \\to \\infty} x_n = \\infty$ cannot hold.\n\n**Q.E.D.**",
        "structure": []
    },
    {
        "id": 2,
        "domain": "High School",
        "informal": "## Goal\n\nFor any $ n $ positive integers $ a_1, a_2, \\ldots, a_n $, we have\n\n$$\n\\frac{a_1 + a_2 + \\cdots + a_n}{n} \\geq \\sqrt[n]{a_1 a_2 \\cdots a_n} \\geq n \\left( \\frac{1}{a_1} + \\frac{1}{a_2} + \\cdots + \\frac{1}{a_n} \\right)^{-1},\n$$\n\nwith equality if and only if $ a_1 = a_2 = \\cdots = a_n $.\n\n**Proof**: First, we prove the inequality on the left-hand side\n\n$$\n\\frac{a_1 + a_2 + \\cdots + a_n}{n} \\geq \\sqrt[n]{a_1 a_2 \\cdots a_n}.\n$$\n\nWhen $ n = 1, 2 $, the inequality is obviously true.\n\nWhen $ n = 2^k (k \\in \\mathbb{N}^*) $, the inequality is the direct result of $ \\frac{a + b}{2} \\geq \\sqrt{ab} $.\n\nWhen $ n \\neq 2^k $, take $ l \\in \\mathbb{N}^* $ such that $ 2^{l-1} < n < 2^l $. Let\n\n$$\n\\sqrt[n]{a_1 a_2 \\cdots a_n} = \\bar{a}.\n$$\n\nAdd $ (2^l - n) $ $ \\bar{a} $'s to $ a_1, a_2, \\ldots, a_n $ to make it $ 2^l $ positive integers. Applying the inequality to these $ 2^l $ positive integers, we get\n\n$$\n\\frac{1}{2^l} \\left[ a_1 + a_2 + \\cdots + a_n + (2^l - n) \\bar{a} \\right] \\geq (\\bar{a})^{1/2^l} = \\bar{a}.\n$$\n\nAfter rearranging, we have\n\n$$\n\\frac{a_1 + a_2 + \\cdots + a_n}{n} \\geq \\sqrt[n]{a_1 a_2 \\cdots a_n}.\n$$\n\nFor $ \\frac{1}{a_1}, \\frac{1}{a_2}, \\ldots, \\frac{1}{a_n} $, using the above conclusion, we obtain the inequality on the right-hand side.\n\n**Q.E.D.**",
        "structure": []
    },
    {
        "id": 2,
        "domain": "IMO",
        "informal": "## Goal\n\nLet $k\\geqslant2$ be an integer. Prove that the smallest integer $n\\geqslant k+1$ with the property that \"there exists a set of $n$ distinct real numbers such that each of its elements can be written as a sum of $k$ other distinct elements of the set\" is $n=k+4$.\n\n## Proof\n\nFirst we show that $n\\geqslant k+ 4$. Suppose that there exists such a set with $n$ numbers and denote them by $a_1<a_2<\\cdots<a_n$.\n\nNote that in order to express $a_1$ as a sum of $k$ distinct elements of the set, we must have $a_1\\geqslant a_2+\\cdots+a_{k+1}$ and, similarly for $a_n$, we must have $a_{n-k}+\\cdots+a_{n-1}\\geqslant a_n.$ We also know $\\mathop{\\text{that }}n\\geqslant k+1.$\n\nIf $n=k+1$ then we have $a_1\\geqslant a_2+\\cdots+a_{k+1}>a_1+\\cdots+a_k\\geqslant a_{k+1}$, which gives a contradiction.\n\nIf $n= k+ 2$ then we have $a_1\\geqslant a_2+ \\cdots + a_{k+ 1}\\geqslant a_{k+ 2}$, that again gives a contradiction. \n\nIf $n= k+ 3$ then we have $a_1\\geqslant a_2+ \\cdots + a_{k+ 1}$ and $a_3+ \\cdots + a_{k+ 2}\\geqslant a_{k+ 3}.$ Adding the two inequalities we get $a_1+a_{k+2}\\geqslant a_2+a_{k+3}$, again a contradiction.\n\nIt remains to give an example of a set with $k+4$ elements satisfying the condition of the problem. We start with the case when $k=2l$ and $l\\geqslant1.$ In that case, denote by $A_i=\\{-i,i\\}$ and take the set $A_1\\cup\\cdots\\cup A_{l+2}$, which has exactly $k+4=2l+4$ elements. We are left to show that this set satisfes the required condition.\n\nNote that if a number $i$ can be expressed in the desired way, then so can $-i$ by negating the expression. Therefore, we consider only $1\\leqslant i\\leqslant l+2$.\n\nIf $i<l+2$, we sum the numbers from some $l-1$ sets $A_j$ with $j\\neq1,i+1$, and the numbers $i+1$ and $-1$.\n\nFor $i= l+ 2$, we sum the numbers from some $l- 1$ sets $A_{j}$ with $j\\neq 1, l+ 1$, and the numbers $l+1$ and $1$.\n\nIt remains to a give a construction for odd $k=2l+1$ with $l\\geqslant1$ (since $k\\geqslant2).$ To that end, we modify the construction for $k=2l$ by adding 0 to the previous set.\n\nThis is a valid set as $0$ can be added to each constructed expression, and $0$ can be expressed as follows: take the numbers $1,2,-3$ and all the numbers from the remaining $l-1$ sets $A_4,A_5,\\cdots,A_{l+2}.$\n\n$Qed.$",
        "structure": []
    },
    {
        "id": 2,
        "domain": "Linear Algebra",
        "informal": "2.19 *linear dependence lemma*\n\nSuppose $v_1, ..., v_m$ is a linearly dependent list in $V$. Then there exists $k \\in \\{1, 2, ..., m\\}$ such that\n\n$$v_k \\in \\operatorname{span}(v_1, ..., v_{k-1}).$$\n\nFurthermore, if $k$ satisfies the condition above and the $k^{th}$ term is removed from $v_1, ..., v_m$, then the span of the remaining list equals $\\operatorname{span}(v_1, ..., v_m)$.\n\n**Proof** Because the list $v_1, ..., v_m$ is linearly dependent, there exist numbers $a_1, ..., a_m \\in \\mathbf{F}$, not all 0, such that\n\n$$a_1 v_1 + \\cdots + a_m v_m = 0.$$\n\nLet $k$ be the largest element of $\\{1, ..., m\\}$ such that $a_k \\neq 0$. Then\n\n$$v_k = -\\frac{a_1}{a_k} v_1 - \\cdots - \\frac{a_{k-1}}{a_k} v_{k-1},$$\n\nwhich proves that $v_k \\in \\operatorname{span}(v_1, ..., v_{k-1})$, as desired.\n\nNow suppose $k$ is any element of $\\{1, ..., m\\}$ such that $v_k \\in \\operatorname{span}(v_1, ..., v_{k-1})$. Let $b_1, ..., b_{k-1} \\in \\mathbf{F}$ be such that\n\n2.20  \n$$v_k = b_1 v_1 + \\cdots + b_{k-1} v_{k-1}.$$\n\nSuppose $u \\in \\operatorname{span}(v_1, ..., v_m)$. Then there exist $c_1, ..., c_m \\in \\mathbf{F}$ such that\n\n$$u = c_1 v_1 + \\cdots + c_m v_m.$$\n\nIn the equation above, we can replace $v_k$ with the right side of 2.20, which shows that $u$ is in the span of the list obtained by removing the $k^{th}$ term from $v_1, ..., v_m$. Thus removing the $k^{th}$ term of the list $v_1, ..., v_m$ does not change the span of the list.\n\n# Structure\n\n```json\n{\n  \"structure\": [\n    {\n      \"type\": \"Hint\",\n      \"text\": \"2.19 *linear dependence lemma*\"\n    },\n    {\n      \"type\": \"Fix\",\n      \"variable\": [\n        \"$V$\",\n        \"$m$\",\n        \"$v_1, ..., v_m$\"\n      ],\n      \"condition\": [\n        \"$V$ is a vector space\",\n        \"$m \\\\in \\\\mathbb{N}^*$\",\n        \"$v_1, ..., v_m$ is a list of vectors in $V$\",\n        \"$v_1, ..., v_m$ is linearly dependent\"\n      ],\n      \"scope\": [\n        {\n          \"type\": \"Show\",\n          \"proposition\": [\n            \"$\\\\exists k \\\\in \\\\{1, ..., m\\\\}$ such that $v_k \\\\in \\\\operatorname{span}(v_1, ..., v_{k-1})$\",\n            \"if $k$ satisfies $v_k \\\\in \\\\operatorname{span}(v_1, ..., v_{k-1})$ and the $k^{th}$ term is removed from $v_1, ..., v_m$, then the span of the remaining list equals $\\\\operatorname{span}(v_1, ..., v_m)$\"\n          ],\n          \"method\": null,\n          \"scope\": [\n            {\n              \"type\": \"Show\",\n              \"proposition\": [\n                \"$\\\\exists k \\\\in \\\\{1, ..., m\\\\}$ such that $v_k \\\\in \\\\operatorname{span}(v_1, ..., v_{k-1})$\"\n              ],\n              \"method\": null,\n              \"scope\": [\n                {\n                  \"type\": \"Obtain\",\n                  \"obtained_variable\": [\n                    \"$a_1, ..., a_m$\"\n                  ],\n                  \"condition\": [\n                    \"$a_1, ..., a_m \\\\in \\\\mathbf{F}$\",\n                    \"not all $a_i$ are 0\",\n                    \"$a_1 v_1 + \\\\cdots + a_m v_m = 0$\"\n                  ],\n                  \"reason\": [\n                    \"the list $v_1, ..., v_m$ is linearly dependent\"\n                  ]\n                },\n                {\n                  \"type\": \"Define\",\n                  \"term\": \"$k$\",\n                  \"definition\": \"$k$ is the largest element of $\\\\{1, ..., m\\\\}$ such that $a_k \\\\neq 0$\"\n                },\n                {\n                  \"type\": \"Have\",\n                  \"claim\": [\n                    \"$v_k = -\\\\frac{a_1}{a_k} v_1 - \\\\cdots - \\\\frac{a_{k-1}}{a_k} v_{k-1}$\"\n                  ],\n                  \"reason\": null\n                },\n                {\n                  \"type\": \"Have\",\n                  \"claim\": [\n                    \"$v_k \\\\in \\\\operatorname{span}(v_1, ..., v_{k-1})$\"\n                  ],\n                  \"reason\": null\n                }\n              ]\n            },\n            {\n              \"type\": \"Show\",\n              \"proposition\": [\n                \"if $k$ satisfies $v_k \\\\in \\\\operatorname{span}(v_1, ..., v_{k-1})$ and the $k^{th}$ term is removed from $v_1, ..., v_m$, then the span of the remaining list equals $\\\\operatorname{span}(v_1, ..., v_m)$\"\n              ],\n              \"method\": null,\n              \"scope\": [\n                {\n                  \"type\": \"Fix\",\n                  \"variable\": [\n                    \"$k$\"\n                  ],\n                  \"condition\": [\n                    \"$k \\\\in \\\\{1, ..., m\\\\}$\",\n                    \"$v_k \\\\in \\\\operatorname{span}(v_1, ..., v_{k-1})$\"\n                  ],\n                  \"scope\": [\n                    {\n                      \"type\": \"Obtain\",\n                      \"obtained_variable\": [\n                        \"$b_1, ..., b_{k-1}$\"\n                      ],\n                      \"condition\": [\n                        \"$b_1, ..., b_{k-1} \\\\in \\\\mathbf{F}$\",\n                        \"$v_k = b_1 v_1 + \\\\cdots + b_{k-1} v_{k-1}$\"\n                      ],\n                      \"reason\": null\n                    },\n                    {\n                      \"type\": \"Fix\",\n                      \"variable\": [\n                        \"$u$\"\n                      ],\n                      \"condition\": [\n                        \"$u \\\\in \\\\operatorname{span}(v_1, ..., v_m)$\"\n                      ],\n                      \"scope\": [\n                        {\n                          \"type\": \"Obtain\",\n                          \"obtained_variable\": [\n                            \"$c_1, ..., c_m$\"\n                          ],\n                          \"condition\": [\n                            \"$c_1, ..., c_m \\\\in \\\\mathbf{F}$\",\n                            \"$u = c_1 v_1 + \\\\cdots + c_m v_m$\"\n                          ],\n                          \"reason\": null\n                        },\n                        {\n                          \"type\": \"Have\",\n                          \"claim\": [\n                            \"$u \\\\in \\\\operatorname{span}(v_1, ..., v_{k-1}, v_{k+1}, ..., v_m)$\"\n                          ],\n                          \"reason\": [\n                            \"replace $v_k$ with $b_1 v_1 + \\\\cdots + b_{k-1} v_{k-1}$ in $u = c_1 v_1 + \\\\cdots + c_m v_m$\"\n                          ]\n                        }\n                      ]\n                    },\n                    {\n                      \"type\": \"Have\",\n                      \"claim\": [\n                        \"$\\\\operatorname{span}(v_1, ..., v_m) = \\\\operatorname{span}(v_1, ..., v_{k-1}, v_{k+1}, ..., v_m)$\"\n                      ],\n                      \"reason\": null\n                    }\n                  ]\n                }\n              ]\n            }\n          ]\n        }\n      ]\n    }\n  ]\n}\n```\n\n",
        "structure": []
    },
    {
        "id": 2,
        "domain": "Logic",
        "informal": "**1.9 Lemma.** *Suppose that* $\\Phi$ *is consistent and negation complete and that it contains witnesses. Then the following holds for all* $\\varphi$ *and* $\\psi$:\n(a) $\\Phi \\vdash \\neg \\varphi$ *iff not* $\\Phi \\vdash \\varphi$.\n(b) $\\Phi \\vdash (\\varphi \\lor \\psi)$ *if and only if* $\\Phi \\vdash \\varphi$ *or* $\\Phi \\vdash \\psi$.\n(c) $\\Phi \\vdash \\exists x \\varphi$ *if and only if there is a term* $t$ *with* $\\Phi \\vdash \\varphi \\frac{t}{x}$.\n\n*Proof.* (a) Since $\\Phi$ is negation complete, we have $\\Phi \\vdash \\varphi$ or $\\Phi \\vdash \\neg \\varphi$; and since $\\Phi$ is consistent, $\\Phi \\vdash \\neg \\varphi$ iff not $\\Phi \\vdash \\varphi$.\n\n(b) First let $\\Phi \\vdash (\\varphi \\lor \\psi)$. If not $\\Phi \\vdash \\varphi$, then $\\Phi \\vdash \\neg \\varphi$ (since $\\Phi$ is negation complete), and IV.3.4 gives $\\Phi \\vdash \\psi$. The other direction follows immediately by the $\\lor$-rules ($\\lor$S) for the succedent.\n\n(c) Let $\\Phi \\vdash \\exists x \\varphi$. Since $\\Phi$ contains witnesses, there is a term $t$ with $\\Phi \\vdash (\\exists x \\varphi \\to \\varphi \\frac{t}{x})$; using Modus ponens, IV.3.5, we get $\\Phi \\vdash \\varphi \\frac{t}{x}$. Conversely let $\\Phi \\vdash \\varphi \\frac{t}{x}$ for a term $t$. Then the rule ($\\exists$S) of the $\\exists$-introduction in the succedent gives $\\Phi \\vdash \\exists x \\varphi$.\n",
        "structure": []
    },
    {
        "id": 2,
        "domain": "Measure Theory",
        "informal": "2.85  *Egorov’s Theorem*\n\nSuppose $(X,S,\\mu)$ is a measure space with $\\mu(X)<\\infty$. Suppose $f_1, f_2, \\ldots$ is a sequence of $S$-measurable functions from $X$ to $\\mathbf{R}$ that converges pointwise on $X$ to a function $f : X \\to \\mathbf{R}$. Then for every $\\varepsilon > 0$, there exists a set $E \\in S$ such that $\\mu(X \\setminus E) < \\varepsilon$ and $f_1, f_2, \\ldots$ converges uniformly to $f$ on $E$.\n\n**Proof** Suppose $\\varepsilon > 0$. Temporarily fix $n \\in \\mathbf{Z}^+$. The definition of pointwise convergence implies that\n\n$$\n\\bigcup_{m=1}^\\infty \\bigcap_{k=m}^\\infty \\{x \\in X : |f_k(x) - f(x)| < \\frac{1}{n}\\} = X.\n\\tag{2.86}\n$$\n\nFor $m \\in \\mathbf{Z}^+$, let\n\n$$\nA_{m,n} = \\bigcap_{k=m}^\\infty \\{x \\in X : |f_k(x) - f(x)| < \\frac{1}{n}\\}.\n$$\n\nThen clearly $A_{1,n} \\subset A_{2,n} \\subset \\cdots$ is an increasing sequence of sets and 2.86 can be rewritten as\n\n$$\n\\bigcup_{m=1}^\\infty A_{m,n} = X.\n$$\n\nThe equation above implies (by 2.59) that $\\lim_{m \\to \\infty} \\mu(A_{m,n}) = \\mu(X)$. Thus there exists $m_n \\in \\mathbf{Z}^+$ such that\n\n$$\n\\mu(X) - \\mu(A_{m_n,n}) < \\frac{\\varepsilon}{2^n}.\n\\tag{2.87}\n$$\n\nNow let\n\n$$\nE = \\bigcap_{n=1}^\\infty A_{m_n,n}.\n$$\n\nThen\n\n$$\n\\mu(X \\setminus E) = \\mu \\left(X \\setminus \\bigcap_{n=1}^\\infty A_{m_n,n}\\right)\n= \\mu \\left(\\bigcup_{n=1}^\\infty (X \\setminus A_{m_n,n})\\right)\n\\leq \\sum_{n=1}^\\infty \\mu(X \\setminus A_{m_n,n})\n< \\varepsilon,\n$$\n\nwhere the last inequality follows from 2.87.\n\nTo complete the proof, we must verify that $f_1, f_2, \\ldots$ converges uniformly to $f$ on $E$. To do this, suppose $\\varepsilon' > 0$. Let $n \\in \\mathbf{Z}^+$ be such that $\\frac{1}{n} < \\varepsilon'$. Then $E \\subset A_{m_n,n}$, which implies that\n\n$$\n|f_k(x) - f(x)| < \\frac{1}{n} < \\varepsilon'\n$$\n\nfor all $k \\geq m_n$ and all $x \\in E$. Hence $f_1, f_2, \\ldots$ does indeed converge uniformly to $f$ on $E$.\n",
        "structure": []
    },
    {
        "id": 2,
        "domain": "Probability Theory",
        "informal": "**Theorem 3.5.** Let $X$ and $Y$ be independent random variables. If both $X$ and $Y$ are integrable, so is $XY$, and\n\n(3.2)  \n$$E\\{XY\\} = E\\{X\\} E\\{Y\\}.$$\n\n**Proof.** Suppose first that $X$ and $Y$ are discrete with possible values $(x_i)$ and $(y_j)$, respectively. Then\n$$\nE\\{|XY|\\} = \\sum_{i,j} |x_i| |y_j| P\\{X = x_i, Y = y_j\\}\n$$\n\n$$\n= \\sum_{i,j} |x_i| |y_j| P\\{X = x_i\\} P\\{Y = y_j\\}\n$$\n\n$$\n= \\sum_i |x_i| P\\{X = x_i\\} \\sum_j |y_j| P\\{Y = y_j\\}\n$$\n\n$$\n= E\\{|X|\\} E\\{|Y|\\},\n$$\n\nwhere the change of order in the summation is justified since the summands are positive. Both $X$ and $Y$ are integrable, so this is finite. Thus $XY$ is integrable. Now remove the absolute value signs. The series converges absolutely, so that the terms can be rearranged, and the same calculation (sans absolute value signs) gives (3.2).\n\nNext, if $X$ and $Y$ are integrable, so are the dyadic approximations $\\bar{X}_n$ and $\\bar{Y}_n$, and, as $\\bar{X}_n - X$ and $\\bar{Y}_n - Y$ are both positive and less than $2^{-n}$,\n\n$$\nE\\{| \\bar{X}_n \\bar{Y}_n - XY | \\} \\leq E\\{| \\bar{X}_n || \\bar{Y}_n - Y | \\} + E\\{|Y| | \\bar{X}_n - X | \\}\n$$\n\n$$\n\\leq 2^{-n} \\left(E\\{|\\bar{X}_n|\\} + E\\{|Y|\\}\\right)\n$$\n\n$$\n\\leq 2^{-n} \\left(2 + E\\{|\\bar{X}_0|\\} + E\\{|\\bar{Y}_0|\\}\\right) \\to 0\n$$\n\nwhere we have used the facts that $|\\bar{X}_n| \\leq 1 + |\\bar{X}_0|$ and $|Y| \\leq 1 + |\\bar{Y}_0|$. In particular, since $\\bar{X}_n \\bar{Y}_n$ is integrable, so is $XY$. Therefore\n\n$$\nE\\{XY\\} = \\lim_n E\\{\\bar{X}_n \\bar{Y}_n\\} = \\lim_n E\\{\\bar{X}_n\\} E\\{\\bar{Y}_n\\} = E\\{X\\} E\\{Y\\}.\n$$\n\n□",
        "structure": []
    },
    {
        "id": 2,
        "domain": "Set Theory",
        "informal": "## Goal\n\n$A \\times \\bigcup B = \\bigcup \\{A \\times X \\mid X \\in B\\}$\n\n## Proof\n\n  - ($\\subseteq$): Let $(a, x) \\in A \\times \\bigcup B$. By definition, $x \\in \\bigcup B$, so there exists $X \\in B$ such that $x \\in X$. Therefore, $(a, x) \\in A \\times X$. Since $A \\times X$ is part of the union $\\bigcup \\{A \\times X \\mid X \\in B\\}$, we conclude $(a, x) \\in \\bigcup \\{A \\times X \\mid X \\in B\\}$.\n\n  - ($\\supseteq$): Let $(a, x) \\in \\bigcup \\{A \\times X \\mid X \\in B\\}$. Then there exists $X \\in B$ such that $(a, x) \\in A \\times X$. By definition of Cartesian product, $a \\in A$ and $x \\in X$. Since $X \\in B$, we have $x \\in \\bigcup B$. Thus, $(a, x) \\in A \\times \\bigcup B$.\n\nHence, $A \\times \\bigcup B = \\bigcup \\{A \\times X \\mid X \\in B\\}$.\n\n# Structure\n\n[Fix] {$A,B$} such that {$A,B$ are sets}\n{\n\t[Show] {$A \\times \\bigcup B = \\bigcup \\{A \\times X \\mid X \\in B\\}$}\n\t{\n\t\t[Show] {$A \\times \\bigcup B \\subseteq \\bigcup \\{A \\times X \\mid X \\in B\\}$}\n\t\t{\n\t\t\t[Fix] {$a,x$} such that {$(a, x) \\in A \\times \\bigcup B$}\n\t\t\t{\n\t\t\t\t[Have] {$x \\in \\bigcup B$} by {definition}\n\t\t\t\t[Obtain] {$X$} such that {$X \\in B$}; {$x \\in X$}\n\t\t\t\t[Have] {$(a, x) \\in A \\times X$}\n\t\t\t\t[Have] {$(a, x) \\in \\bigcup \\{A \\times X \\mid X \\in B\\}$} by {$A \\times X$ is part of the union $\\bigcup \\{A \\times X \\mid X \\in B\\}$}\n\t\t\t}\n\t\t}\n\t\t[Show] {$\\bigcup \\{A \\times X \\mid X \\in B\\} \\subseteq A \\times \\bigcup B$}\n\t\t{\n\t\t\t[Fix] {$a, x$} such that {$(a, x) \\in \\bigcup \\{A \\times X \\mid X \\in B\\}$}\n\t\t\t{\n\t\t\t\t[Obtain] {$X$} such that {$X \\in B$}; {$(a, x) \\in A \\times X$}\n\t\t\t\t[Have] {$a \\in A$}; {$x \\in X$} by {definition of Cartesian product}\n\t\t\t\t[Have] {$x \\in \\bigcup B$} by {$X \\in B$}\n\t\t\t\t[Have] {$(a, x) \\in A \\times \\bigcup B$}\n\t\t\t}\n\t\t}\n\t\t[Have] {$A \\times \\bigcup B = \\bigcup \\{A \\times X \\mid X \\in B\\}$}\n\t}\n}",
        "structure": []
    },
    {
        "id": 2,
        "domain": "Topology Point Set",
        "informal": "**Lemma 13.1.**  *Let $X$ be a set; let $\\mathcal{B}$ be a basis for a topology $\\mathcal{T}$ on $X$. Then $\\mathcal{T}$ equals the collection of all unions of elements of $\\mathcal{B}$.*\n\n*Proof.* Given a collection of elements of $\\mathcal{B}$, they are also elements of $\\mathcal{T}$. Because $\\mathcal{T}$ is a topology, their union is in $\\mathcal{T}$. Conversely, given $U \\in \\mathcal{T}$, choose for each $x \\in U$ an element $B_x$ of $\\mathcal{B}$ such that $x \\in B_x \\subset U$. Then $U = \\bigcup_{x \\in U} B_x$, so $U$ equals a union of elements of $\\mathcal{B}$. ■\n",
        "structure": []
    },
    {
        "id": 3,
        "domain": "Abstract Algebra",
        "informal": "Theorem\nLet $\\phi : G_1 \\to G_2$ be a group homomorphism.\nLet $\\ker(\\phi)$ be the kernel of $\\phi$.\n\nThen:\n\n$$\\mathrm{Img}(\\phi) \\cong G_1 / \\ker(\\phi)$$\n\nwhere $\\cong$ denotes group isomorphism.\n\nProof\nLet $K = \\ker(\\phi)$.\nBy Kernel is Normal Subgroup of Domain, $G_1 / K$ exists.\nWe need to establish that the mapping $\\theta : G_1 / K \\to G_2$ defined as:\n$$\\forall x \\in G_1 : \\theta(xK) = \\phi(x)$$\nis well-defined.\nThat is, we need to ensure that:\n$$\\forall x,y \\in G_1 : xK = yK \\implies \\theta(xK) = \\theta(yK)$$\n\nLet $x,y \\in G_1 : xK = yK$.\nThen:\n\n$xK = yK$\n$\\iff x^{-1} y \\in K = \\ker(\\phi) \\quad \\text{(Left Cosets are Equal iff Product with Inverse in Subgroup)}$\n$\\iff \\phi(x^{-1} y) = e_{G_2}$\n$\\iff (\\phi(x))^{-1} \\phi(y) = e_{G_2} $\n$\\iff \\phi(x) = \\phi(y)$\n\nThus we see that $\\theta$ is well-defined.\n\nWe also note that:\n\n$$\\mathrm{Img}(\\theta) = \\{\\theta(xK) : x \\in G\\}$$\n\nSo:\n\n$$\n\\mathrm{Img}(\\theta) = \\{\\theta(xK) : x \\in G\\} \\\\\n= \\{\\phi(x) : x \\in G\\} \\\\\n= \\mathrm{Img}(\\phi)\n$$\n\nWe also note that $\\theta$ is a homomorphism:\n\n$$\n\\forall x,y \\in G : \\quad \\theta(xK yK) \\\\\n= \\theta(xyK) \\\\\n= \\phi(xy) \\\\\n= \\phi(x) \\phi(y) \\\\\n= \\theta(xK) \\theta(yK)\n$$\n\nThus $\\theta$ is a monomorphism whose image equals $\\mathrm{Img}(\\phi)$.\n\nThe result follows.\n\n■",
        "structure": []
    },
    {
        "id": 3,
        "domain": "Analysis",
        "informal": "**Example 2** Compute the improper integral $\\int_0^{+\\infty} te^{-pt} dt$, where $p$ is a constant and $p > 0$.\n\n**Solution**\n\n$\\int_0^{+\\infty} te^{-pt} dt = \\left[ \\int te^{-pt} dt \\right]_0^{+\\infty} = \\left[ -\\frac{1}{p} \\int t d(e^{-pt}) \\right]_0^{+\\infty}$\n\n$= \\left[ -\\frac{t}{p} e^{-pt} + \\frac{1}{p} \\int e^{-pt} dt \\right]_0^{+\\infty}$\n\n$= \\left[ -\\frac{t}{p} e^{-pt} \\right]_0^{+\\infty} - \\left[ \\frac{1}{p^2} e^{-pt} \\right]_0^{+\\infty}$\n\n$= -\\frac{1}{p} \\lim_{t \\to +\\infty} te^{-pt} - 0 - \\frac{1}{p^2} (0 - 1) = \\frac{1}{p^2}$\n\nNote: The limit $\\lim_{t \\to +\\infty} te^{-pt}$ in the above expression is an indeterminate form, which can be determined using L'Hôpital's rule.",
        "structure": []
    },
    {
        "id": 3,
        "domain": "High School",
        "informal": "## Goal\n\nProve that the sequence $y_n = \\left(1 + \\frac{1}{n}\\right)^{n+1}$ is decreasing.\n\n**Proof.** Let $n \\geq 2$. Using Bernoulli's inequality, we find that\n$$\n\\frac{y_{n-1}}{y_n} = \\frac{\\left(1 + \\frac{1}{n-1}\\right)^n}{\\left(1 + \\frac{1}{n}\\right)^{n+1}} = \\frac{n2^n}{(n^2 - 1)^n} \\cdot \\frac{n}{n + 1} = \\left(1 + \\frac{1}{n^2 - 1}\\right)^n \\cdot \\frac{n}{n + 1} \\geq\n$$\n\n$$\n\\left(1 + \\frac{n}{n^2 - 1}\\right) \\cdot \\frac{n}{n + 1} > \\left(1 + \\frac{1}{n}\\right) \\cdot \\frac{n}{n + 1} = 1.\n$$\n\nThis finishes the proof.\n\n",
        "structure": []
    },
    {
        "id": 3,
        "domain": "IMO",
        "informal": "## Goal\n\nA number is called Norwegian if it has three distinct positive divisors whose sum is equal to $2022$. Show that the smallest Norwegian number is $1344$.\n(Note: The total number of positive divisors of a Norwegian number is allowed to be larger than 3.)\n\n## Proof\n\nObserve that $1344$ is a Norwegian number as $6$, $672$ and $1344$ are three distinct divisors of $1344$ and $6+672+1344=2022$. It remains to show that this is the smallest such number.\n\nAssume for contradiction that $N<1344$ is Norwegian and let $N/a,N/b$ and $N/c$ be the three distinct divisors of $N$, with $a<b<c.$ Then\n\n$$\n2022=N\\left(\\frac1a+\\frac1b+\\frac1c\\right)<1344\\left(\\frac1a+\\frac1b+\\frac1c\\right)\n$$\n\nand so\n\n$$\n\\left(\\frac1a+\\frac1b+\\frac1c\\right)>\\frac{2022}{1344}=\\frac{337}{224}=\\frac32+\\frac1{224}\n$$\n\nIf $a>1$ then\n\n$$\n\\begin{aligned}\\frac{1}{a}+\\frac{1}{b}+\\frac{1}{c}\\leqslant\\frac{1}{2}+\\frac{1}{3}+\\frac{1}{4}=\\frac{13}{12}<\\frac{3}{2},\\end{aligned}\n$$\n\nso it must be the case that $a=1.$ Similarly, it must hold that $b<4$ since otherwise\n\n$$\n1+\\dfrac{1}{b}+\\dfrac{1}{c}\\leqslant1+\\dfrac{1}{4}+\\dfrac{1}{5}<\\dfrac{3}{2}\n$$\n\nThis leaves two cases to check, $b=2$ and $b=3$.\n\n$\\textbf{Case }b= 3.$ Then\n\n$$\n\\begin{aligned}\\frac1c>\\frac32+\\frac1{224}-1-\\frac13>\\frac16,\\end{aligned}\n$$\n\nso $c=4$ or $c=5.$ If $c=4$ then\n\n$$\n2022=N\\left(1+\\dfrac{1}{3}+\\dfrac{1}{4}\\right)=\\dfrac{19}{12}N\n$$\n\nbut this is impossible as $19\\not\\mid 2022$. If $c=5$ then\n\n$$\n2022=N\\left(1+\\frac13+\\frac15\\right)=\\frac{23}{15}N\n$$\n\nwhich again is impossible, as 23 $\\nmid2022.$\n\nCase $b=2.$ Note that $c<224$ since\n\n$$\n\\frac1c>\\frac32+\\frac1{224}-1-\\frac12=\\frac1{224}\n$$\n\nIt holds that\n\n$$\n2022=N\\left(1+\\frac12+\\frac1c\\right)=\\frac{3c+2}{2c}N\\Rightarrow(3c+2)N=4044c\n$$\n\nSince $(c,3c-2)=(c,2)\\in\\{1,2\\}$, then $3c+2\\mid8088=2^3\\cdot3\\cdot337$ which implies that $3c+2\\mid2^3\\cdot337.$ But since $3c+2\\geqslant3\\cdot3+2>8=2^3$ and $3c+2\\neq337$, then it must hold that $3c+2\\geqslant2\\cdot337$, contradicting $c<224.$\n\n$Qed.$\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
        "structure": []
    },
    {
        "id": 3,
        "domain": "Linear Algebra",
        "informal": "2.25 *finite-dimensional subspaces*\n\nEvery subspace of a finite-dimensional vector space is finite-dimensional.\n\n**Proof** Suppose $V$ is finite-dimensional and $U$ is a subspace of $V$. We need to prove that $U$ is finite-dimensional. We do this through the following multistep construction.\n\n**Step 1**  \nIf $U = \\{0\\}$, then $U$ is finite-dimensional and we are done. If $U \\neq \\{0\\}$, then choose a nonzero vector $u_1 \\in U$.\n\n**Step k**  \nIf $U = \\operatorname{span}(u_1, ..., u_{k-1})$, then $U$ is finite-dimensional and we are done. If $U \\neq \\operatorname{span}(u_1, ..., u_{k-1})$, then choose a vector $u_k \\in U$ such that\n\n$$u_k \\notin \\operatorname{span}(u_1, ..., u_{k-1}).$$\n\nAfter each step, as long as the process continues, we have constructed a list of vectors such that no vector in this list is in the span of the previous vectors. Thus after each step we have constructed a linearly independent list, by the linear dependence lemma (2.19). This linearly independent list cannot be longer than any spanning list of $V$ (by 2.22). Thus the process eventually terminates, which means that $U$ is finite-dimensional.\n",
        "structure": []
    },
    {
        "id": 3,
        "domain": "Logic",
        "informal": "We now use the Compactness Theorem to obtain variants of the Löwenheim-Skolem Theorem.\n\n**2.2 Theorem.** *Let $\\Phi$ be a set of formulas which is satisfiable over arbitrarily large finite domains (i.e. for every $n \\in \\mathbb{N}$ there is an interpretation satisfying $\\Phi$ over a finite domain which contains at least $n$ elements). Then $\\Phi$ is also satisfiable over an infinite domain.*\n\n*Proof.* Let  \n$$\\Psi := \\Phi \\cup \\{\\varphi_{\\ge n} \\mid 2 \\le n\\}$$  \n($\\varphi_{\\ge n}$ was introduced in III.6.3). Every interpretation which satisfies $\\Psi$ is a model of $\\Phi$ and has an infinite domain. Therefore we need only prove that $\\Psi$ is satisfiable. By the Compactness Theorem it is sufficient to show that every finite subset $\\Psi_0$ of $\\Psi$ is satisfiable. For each such $\\Psi_0$ there is an $n_0 \\in \\mathbb{N}$ such that  \n$$(*) \\quad \\Psi_0 \\subseteq \\Phi \\cup \\{\\varphi_{\\ge n} \\mid 2 \\le n \\le n_0\\}.$$  \nAccording to the hypothesis of the theorem there is an interpretation $\\mathfrak{I}$ which satisfies $\\Phi$ and whose domain contains at least $n_0$ elements. By $(*)$, $\\mathfrak{I}$ is also a model of $\\Psi_0$.  \n$\\square$\n",
        "structure": []
    },
    {
        "id": 3,
        "domain": "Measure Theory",
        "informal": "3.7  *integral of a simple function*\n\nSuppose $(X,S,\\mu)$ is a measure space, $E_1, \\ldots, E_n$ are disjoint sets in $S$, and $c_1, \\ldots, c_n \\in [0, \\infty]$. Then\n\n$$\n\\int \\left(\\sum_{k=1}^n c_k \\chi_{E_k} \\right) d\\mu = \\sum_{k=1}^n c_k \\mu(E_k).\n$$\n\n**Proof** Without loss of generality, we can assume that $E_1, \\ldots, E_n$ is an $S$-partition of $X$ [by replacing $n$ by $n+1$ and setting $E_{n+1} = X \\setminus (E_1 \\cup \\ldots \\cup E_n)$ and $c_{n+1} = 0$].\n\nIf $P$ is the $S$-partition $E_1, \\ldots, E_n$ of $X$, then $\\mathcal{L} \\left(\\sum_{k=1}^n c_k \\chi_{E_k}, P \\right) = \\sum_{k=1}^n c_k \\mu(E_k)$. Thus\n\n$$\n\\int \\left(\\sum_{k=1}^n c_k \\chi_{E_k} \\right) d\\mu \\geq \\sum_{k=1}^n c_k \\mu(E_k).\n$$\n\nTo prove the inequality in the other direction, suppose that $P$ is an $S$-partition $A_1, \\ldots, A_m$ of $X$. Then\n\n$$\n\\mathcal{L} \\left(\\sum_{k=1}^n c_k \\chi_{E_k}, P \\right) = \\sum_{j=1}^m \\mu(A_j) \\min_{\\{i : A_j \\cap E_i \\neq \\varnothing\\}} c_i\n= \\sum_{j=1}^m \\sum_{k=1}^n \\mu(A_j \\cap E_k) \\min_{\\{i : A_j \\cap E_i \\neq \\varnothing\\}} c_i\n\\leq \\sum_{j=1}^m \\sum_{k=1}^n \\mu(A_j \\cap E_k) c_k\n= \\sum_{k=1}^n c_k \\sum_{j=1}^m \\mu(A_j \\cap E_k)\n= \\sum_{k=1}^n c_k \\mu(E_k).\n$$\n\nThe inequality above implies that $\\int \\left(\\sum_{k=1}^n c_k \\chi_{E_k} \\right) d\\mu \\leq \\sum_{k=1}^n c_k \\mu(E_k)$, completing the proof.\n",
        "structure": []
    },
    {
        "id": 3,
        "domain": "Probability Theory",
        "informal": "**Proposition 3.44.** Let $X$ and $Y$ have joint density $f_{XY}(x,y)$. Then $Z \\equiv X + Y$ has a density $f_Z(z)$ given by\n\n$$\nf_Z(z) = \\int_{-\\infty}^\\infty f_{XY}(x, z - x) \\, dx = \\int_{-\\infty}^\\infty f_{XY}(z - y, y) \\, dy.\n$$\n\n**Proof.** $X + Y \\leq z \\iff (X,Y) \\in \\{(x,y) : x + y \\leq z\\}$. Then the density $F_{X+Y}$ is\n\n$$\nF_{X+Y}(z) = P\\{X + Y \\leq z\\} = \\iint_{\\{u+v \\leq z\\}} f_{XY}(u,v) \\, du \\, dv.\n$$\n\nLet $x = u$, $y = u + v$, a transformation with Jacobian 1. Then this is\n\n$$\n= \\iint_{\\{(x,y): y \\leq z\\}} f_{XY}(x, y - x) \\, dx \\, dy = \\int_{-\\infty}^z \\left[ \\int_{-\\infty}^\\infty f_{XY}(x, y - x) \\, dx \\right] dy.\n$$\n\nBut $F_{X+Y}(z) = \\int_{-\\infty}^z f_{X+Y}(y) \\, dy$, so, comparing the two integrals for $F_{X+Y}$, we see that $f_{X+Y}(y)$ must equal the term in square brackets. Add $X$ and $Y$ in the opposite order to get the second formula.  □",
        "structure": []
    },
    {
        "id": 3,
        "domain": "Set Theory",
        "informal": "## Goal\n\n$(R^{-1})^{-1} = R$\n\n## Proof\n\n- Let $(x, y) \\in (R^{-1})^{-1}$. By definition of inverse, $(y, x) \\in R^{-1}$, which implies $(x, y) \\in R$. Hence, $(R^{-1})^{-1} \\subseteq R$.\n\n- Conversely, let $(x, y) \\in R$. Then $(y, x) \\in R^{-1}$, so $(x, y) \\in (R^{-1})^{-1}$. Hence, $R \\subseteq (R^{-1})^{-1}$.\n\nTherefore, $(R^{-1})^{-1} = R$.",
        "structure": []
    },
    {
        "id": 3,
        "domain": "Topology Point Set",
        "informal": "**Theorem 16.3.** *If $A$ is a subspace of $X$ and $B$ is a subspace of $Y$, then the product topology on $A \\times B$ is the same as the topology $A \\times B$ inherits as a subspace of $X \\times Y$.*\n\n*Proof.* The set $U \\times V$ is the general basis element for $X \\times Y$, where $U$ is open in $X$ and $V$ is open in $Y$. Therefore, $(U \\times V) \\cap (A \\times B)$ is the general basis element for the subspace topology on $A \\times B$. Now\n\n$$(U \\times V) \\cap (A \\times \\mathbf{B}) = (U \\cap A) \\times (V \\cap \\mathbf{B}).$$\n\nSince $U \\cap A$ and $V \\cap B$ are the general open sets for the subspace topologies on $A$ and $B$, respectively, the set $(U \\cap A) \\times (V \\cap B)$ is the general basis element for the product topology on $A \\times B$.\n\nThe conclusion we draw is that the bases for the subspace topology on $A \\times B$ and for the product topology on $A \\times B$ are the same. Hence the topologies are the same. ■\n\n",
        "structure": []
    },
    {
        "id": 4,
        "domain": "Abstract Algebra",
        "informal": "**1.1.4 Proposition** If $G$ is a finite cyclic group of order $n$, then $G$ has exactly one (necessarily cyclic) subgroup of order $n/d$ for each positive divisor $d$ of $n$, and $G$ has no other subgroups. If $G$ is an infinite cyclic group, the (necessarily cyclic) subgroups of $G$ are of the form $\\{1, b, b^2, \\ldots\\}$, where $b$ is an arbitrary element of $G$, or in additive notation, $\\{0, b, 2b, \\ldots\\}$.\n\n*Proof.* Again, an informal argument is helpful. Suppose that $H$ is a subgroup of $\\mathbb{Z}_{20}$ (the integers with addition modulo 20). If the smallest positive integer in $H$ is 6 (a non-divisor of 20) then $H$ contains 6, 12, 18, 4 (oops, a contradiction, 6 is supposed to be the smallest positive integer). On the other hand, if the smallest positive integer in $H$ is 4, then $H = \\{4,8,12,16,0\\}$. Similarly, if the smallest positive integer in a subgroup $H$ of the additive group of integers $\\mathbb{Z}$ is 5, then $H = \\{0, 5, 10, 15, 20, \\ldots\\}$.♣",
        "structure": []
    },
    {
        "id": 4,
        "domain": "Analysis",
        "informal": "Suppose $f(x)$ is continuous on $[0,1]$ , proof that $\\lim_{h \\to 0^+} \\int_0^1 \\frac{h}{h^2 + x^2} f(x) dx = \\frac{\\pi}{2} f(0)$\n\n**Proof** (Fitting method) Since $\\lim_{h \\to 0} \\int_0^1 \\frac{h}{h^2 + x^2} dx = \\frac{\\pi}{2}$, the limit value can be rewritten as\n$$\n\\frac{\\pi}{2} f(0) = \\lim_{h \\to 0} \\int_0^1 \\frac{h}{h^2 + x^2} f(0) dx.\n$$\n\nThe problem reduces to proving: $\\lim_{h \\to 0} \\int_0^1 \\frac{h}{h^2 + x^2} [f(x) - f(0)] dx = 0.$ However,\n\n$$\n\\int_0^1 \\frac{h}{h^2 + x^2} [f(x) - f(0)] dx = \\left( \\int_0^\\delta + \\int_\\delta^1 \\right) \\frac{h}{h^2 + x^2} [f(x) - f(0)] dx.\n$$\n\nBecause $f(x)$ is continuous at $x = 0$, so $\\forall \\varepsilon > 0$, when $\\delta > 0$ is sufficiently small, on $[0, \\delta]$, $|f(x) - f(0)| < \\frac{\\varepsilon}{\\pi}$. Thus,\n\n$$\n\\begin{aligned}\n\\left| \\int_0^\\delta \\frac{h}{h^2 + x^2} [f(x) - f(0)] dx \\right| &\\leq \\int_0^\\delta \\frac{h|f(x)-f(0)|}{h^2 + x^2} dx \\\\\n&\\leq \\frac{\\varepsilon}{\\pi} \\int_{0}^{\\delta}\\frac{h}{h^2+x^2}dx \\\\\n&= \\frac{\\varepsilon}{\\pi} \\arctan \\frac{\\delta}{h} \\leq \\frac{\\varepsilon}{\\pi} \\cdot \\frac{\\pi}{2} = \\frac{\\varepsilon}{2}.\n\\end{aligned}\n$$\n\nNow, fix this $\\delta$. Then the second integral satisfies\n\n$$\n\\left| \\int_\\delta^1 \\frac{h}{h^2 + x^2} [f(x) - f(0)] dx \\right| \\leq h \\int_\\delta^1 \\frac{1}{x^2} |f(x) - f(0)| dx \\equiv h \\cdot M_0.\n$$\n\nTherefore, when $0 < h < \\frac{\\epsilon}{2M_0}$, we have $\\left| \\int_0^1 \\frac{h}{h^2 + x^2} [f(x) - f(0)] dx \\right| < \\frac{\\epsilon}{2} + \\frac{\\epsilon}{2} = \\epsilon.$ The proof is complete.",
        "structure": []
    },
    {
        "id": 4,
        "domain": "IMO",
        "informal": "## Goal\n\nProve that for all positive integers $n>2$, \n\n$$\nn!\\mid \\left(\\prod\\limits_{p<q\\leqslant n,}(p+q)\\right)\n$$\n\nthis only holds for $n=7.$\n\n## Proof\n\nAssume that $n$ satisfies $n!\\mid \\prod\\limits_{p<q\\leq n}(p+q)$ and let $2=p_1<p_2<\\cdots<p_m\\leq n$ be the primes in $1,2,\\cdots,n$. Each such prime divides $n!$. In particular, $p_m\\mid p_i+p_j$ for some $p_i<p_j\\leqslant n.$ But\n\n$$0<\\frac{p_i+p_j}{p_m}<\\frac{p_m+p_m}{p_m}=2,$$\n\nso $p_m=p_i+p_j$ which implies $m\\geqslant3,p_i=2$ and $p_m=2+p_j=2+p_{m-1}.$\n\nSimilarly, $p_{m-1}\\mid p_{k}+p_{l}$ for some $p_{k}<p_{l}\\leqslant n.$ But\n\n$$0<\\dfrac{p_l+p_k}{p_{m-1}}\\leqslant\\dfrac{p_m+p_{m-1}}{p_{m-1}}=\\dfrac{2p_{m-1}+2}{p_{m-1}}<3,$$\n\nso either $p_m-1=p_l+p_k$ or $2p_{m-1}=p_l+p_k.$ As above, the former case gives $p_m-1=2+p_{m-2}.$ If $2p_{m-1}=p_{l}+p_{k}$,then $p_{m-1}<p_{k}$, so $k=m$ and\n\n$$2p_{m-1}=p_l+p_{m-1}+2\\Rightarrow p_{m-1}=p_l+2=p_{m-2}+2.$$\n\nEither way, $p_{m-2} > 2$ and 3 divides one of $p_{m-2}, p_{m-1} = p_{m-2} + 2$ and $p_m = p_{m-2} + 4$. This implies $p_{m-2} = 3$ and thus $p_m = 7$, giving $7 \\leq n < 11$.\n\nFinally, a quick computation shows that $7! \\mid \\prod_{p < q \\leq 7} (p + q)$ but $8! \\nmid \\prod_{p < q \\leq 7} (p + q)$, so neither does $9!$ and $10!$.\n\n$Qed.$",
        "structure": []
    },
    {
        "id": 4,
        "domain": "Linear Algebra",
        "informal": "The next result says that we can check whether a linear map is injective by checking whether 0 is the only vector that gets mapped to 0. As a simple application of this result, we see that of the linear maps whose null spaces we computed in 3.12, only multiplication by $x^2$ is injective (except that the zero map is injective in the special case $V = \\{0\\}$).\n\n3.15 *injectivity* $\\iff$ *null space equals* $\\{0\\}$\n\nLet $T \\in \\mathcal{L}(V, W)$. Then $T$ is injective if and only if $\\operatorname{null} T = \\{0\\}$.\n\n**Proof** First suppose $T$ is injective. We want to prove that $\\operatorname{null} T = \\{0\\}$. We already know that $\\{0\\} \\subseteq \\operatorname{null} T$ (by 3.10). To prove the inclusion in the other direction, suppose $v \\in \\operatorname{null} T$. Then\n\n$$T(v) = 0 = T(0).$$\n\nBecause $T$ is injective, the equation above implies that $v = 0$. Thus we can conclude that $\\operatorname{null} T = \\{0\\}$, as desired.\n\nTo prove the implication in the other direction, now suppose $\\operatorname{null} T = \\{0\\}$. We want to prove that $T$ is injective. To do this, suppose $u, v \\in V$ and $Tu = Tv$. Then\n\n$$0 = Tu - Tv = T(u - v).$$\n\nThus $u - v$ is in $\\operatorname{null} T$, which equals $\\{0\\}$. Hence $u - v = 0$, which implies that $u = v$. Hence $T$ is injective, as desired.\n",
        "structure": []
    },
    {
        "id": 4,
        "domain": "Logic",
        "informal": "**4.2 Lemma.** *For two $S$-structures $\\mathfrak{A}$ and $\\mathfrak{B}$,*  \n$$\\mathfrak{B} \\equiv \\mathfrak{A} \\quad \\text{iff} \\quad \\mathfrak{B} \\models \\mathrm{Th}(\\mathfrak{A}).$$\n\n*Proof.* If $\\mathfrak{B} \\equiv \\mathfrak{A}$ then, since $\\mathfrak{A} \\models \\mathrm{Th}(\\mathfrak{A})$, also $\\mathfrak{B} \\models \\mathrm{Th}(\\mathfrak{A})$. Conversely, if $\\mathfrak{B} \\models \\mathrm{Th}(\\mathfrak{A})$ then, given an $S$-sentence $\\varphi$, we examine the two possibilities:  \n(i) If $\\mathfrak{A} \\models \\varphi$ then $\\varphi \\in \\mathrm{Th}(\\mathfrak{A})$ and hence $\\mathfrak{B} \\models \\varphi$.  \n(ii) If not $\\mathfrak{A} \\models \\varphi$ then $\\neg \\varphi \\in \\mathrm{Th}(\\mathfrak{A})$; thus $\\mathfrak{B} \\models \\neg \\varphi$ and therefore not $\\mathfrak{B} \\models \\varphi$.  \n$\\square$\n",
        "structure": []
    },
    {
        "id": 4,
        "domain": "Measure Theory",
        "informal": "3.26  *Bounded Convergence Theorem*\n\nSuppose $(X,S,\\mu)$ is a measure space with $\\mu(X) < \\infty$. Suppose $f_1, f_2, \\ldots$ is a sequence of $S$-measurable functions from $X$ to $\\mathbf{R}$ that converges pointwise on $X$ to a function $f : X \\to \\mathbf{R}$. If there exists $c \\in (0, \\infty)$ such that\n\n$$\n|f_k(x)| \\leq c\n$$\n\nfor all $k \\in \\mathbf{Z}^+$ and all $x \\in X$, then\n\n$$\n\\lim_{k \\to \\infty} \\int f_k \\, d\\mu = \\int f \\, d\\mu.\n$$\n\n**Proof** The function $f$ is $S$-measurable by 2.48.\n\nSuppose $c$ satisfies the hypothesis of this theorem. Let $\\varepsilon > 0$. By Egorov’s Theorem (2.85), there exists $E \\in S$ such that $\\mu(X \\setminus E) < \\frac{\\varepsilon}{4c}$ and $f_1, f_2, \\ldots$ converges uniformly to $f$ on $E$. Now\n\n$$\n\\left| \\int f_k \\, d\\mu - \\int f \\, d\\mu \\right| = \\left| \\int_{X \\setminus E} f_k \\, d\\mu - \\int_{X \\setminus E} f \\, d\\mu + \\int_E (f_k - f) \\, d\\mu \\right|\n\\leq \\int_{X \\setminus E} |f_k| \\, d\\mu + \\int_{X \\setminus E} |f| \\, d\\mu + \\int_E |f_k - f| \\, d\\mu\n< \\frac{\\varepsilon}{2} + \\mu(E) \\sup_E |f_k - f|,\n$$\n\nwhere the last inequality follows from 3.25. Because $f_1, f_2, \\ldots$ converges uniformly to $f$ on $E$ and $\\mu(E) < \\infty$, the right side of the inequality above is less than $\\varepsilon$ for $k$ sufficiently large, which completes the proof.\n",
        "structure": []
    },
    {
        "id": 4,
        "domain": "Probability Theory",
        "informal": "**Proposition 4.5.** Suppose $X_n$ converges to $X$ a.e. Then $X_n$ converges to $X$ in probability.\n\n**Proof.** There is a set $\\Lambda$ of probability zero such that if $\\omega \\notin \\Lambda$, $X_n(\\omega)$ converges to $X(\\omega)$. Consider $Z_n \\overset{\\text{def}}{=} \\sup \\{|X_k - X| : k \\geq n\\}$. Note that $X_n(\\omega) \\longrightarrow X(\\omega) \\iff Z_n(\\omega) \\longrightarrow 0$. Let $\\epsilon > 0$ and let $\\Gamma_n^\\epsilon = \\{Z_n \\geq \\epsilon\\}$. Notice that if $\\omega \\in \\bigcap_n \\Gamma_n^\\epsilon$, then $Z_n(\\omega)$ does not converge to zero, so that $\\bigcap_n \\Gamma_n^\\epsilon \\subset \\Lambda$. Now $\\Gamma_n^\\epsilon \\supset \\Gamma_{n+1}^\\epsilon \\supset \\ldots$, so $P\\{\\Gamma_n^\\epsilon\\} \\longrightarrow P\\{\\bigcap_n \\Gamma_n^\\epsilon\\} \\leq P\\{\\Lambda\\} = 0$. Thus, as $|X_n - X| \\leq Z_n$, $P\\{|X_n - X| \\geq \\epsilon\\} \\leq P\\{\\Gamma_n^\\epsilon\\} \\longrightarrow 0$. This holds for all $\\epsilon > 0$, so $X_n \\longrightarrow 0$ in probability.  □",
        "structure": []
    },
    {
        "id": 4,
        "domain": "Set Theory",
        "informal": "## Goal\n\n$R \\circ (S \\cup T) = (R \\circ S) \\cup (R \\circ T)$\n\n## Proof\n\n- Let $(x, z) \\in R \\circ (S \\cup T)$. Then there exists $y$ such that $(x, y) \\in R$ and $(y, z) \\in S \\cup T$. If $(y, z) \\in S$, then $(x, z) \\in R \\circ S$; if $(y, z) \\in T$, then $(x, z) \\in R \\circ T$. Thus, $(x, z) \\in (R \\circ S) \\cup (R \\circ T)$, so $R \\circ (S \\cup T) \\subseteq (R \\circ S) \\cup (R \\circ T)$.\n\n- Conversely, let $(x, z) \\in (R \\circ S) \\cup (R \\circ T)$. If $(x, z) \\in R \\circ S$, there exists $y$ with $(x, y) \\in R$ and $(y, z) \\in S \\subseteq S \\cup T$, so $(x, z) \\in R \\circ (S \\cup T)$. Similarly for $R \\circ T$. Hence, $(R \\circ S) \\cup (R \\circ T) \\subseteq R \\circ (S \\cup T)$.\n\n- Therefore, $R \\circ (S \\cup T) = (R \\circ S) \\cup (R \\circ T)$.",
        "structure": []
    },
    {
        "id": 4,
        "domain": "Topology Point Set",
        "informal": "**Theorem 16.3.** *If $A$ is a subspace of $X$ and $B$ is a subspace of $Y$, then the product topology on $A \\times B$ is the same as the topology $A \\times B$ inherits as a subspace of $X \\times Y$.*\n\n*Proof.* The set $U \\times V$ is the general basis element for $X \\times Y$, where $U$ is open in $X$ and $V$ is open in $Y$. Therefore, $(U \\times V) \\cap (A \\times B)$ is the general basis element for the subspace topology on $A \\times B$. Now\n\n$$(U \\times V) \\cap (A \\times \\mathbf{B}) = (U \\cap A) \\times (V \\cap \\mathbf{B}).$$\n\nSince $U \\cap A$ and $V \\cap B$ are the general open sets for the subspace topologies on $A$ and $B$, respectively, the set $(U \\cap A) \\times (V \\cap B)$ is the general basis element for the product topology on $A \\times B$.\n\nThe conclusion we draw is that the bases for the subspace topology on $A \\times B$ and for the product topology on $A \\times B$ are the same. Hence the topologies are the same. ■\n\n",
        "structure": []
    },
    {
        "id": 5,
        "domain": "Abstract Algebra",
        "informal": "**1.4.1 Factor Theorem** Any homomorphism $f$ whose kernel $K$ contains $N$ can be factored through $G/N$. In other words, in Figure 1.4.1 there is a unique homomorphism $\\overline{f} : G/N \\to H$ such that $\\overline{f} \\circ \\pi = f$. Furthermore,  \n(i) $\\overline{f}$ is an epimorphism if and only if $f$ is an epimorphism;  \n(ii) $\\overline{f}$ is a monomorphism if and only if $K = N$;  \n(iii) $\\overline{f}$ is an isomorphism if and only if $f$ is an epimorphism and $K = N$.  \n\n*Proof.* If the diagram is to commute, then $\\overline{f}(aN)$ must be $f(a)$, and it follows that $\\overline{f}$, if it exists, is unique. The definition of $\\overline{f}$ that we have just given makes sense, because if $aN = bN$, then $a^{-1}b \\in N \\subseteq K$, so $f(a^{-1}b) = 1$, and therefore $f(a) = f(b)$. Since\n\n$$\\overline{f}(aNbN) = \\overline{f}(abN) = f(ab) = f(a) f(b) = \\overline{f}(aN) \\overline{f}(bN),$$\n\n$\\overline{f}$ is a homomorphism. By construction, $\\overline{f}$ has the same image as $f$, proving (i). Now the kernel of $\\overline{f}$ is\n\n$$\\{aN : f(a) = 1\\} = \\{aN : a \\in K\\} = K/N.$$\n\nBy (1.3.13), a homomorphism is injective, i.e., a monomorphism, if and only if its kernel is trivial. Thus $\\overline{f}$ is a monomorphism if and only if $K/N$ consists only of the identity element $N$. This means that if $a$ is any element of $K$, then the coset $aN$ coincides with $N$, which forces $a$ to belong to $N$. Thus $\\overline{f}$ is a monomorphism if and only if $K = N$, proving (ii). Finally, (iii) follows immediately from (i) and (ii). ♣",
        "structure": []
    },
    {
        "id": 5,
        "domain": "Analysis",
        "informal": "**Goal:** If a monotonic sequence has a convergent subsequence, then the monotonic sequence itself is convergent.\n\n**Proof:** Without loss of generality, assume the sequence $\\{x_n\\}$ is monotonically increasing, and one of its subsequences $\\{x_{p_n}\\}$ converges to $a$. Then for any given $\\varepsilon > 0$, there exists a positive integer $N$ such that when $k > N$, $|x_{p_k} - a| < \\varepsilon$. Let $N' = p_{N+1}$. Suppose $n > N'$. Since $p_1 < p_2 < p_3 < \\cdots \\rightarrow +\\infty$, there must exist some $p_k$ (with $k > N$) such that $p_k \\leq n < p_{k+1}$. From the above, we know\n\n$\n|x_{p_k} - a| < \\varepsilon,\\; |x_{p_{k+1}} - a| < \\varepsilon.\n$\n\nAnd since $x_{p_k} \\leq x_n \\leq x_{p_{k+1}}$ (because $\\{x_n\\}$ is increasing), it must be that $|x_n - a| < \\varepsilon$. From this, it follows that $\\lim_{n \\to \\infty} x_n = a$, i.e., $\\{x_n\\}$ is convergent.",
        "structure": []
    },
    {
        "id": 5,
        "domain": "IMO",
        "informal": "## Goal\n\nLet $n$ be a positive integer, and let $x$ and $y$ be positive real numbers such that $x^n + y^n = 1$. Prove that\n$$\n\\left( \\sum_{k=1}^n \\frac{1 + x^{2k}}{1 + x^{4k}} \\right) \\left( \\sum_{k=1}^n \\frac{1 + y^{2k}}{1 + y^{4k}} \\right) < \\frac{1}{(1 - x)(1 - y)}.\n$$\n\n## Proof\n\nFor each real $t \\in (0, 1)$,\n\n$$\n\\frac{1 + t^2}{1 + t^4} = \\frac{1}{t} - \\frac{(1 - t)(1 - t^3)}{t(1 + t^4)} < \\frac{1}{t}.\n$$\n\nSubstituting $t = x^k$ and $t = y^k$,\n\n$$\n0 < \\sum_{k=1}^n \\frac{1 + x^{2k}}{1 + x^{4k}} < \\sum_{k=1}^n \\frac{1}{x^k} = \\frac{1 - x^n}{x^n (1 - x)}\n$$\n\nand\n\n$$\n0 < \\sum_{k=1}^n \\frac{1 + y^{2k}}{1 + y^{4k}} < \\sum_{k=1}^n \\frac{1}{y^k} = \\frac{1 - y^n}{y^n (1 - y)}.\n$$\n\nSince $1 - y^n = x^n$ and $1 - x^n = y^n$,\n\n$$\n\\frac{1 - x^n}{x^n (1 - x)} = \\frac{y^n}{x^n (1 - x)}, \\quad \\frac{1 - y^n}{y^n (1 - y)} = \\frac{x^n}{y^n (1 - y)},\n$$\n\nand therefore\n\n$$\n\\left( \\sum_{k=1}^n \\frac{1 + x^{2k}}{1 + x^{4k}} \\right) \\left( \\sum_{k=1}^n \\frac{1 + y^{2k}}{1 + y^{4k}} \\right) < \\frac{y^n}{x^n (1 - x)} \\cdot \\frac{x^n}{y^n (1 - y)} = \\frac{1}{(1 - x)(1 - y)}.\n$$\n$Qed.$",
        "structure": []
    },
    {
        "id": 5,
        "domain": "Linear Algebra",
        "informal": "3.21 *fundamental theorem of linear maps*\n\nSuppose $V$ is finite-dimensional and $T \\in \\mathcal{L}(V,W)$. Then range $T$ is finite-dimensional and\n\n$$\\dim V = \\dim \\operatorname{null} T + \\dim \\operatorname{range} T.$$\n\n**Proof** Let $u_1, ..., u_m$ be a basis of null $T$; thus $\\dim \\operatorname{null} T = m$. The linearly independent list $u_1, ..., u_m$ can be extended to a basis\n\n$$u_1, ..., u_m, v_1, ..., v_n$$\n\nof $V$ (by 2.32). Thus $\\dim V = m + n$. To complete the proof, we need to show that range $T$ is finite-dimensional and $\\dim \\operatorname{range} T = n$. We will do this by proving that $Tv_1, ..., Tv_n$ is a basis of range $T$.\n\nLet $v \\in V$. Because $u_1, ..., u_m, v_1, ..., v_n$ spans $V$, we can write\n\n$$v = a_1 u_1 + \\cdots + a_m u_m + b_1 v_1 + \\cdots + b_n v_n,$$\n\nwhere the $a$'s and $b$'s are in $\\mathbf{F}$. Applying $T$ to both sides of this equation, we get\n\n$$Tv = b_1 T v_1 + \\cdots + b_n T v_n,$$\n\nwhere the terms of the form $T u_k$ disappeared because each $u_k$ is in null $T$. The last equation implies that the list $Tv_1, ..., Tv_n$ spans range $T$. In particular, range $T$ is finite-dimensional.\n\nTo show $Tv_1, ..., Tv_n$ is linearly independent, suppose $c_1, ..., c_n \\in \\mathbf{F}$ and\n\n$$c_1 T v_1 + \\cdots + c_n T v_n = 0.$$\n\nThen\n\n$$T(c_1 v_1 + \\cdots + c_n v_n) = 0.$$\n\nHence\n\n$$c_1 v_1 + \\cdots + c_n v_n \\in \\operatorname{null} T.$$\n\nBecause $u_1, ..., u_m$ spans null $T$, we can write\n\n$$c_1 v_1 + \\cdots + c_n v_n = d_1 u_1 + \\cdots + d_m u_m,$$\n\nwhere the $d$'s are in $\\mathbf{F}$. This equation implies that all the $c$'s (and $d$'s) are 0 (because $u_1, ..., u_m, v_1, ..., v_n$ is linearly independent). Thus $Tv_1, ..., Tv_n$ is linearly independent and hence is a basis of range $T$, as desired.\n",
        "structure": []
    },
    {
        "id": 5,
        "domain": "Logic",
        "informal": "**6.11 $\\beta$-Function Lemma$^3$.** *There is a function $\\beta: \\mathbb{N}^3 \\to \\mathbb{N}$ with the following properties:*\n\n(a) *For every sequence $(a_0, \\ldots, a_r)$ over $\\mathbb{N}$ there exist $t,p \\in \\mathbb{N}$ such that for all $i \\le r$*  \n$$\\beta(t,p,i) = a_i.$$\n\n(b) *$\\beta$ is definable in $L^{S_{ar}}$, i.e. there exists an $S_{ar}$-formula $\\varphi_\\beta(v_0, v_1, v_2, v_3)$ such that for all $t,p,i,a \\in \\mathbb{N}$,*  \n$$\\mathbb{N} \\models \\varphi_\\beta[t,p,i,a] \\quad \\text{iff} \\quad \\beta(t,p,i) = a.$$\n\n*Proof.* Given $(a_0, \\ldots, a_r)$, we choose a prime $p$ which is larger than $a_0, \\ldots, a_r, r+1$ and set  \n$$(*) \\quad t := 1 \\cdot p^0 + a_0 p^1 + 2 p^2 + a_1 p^3 + \\ldots + (i+1) p^{2i} + a_i p^{2i+1} + \\ldots + (r+1) p^{2r} + a_r p^{2r+1}.$$\n\nBy choice of $p$ the right-hand side is the $p$-adic representation of $t$.\n\nFirst, we show that for all $i$, $0 \\le i \\le r$,  \n$$(**) \\quad a = a_i \\quad \\text{iff} \\quad \\text{there are } b_0, b_1, b_2 \\text{ such that}$$  \n(i) $t = b_0 + b_1 ((i+1) + a p + b_2 p^2)$,  \n(ii) $a < p$,  \n(iii) $b_0 < b_1$,  \n(iv) $b_1 = p^{2m}$ for a suitable $m$.\n\nThe implication from left to right follows immediately from $(*)$ with  \n$$b_0 := 1 \\cdot p^0 + \\ldots + a_{i-1} p^{2i-1},$$  \n$$b_1 := p^{2i},$$  \n$$b_2 := (i+2) + a_{i+1} p + \\ldots + a_r p^{2(r-i)-1}.$$\n\nConversely, suppose (i) – (iv) hold for $b_0, b_1, b_2$ and let $b_1 = p^{2m}$. From (i) we obtain  \n$$t = b_0 + (i+1) p^{2m} + a p^{2m+1} + b_2 p^{2m+2}.$$\n\nSince $b_0 < p^{2m}$, $a < p$, and $i+1 < p$, and since the $p$-adic representation of $t$ is unique, a comparison with $(*)$ yields $m = i$ and $a = a_i$.\n\nObviously, (iv) from $(**)$ is equivalent to  \n(iv)$'$ $b_1$ is a square and for all $d \\neq 1$ with $d \\mid b_1$ we have $p \\nmid d$.\n\nWe define $\\beta(t,p,i)$ to be the uniquely determined (and hence the smallest) $a$ for which the right-hand side of $(**)$ (with (iv)$'$ instead of (iv)) holds. We extend this definition to arbitrary triples of natural numbers by specifying:\n\nLet $\\beta(u,q,j)$ be the smallest $a$ such that there are $b_0, b_1, b_2$ with  \n(i) $u = b_0 + b_1 ((j+1) + a q + b_2 q^2)$,  \n(ii) $a < q$,  \n(iii) $b_0 < b_1$,  \n(iv)$'$ $b_1$ is a square, and for all $d \\neq 1$ with $d \\mid b_1$ we have $q \\nmid d$.\n\nIf no such $a$ exists let $\\beta(u,q,j) = 0$.\n\nThen $\\beta$ has the properties required in (a).\n\nThe definition of $\\beta$ just given leads immediately to an $S_{ar}$-formula $\\varphi_\\beta(v_0, v_1, v_2, v_3)$ defining $\\beta$. So (b) holds as well.  \n$\\square$\n\n",
        "structure": []
    },
    {
        "id": 5,
        "domain": "Measure Theory",
        "informal": "3.31  *Dominated Convergence Theorem*\n\nSuppose $(X,S,\\mu)$ is a measure space, $f : X \\to [-\\infty, \\infty]$ is $S$-measurable, and $f_1, f_2, \\ldots$ are $S$-measurable functions from $X$ to $[-\\infty, \\infty]$ such that\n\n$$\n\\lim_{k \\to \\infty} f_k(x) = f(x)\n$$\n\nfor almost every $x \\in X$. If there exists an $S$-measurable function $g : X \\to [0, \\infty]$ such that\n\n$$\n\\int g \\, d\\mu < \\infty \\quad \\text{and} \\quad |f_k(x)| \\leq g(x)\n$$\n\nfor every $k \\in \\mathbf{Z}^+$ and almost every $x \\in X$, then\n\n$$\n\\lim_{k \\to \\infty} \\int f_k \\, d\\mu = \\int f \\, d\\mu.\n$$\n\n**Proof** Suppose $g : X \\to [0, \\infty]$ satisfies the hypotheses of this theorem. If $E \\in S$, then\n\n$$\n\\left| \\int f_k \\, d\\mu - \\int f \\, d\\mu \\right| = \\left| \\int_{X \\setminus E} f_k \\, d\\mu - \\int_{X \\setminus E} f \\, d\\mu + \\int_E f_k \\, d\\mu - \\int_E f \\, d\\mu \\right|\n\\leq \\left| \\int_{X \\setminus E} f_k \\, d\\mu \\right| + \\left| \\int_{X \\setminus E} f \\, d\\mu \\right| + \\left| \\int_E f_k \\, d\\mu - \\int_E f \\, d\\mu \\right|\n\\leq 2 \\int_{X \\setminus E} g \\, d\\mu + \\left| \\int_E f_k \\, d\\mu - \\int_E f \\, d\\mu \\right|.\n\\tag{3.32}\n$$\n\n**Case 1:** Suppose $\\mu(X) < \\infty$.  \nLet $\\varepsilon > 0$. By 3.28, there exists $\\delta > 0$ such that\n\n$$\n\\int_B g \\, d\\mu < \\frac{\\varepsilon}{4}\n\\tag{3.33}\n$$\n\nfor every set $B \\in S$ such that $\\mu(B) < \\delta$. By Egorov’s Theorem (2.85), there exists a set $E \\in S$ such that $\\mu(X \\setminus E) < \\delta$ and $f_1, f_2, \\ldots$ converges uniformly to $f$ on $E$. Now 3.32 and 3.33 imply that\n\n$$\n\\left| \\int f_k \\, d\\mu - \\int f \\, d\\mu \\right| < \\frac{\\varepsilon}{2} + \\left| \\int_E (f_k - f) \\, d\\mu \\right|.\n$$\n\nBecause $f_1, f_2, \\ldots$ converges uniformly to $f$ on $E$ and $\\mu(E) < \\infty$, the last term on the right is less than $\\frac{\\varepsilon}{2}$ for all sufficiently large $k$. Thus $\\lim_{k \\to \\infty} \\int f_k \\, d\\mu = \\int f \\, d\\mu$, completing the proof of case 1.\n\n**Case 2:** Suppose $\\mu(X) = \\infty$.  \nLet $\\varepsilon > 0$. By 3.29, there exists $E \\in S$ such that $\\mu(E) < \\infty$ and\n\n$$\n\\int_{X \\setminus E} g \\, d\\mu < \\frac{\\varepsilon}{4}.\n$$\n\nThe inequality above and 3.32 imply that\n\n$$\n\\left| \\int f_k \\, d\\mu - \\int f \\, d\\mu \\right| < \\frac{\\varepsilon}{2} + \\left| \\int_E f_k \\, d\\mu - \\int_E f \\, d\\mu \\right|.\n$$\n\nBy case 1 as applied to the sequence $f_1|_E, f_2|_E, \\ldots$, the last term on the right is less than $\\frac{\\varepsilon}{2}$ for all sufficiently large $k$. Thus $\\lim_{k \\to \\infty} \\int f_k \\, d\\mu = \\int f \\, d\\mu$, completing the proof of case 2.\n",
        "structure": []
    },
    {
        "id": 5,
        "domain": "Probability Theory",
        "informal": "**Theorem 4.9** (Borel-Cantelli Lemma). Let $\\Lambda_1, \\Lambda_2, \\ldots$ be a sequence of events. Suppose that $\\sum_{n=1}^\\infty P\\{\\Lambda_n\\} < \\infty$. Then\n\n$$\nP\\{\\limsup_n \\Lambda_n\\} = 0.\n$$\n\nConversely, if the $\\Lambda_n$ are independent, there are only two possibilities:\n\n$$\nP\\{\\limsup_n \\Lambda_n\\} = \\begin{cases}\n0 & \\iff \\sum_{n=1}^\\infty P\\{\\Lambda_n\\} < \\infty, \\\\\n1 & \\iff \\sum_{n=1}^\\infty P\\{\\Lambda_n\\} = \\infty.\n\\end{cases}\n$$\n\n**Proof.** The lim sup of the $\\Lambda_n$ can be written as $\\lim_{m \\to \\infty} \\bigcup_{n=m}^\\infty \\Lambda_n$. Notice that the union decreases as $m$ increases, so\n\n$$\nP\\{\\limsup_n \\Lambda_n\\} = \\lim_{m \\to \\infty} P\\left\\{\\bigcup_{n=m}^\\infty \\Lambda_n\\right\\}\n\\leq \\lim_{m \\to \\infty} \\sum_{n=m}^\\infty P\\{\\Lambda_n\\}.\n$$\n\nBut this is the tail of a convergent series (the series $\\sum_n P\\{\\Lambda_n\\}$ converges by hypothesis) so it converges to zero.\n\nFor the converse, consider\n\n$$\nP\\{\\limsup_n \\Lambda_n\\} = 1 - P\\{\\liminf_n \\Lambda_n^c\\}\n= 1 - \\lim_{m \\to \\infty} P\\left\\{\\bigcap_{n=m}^\\infty \\Lambda_n^c\\right\\}\n= 1 - \\lim_{m \\to \\infty} \\prod_{n=m}^\\infty P\\{\\Lambda_n^c\\}.\n$$\n\nBut if $0 < x_n < 1$, $\\prod_n (1 - x_n)$ is either zero or strictly positive according to whether $\\sum_n x_n = \\infty$ or $\\sum_n x_n < \\infty$, respectively. If the sum is finite, $\\lim_{m \\to \\infty} \\prod_{n=m}^\\infty (1 - x_n) = 1$. Thus, the probability of the lim sup equals\n\n$$\n1 - \\lim_{m \\to \\infty} \\prod_{n=m}^\\infty (1 - P\\{\\Lambda_n\\}) = \\begin{cases}\n1 & \\text{if } \\sum_n P\\{\\Lambda_n\\} = \\infty, \\\\\n0 & \\text{if } \\sum_n P\\{\\Lambda_n\\} < \\infty.\n\\end{cases}\n$$\n\n□\n",
        "structure": []
    },
    {
        "id": 5,
        "domain": "Set Theory",
        "informal": "## Goal\n\n$R$ is antisymmetric $\\iff R \\cap R^{-1} \\subseteq I_A$\n\n## Proof\n\n  - ($\\Rightarrow$) If $R$ is antisymmetric, then $(a, b) \\in R \\cap R^{-1}$ implies $(a, b) \\in R$ and $(b, a) \\in R$, so $a = b$. Hence, $(a, b) \\in I_A$, proving $R \\cap R^{-1} \\subseteq I_A$.\n\n  - ($\\Leftarrow$) If $R \\cap R^{-1} \\subseteq I_A$, then whenever $(a, b) \\in R$ and $(b, a) \\in R$, we have $(a, b) \\in I_A$, so $a = b$. Thus, $R$ is antisymmetric.",
        "structure": []
    },
    {
        "id": 5,
        "domain": "Topology Point Set",
        "informal": "**Theorem 17.9.** *Let $X$ be a space satisfying the $T_1$ axiom; let $A$ be a subset of $X$. Then the point $x$ is a limit point of $A$ if and only if every neighborhood of $x$ contains infinitely many points of $A$.*\n\n*Proof.* If every neighborhood of $x$ intersects $A$ in infinitely many points, it certainly intersects $A$ in some point other than $x$ itself, so that $x$ is a limit point of $A$.\n\nConversely, suppose that $x$ is a limit point of $A$, and suppose some neighborhood $U$ of $x$ intersects $A$ in only finitely many points. Then $U$ also intersects $A - \\{x\\}$ in finitely many points; let $\\{x_1, \\ldots, x_m\\}$ be the points of $U \\cap (A - \\{x\\})$. The set $X - \\{x_1, \\ldots, x_m\\}$ is an open set of $X$, since the finite point set $\\{x_1, \\ldots, x_m\\}$ is closed; then\n\n$$U \\cap (X - \\{x_1, \\ldots, x_m\\})$$\n\nis a neighborhood of $x$ that intersects the set $A - \\{x\\}$ not at all. This contradicts the assumption that $x$ is a limit point of $A$. ■\n\n\n",
        "structure": []
    },
    {
        "id": 6,
        "domain": "Abstract Algebra",
        "informal": "**1.4.4 Second Isomorphism Theorem** If $H$ and $N$ are subgroups of $G$, with $N$ normal in $G$, then\n\n$$H / (H \\cap N) \\cong HN / N.$$\n\nNote that we write $HN / N$ rather than $H / N$, since $N$ need not be a subgroup of $H$.\n\n*Proof.* Let $\\pi$ be the canonical epimorphism from $G$ to $G / N$, and let $\\pi_0$ be the restriction of $\\pi$ to $H$. Then the kernel of $\\pi_0$ is $H \\cap N$, so by the first isomorphism theorem, $H / (H \\cap N)$ is isomorphic to the image of $\\pi_0$, which is $\\{hN : h \\in H\\} = HN / N$. (To justify the last equality, note that for any $n \\in N$ we have $hnN = hN$.) ♣",
        "structure": []
    },
    {
        "id": 6,
        "domain": "Analysis",
        "informal": "## Goal\n\nIf a function $ f(x) $ is continuous on the interval $[a, b]$ and $ f(a) \\cdot f(b) < 0 $, then there exists a $ \\xi \\in (a, b) $ such that $ f(\\xi) = 0 $.\n\n**Proof**: Without loss of generality, assume $ f(a) < 0 $ and $ f(b) > 0 $. Define the set $ V $ as:\n\n$$\nV = \\{ x \\mid f(x) < 0, x \\in [a, b] \\}.\n$$\n\nClearly, the set $ V $ is bounded and non-empty, so it must have a supremum. Let\n\n$$\n\\xi = \\sup V.\n$$\n\nWe will show that $ \\xi \\in (a, b) $ and $ f(\\xi) = 0 $.\n\nDue to the continuity of $ f(x) $ and the fact that $ f(a) < 0 $, there exists $ \\delta_1 > 0 $ such that for all $ x \\in [a, a + \\delta_1] $, $ f(x) < 0 $. Similarly, since $ f(b) > 0 $, there exists $ \\delta_2 > 0 $ such that for all $ x \\in [b - \\delta_2, b] $, $ f(x) > 0 $. Therefore, we have\n\n$$\na + \\delta_1 \\leq \\xi \\leq b - \\delta_2,\n$$\n\nwhich implies $ \\xi \\in (a, b) $.\n\nTake $ x_n \\in V $ for $ n = 1, 2, \\ldots $ such that $ x_n \\to \\xi $ as $ n \\to \\infty $. Since $ f(x_n) < 0 $, we have\n\n$$\nf(\\xi) = \\lim_{n \\to \\infty} f(x_n) \\leq 0.\n$$\n\nIf $ f(\\xi) < 0 $, due to the continuity of $ f(x) $ at $ \\xi $, there exists $ \\delta > 0 $ such that for all $ x \\in (\\xi - \\delta, \\xi + \\delta) $,\n\n$$\nf(x) < 0,\n$$\n\nwhich contradicts the fact that $ \\xi = \\sup V $. Therefore, it must be that\n\n$$\nf(\\xi) = 0.\n$$\n\n**Q.E.D.**",
        "structure": []
    },
    {
        "id": 6,
        "domain": "IMO",
        "informal": "## Goal\n\nLet $a_1, a_2, \\ldots, a_{100}$ be nonnegative real numbers such that $a_1^2 + a_2^2 + \\cdots + a_{100}^2 = 1$. Prove that\n$$\na_1^2 a_2 + a_2^2 a_3 + \\cdots + a_{100}^2 a_1 < \\frac{12}{25}.\n$$\n\n## Proof\n\nLet $S = \\sum_{k=1}^{100} a_k^2 a_{k+1}$. (As usual, we consider the indices modulo 100, e.g. we set $a_{101} = a_1$ and $a_{102} = a_2$.)\n\nApplying the Cauchy-Schwarz inequality to sequences $(a_{k+1})$ and $(a_k^2 + 2a_k a_{k+1} a_{k+2})$, and then the AM-GM inequality to numbers $a_k^2 a_{k+1}$ and $a_k^2 a_{k+2}$,\n\n$$\n(3S)^2 = \\left( \\sum_{k=1}^{100} a_{k+1} (a_k^2 + 2a_k a_{k+1} a_{k+2}) \\right)^2 \\leq \\left( \\sum_{k=1}^{100} a_{k+1}^2 \\right) \\left( \\sum_{k=1}^{100} (a_k^2 + 2a_k a_{k+1} a_{k+2})^2 \\right)\n$$\n\n$$\n= 1 \\cdot \\sum_{k=1}^{100} (a_k^4 + 4a_k^2 a_{k+1} a_k a_{k+2} + 4a_k^2 a_{k+1}^2) \n$$\n\n$$\n\\leq \\sum_{k=1}^{100} (a_k^4 + 2a_k^2 (a_k^2 + a_{k+2}^2) + 4a_k^2 a_{k+1}^2) = \\sum_{k=1}^{100} (a_k^4 + 6a_k^2 a_{k+1}^2 + 2a_k^2 a_{k+2}^2).\n$$\n\nApplying the trivial estimates\n\n$$\n\\sum_{k=1}^{100} (a_k^4 + 2a_k^2 a_{k+1}^2 + 2a_k^2 a_{k+2}^2) \\leq \\left( \\sum_{k=1}^{100} a_k^2 \\right)^2 \n$$\n\nand \n\n$$\n\\sum_{k=1}^{100} a_{2i-1}^2 a_{2i}^2 \\leq \\left( \\sum_{i=1}^{50} a_{2i-1}^2 \\right) \\left( \\sum_{j=1}^{50} a_{2j}^2 \\right),\n$$\n\nwe obtain that\n\n$$\n(3S)^2 \\leq \\left( \\sum_{k=1}^{100} a_k^2 \\right)^2 + 4 \\left( \\sum_{i=1}^{50} a_{2i-1}^2 \\right) \\left( \\sum_{j=1}^{50} a_{2j}^2 \\right) \\leq 1 + \\left( \\sum_{i=1}^{50} a_{2i-1}^2 + \\sum_{j=1}^{50} a_{2j}^2 \\right)^2 = 2,\n$$\n\nhence\n\n$$\nS \\leq \\frac{\\sqrt{2}}{3} \\approx 0.4714 < \\frac{12}{25} = 0.48.\n$$\n\n$Qed.$",
        "structure": []
    },
    {
        "id": 6,
        "domain": "Linear Algebra",
        "informal": "3.56 *column–row factorization*\n\nSuppose $A$ is an $m$-by-$n$ matrix with entries in $\\mathbf{F}$ and column rank $c \\geq 1$. Then there exist an $m$-by-$c$ matrix $C$ and a $c$-by-$n$ matrix $R$, both with entries in $\\mathbf{F}$, such that $A = CR$.\n\n**Proof** Each column of $A$ is an $m$-by-1 matrix. The list $A_{\\cdot,1}, ..., A_{\\cdot,n}$ of columns of $A$ can be reduced to a basis of the span of the columns of $A$ (by 2.30). This basis has length $c$, by the definition of the column rank. The $c$ columns in this basis can be put together to form an $m$-by-$c$ matrix $C$.\n\nIf $k \\in \\{1, ..., n\\}$, then column $k$ of $A$ is a linear combination of the columns of $C$. Make the coefficients of this linear combination into column $k$ of a $c$-by-$n$ matrix that we call $R$. Then $A = CR$, as follows from 3.51(a).\n",
        "structure": []
    },
    {
        "id": 6,
        "domain": "Logic",
        "informal": "**7.5 Fixed Point Theorem.** *Suppose $\\Phi$ allows representations. Then, for every $\\psi \\in L^{S_{ar}}_1$, there is an $S_{ar}$-sentence $\\varphi$ such that*  \n$$\\Phi \\vdash \\varphi \\leftrightarrow \\psi(\\mathfrak{n}^\\varphi).$$\n\nIntuitively, $\\varphi$ says: \"I have the property $\\psi$.\"\n\n*Proof.* Let $F: \\mathbb{N} \\times \\mathbb{N} \\to \\mathbb{N}$ be given by  \n$$F(n,m) = \\begin{cases} n^{\\chi(m)}, & \\text{if } n = n^\\chi \\text{ for some } \\chi \\in L^{S_{ar}}_1; \\\\ 0, & \\text{otherwise}. \\end{cases}$$\n\nClearly, $F$ is computable, and for $\\chi \\in L^{S_{ar}}_1$ we have  \n$$F(n^\\chi, m) = n^{\\chi(m)}.$$\n\nSince $\\Phi$ allows representations, $F$ can be represented in $\\Phi$ by a suitable $S_{ar}$-formula $\\alpha(v_0, v_1, v_2)$. We write $x,y,z$ for $v_0, v_1, v_2$. For given $\\psi \\in L^{S_{ar}}_1$ we set  \n$$\\beta := \\forall z (\\alpha(x, x, z) \\to \\psi(z)), \\quad \\varphi := \\forall z (\\alpha(n^\\beta, n^\\beta, z) \\to \\psi(z)).$$\n\nSince $\\beta \\in L^{S_{ar}}_1$ and $\\varphi = \\beta \\mathfrak{n}^\\beta_x$, we have $F(n^\\beta, n^\\beta) = n^\\varphi$ and hence  \n$$(1) \\quad \\Phi \\vdash \\alpha(n^\\beta, n^\\beta, n^\\varphi).$$\n\nNow we show the claim for $\\varphi$ and $\\psi$, i.e.  \n$$\\Phi \\vdash \\varphi \\leftrightarrow \\psi(n^\\varphi).$$\n\nFor the direction from left to right, we have by definition of $\\varphi$ that  \n$$\\Phi \\cup \\{\\varphi\\} \\vdash \\alpha(n^\\beta, n^\\beta, n^\\varphi) \\to \\psi(n^\\varphi),$$  \nby (1) therefore $\\Phi \\vdash \\varphi \\to \\psi(n^\\varphi)$.\n\nOn the other hand, $\\alpha$ represents the function $F$ in $\\Phi$, in particular  \n$$\\Phi \\vdash \\exists^{=1} z \\alpha(n^\\beta, n^\\beta, z);$$  \nthus by (1)  \n$$\\Phi \\vdash \\forall z (\\alpha(n^\\beta, n^\\beta, z) \\to z \\equiv n^\\varphi)$$  \nand therefore  \n$$\\Phi \\vdash \\psi(n^\\varphi) \\to \\forall z (\\alpha(n^\\beta, n^\\beta, z) \\to \\psi(z)),$$  \nthat is  \n$$\\Phi \\vdash \\psi(n^\\varphi) \\to \\varphi.$$  \n$\\square$\n\n",
        "structure": []
    },
    {
        "id": 6,
        "domain": "Measure Theory",
        "informal": "Now we come to some easy but important properties of $\\sigma$-algebras.\n\n2.25  *$\\sigma$-algebras are closed under countable intersection*\n\nSuppose $S$ is a $\\sigma$-algebra on a set $X$. Then\n\n(a) $X \\in S$;\n\n(b) if $D, E \\in S$, then $D \\cup E \\in S$ and $D \\cap E \\in S$ and $D \\setminus E \\in S$;\n\n(c) if $E_1, E_2, \\ldots$ is a sequence of elements of $S$, then $\\bigcap_{k=1}^\\infty E_k \\in S$.\n\n**Proof** Because $\\emptyset \\in S$ and $X = X \\setminus \\emptyset$, the first two bullet points in the definition of $\\sigma$-algebra (2.23) imply that $X \\in S$, proving (a).\n\nSuppose $D, E \\in S$. Then $D \\cup E$ is the union of the sequence $D, E, \\emptyset, \\emptyset, \\ldots$ of elements of $S$. Thus the third bullet point in the definition of $\\sigma$-algebra (2.23) implies that $D \\cup E \\in S$.\n\nDe Morgan’s Laws tell us that\n\n$$\nX \\setminus (D \\cap E) = (X \\setminus D) \\cup (X \\setminus E).\n$$\n\nIf $D, E \\in S$, then the right side of the equation above is in $S$; hence $X \\setminus (D \\cap E) \\in S$; thus the complement in $X$ of $X \\setminus (D \\cap E)$ is in $S$; in other words, $D \\cap E \\in S$.\n\nBecause $D \\setminus E = D \\cap (X \\setminus E)$, we see that if $D, E \\in S$, then $D \\setminus E \\in S$, completing the proof of (b).\n\nFinally, suppose $E_1, E_2, \\ldots$ is a sequence of elements of $S$. De Morgan’s Laws tell us that\n\n$$\nX \\setminus \\bigcap_{k=1}^\\infty E_k = \\bigcup_{k=1}^\\infty (X \\setminus E_k).\n$$\n\nThe right side of the equation above is in $S$. Hence the left side is in $S$, which implies that $X \\setminus \\left( X \\setminus \\bigcap_{k=1}^\\infty E_k \\right) \\in S$. In other words, $\\bigcap_{k=1}^\\infty E_k \\in S$, proving (c).\n\n",
        "structure": []
    },
    {
        "id": 6,
        "domain": "Probability Theory",
        "informal": "**Theorem 5.1** (Weak Law of Large Numbers). Let $X_1, X_2, \\ldots$ be a sequence of independent, mean zero random variables. Suppose the variance of $X_i$ is less than or equal to $\\sigma^2$. Then\n\n$$\n\\lim_{n \\to \\infty} \\frac{1}{n} S_n = 0 \\quad \\text{in probability}.\n$$\n\n**Proof.** \n$$\nE \\left\\{ \\left(\\frac{1}{n} \\sum_{i=1}^n X_i \\right)^2 \\right\\} = \\frac{1}{n^2} \\sum_{i=1}^n E\\{X_i^2\\} \\leq \\frac{1}{n^2} n \\sigma^2 = \\frac{\\sigma^2}{n}.\n$$\n\nThus by Chebyshev's inequality, if $\\epsilon > 0$, then\n\n$$\nP \\left\\{ \\frac{1}{n} |S_n| \\geq \\epsilon \\right\\} \\leq \\frac{\\sigma^2}{n \\epsilon^2} \\longrightarrow 0.\n$$\n\nThus $\\lim_{n \\to \\infty} \\frac{1}{n} S_n = 0$ in probability, as claimed.  □\n",
        "structure": []
    },
    {
        "id": 6,
        "domain": "Set Theory",
        "informal": "## Goal\n\nIf $R$ is an equivalence relation on $A$ and $a, b \\in A$, then $aRb$ if and only if $[a]_R = [b]_R$.\n\n## Proof\n\n(⇒) Assume $aRb$. We show $[a]_R = [b]_R$ by mutual inclusion.\n\n- $[a]_R \\subseteq [b]_R$: Let $c \\in [a]_R$. By definition, $cRa$. Since $R$ is symmetric, $aRb$ implies $bRa$. By transitivity of $R$, $cRa$ and $aRb$ yield $cRb$. Hence, $c \\in [b]_R$.\n\n- $[b]_R \\subseteq [a]_R$: Let $d \\in [b]_R$. By definition, $dRb$. From $aRb$ and symmetry, $bRa$. By transitivity, $dRb$ and $bRa$ imply $dRa$. Hence, $d \\in [a]_R$.\n\nThus, $[a]_R = [b]_R$.\n\n(⇐) Assume $[a]_R = [b]_R$. Since $R$ is reflexive, $a \\in [a]_R$. Therefore, $a \\in [b]_R$, which by definition means $aRb$.\n\nHence, $aRb$ if and only if $[a]_R = [b]_R$. \n\nQed.",
        "structure": []
    },
    {
        "id": 6,
        "domain": "Topology Point Set",
        "informal": "**Theorem 18.2 (Rules for constructing continuous functions).** Let $X$, $Y$, and $Z$ be topological spaces.\n\n(a) (Constant function) If $f : X \\to Y$ maps all of $X$ into the single point $y_0$ of $Y$, then $f$ is continuous.\n\n(b) (Inclusion) If $A$ is a subspace of $X$, the inclusion function $j : A \\to X$ is continuous.\n\n(c) (Composites) If $f : X \\to Y$ and $g : Y \\to Z$ are continuous, then the map $g \\circ f : X \\to Z$ is continuous.\n\n(d) (Restricting the domain) If $f : X \\to Y$ is continuous, and if $A$ is a subspace of $X$, then the restricted function $f|A : A \\to Y$ is continuous.\n\n(e) (Restricting or expanding the range) Let $f : X \\to Y$ be continuous. If $Z$ is a subspace of $Y$ containing the image set $f(X)$, then the function $g : X \\to Z$ obtained by restricting the range of $f$ is continuous. If $Z$ is a space having $Y$ as a subspace, then the function $h : X \\to Z$ obtained by expanding the range of $f$ is continuous.\n\n(f) (Local formulation of continuity) The map $f : X \\to Y$ is continuous if $X$ can be written as the union of open sets $U_\\alpha$ such that $f|U_\\alpha$ is continuous for each $\\alpha$.\n\n*Proof.* (a) Let $f(x) = y_0$ for every $x$ in $X$. Let $V$ be open in $Y$. The set $f^{-1}(V)$ equals $X$ or $\\emptyset$, depending on whether $V$ contains $y_0$ or not. In either case, it is open.\n\n(b) If $U$ is open in $X$, then $j^{-1}(U) = U \\cap A$, which is open in $A$ by definition of the subspace topology.\n\n(c) If $U$ is open in $Z$, then $g^{-1}(U)$ is open in $Y$ and $f^{-1}(g^{-1}(U))$ is open in $X$. But\n\n$$f^{-1}(g^{-1}(U)) = (g \\circ f)^{-1}(U),$$\n\nby elementary set theory.\n\n(d) The function $f|A$ equals the composite of the inclusion map $j : A \\to X$ and the map $f : X \\to Y$, both of which are continuous.\n\n(e) Let $f : X \\to Y$ be continuous. If $f(X) \\subset Z \\subset Y$, we show that the function $g : X \\to Z$ obtained from $f$ is continuous. Let $B$ be open in $Z$. Then $B = Z \\cap U$ for some open set $U$ of $Y$. Because $Z$ contains the entire image set $f(X)$,\n\n$$f^{-1}(U) = g^{-1}(B),$$\n\nby elementary set theory. Since $f^{-1}(U)$ is open, so is $g^{-1}(B)$.\n\nTo show $h : X \\to Z$ is continuous if $Z$ has $Y$ as a subspace, note that $h$ is the composite of the map $f : X \\to Y$ and the inclusion map $j : Y \\to Z$.\n\n(f) By hypothesis, we can write $X$ as a union of open sets $U_\\alpha$, such that $f|U_\\alpha$ is continuous for each $\\alpha$. Let $V$ be an open set in $Y$. Then\n\n$$f^{-1}(V) \\cap U_\\alpha = (f|U_\\alpha)^{-1}(V),$$\n\nbecause both expressions represent the set of those points $x$ lying in $U_\\alpha$ for which $f(x) \\in V$. Since $f|U$ is continuous, this set is open in $U_\\alpha$, and hence open in $X$. But\n\n$$f^{-1}(V) = \\bigcup_\\alpha (f^{-1}(V) \\cap U_\\alpha),$$\n\nso that $f^{-1}(V)$ is also open in $X$. ■\n\n\n\n",
        "structure": []
    },
    {
        "id": 7,
        "domain": "Abstract Algebra",
        "informal": "**1.4.5 Third Isomorphism Theorem** If $N$ and $H$ are normal subgroups of $G$, with $N$ contained in $H$, then\n\n$$G / H \\cong (G / N) / (H / N),$$\n\na “cancellation law”.\n\n*Proof.* This will follow directly from the first isomorphism theorem if we can find an epimorphism of $G / N$ onto $G / H$ with kernel $H / N$, and there is a natural candidate: $f(aN) = aH$. To check that $f$ is well-defined, note that if $aN = bN$ then $a^{-1} b \\in N \\subseteq H$, so $aH = bH$. Since $a$ is an arbitrary element of $G$, $f$ is surjective, and by definition of coset multiplication, $f$ is a homomorphism. But the kernel of $f$ is\n\n$$\\{aN : aH = H\\} = \\{aN : a \\in H\\} = H / N.$$ ♣",
        "structure": []
    },
    {
        "id": 7,
        "domain": "Analysis",
        "informal": "Let $\\{x_n\\}$ and $\\{y_n\\}$ be numerical sequences. Prove that if $\\lim_{n \\to \\infty} x_n = A$ and $\\lim_{n \\to \\infty} y_n = B$, then $\\lim_{n \\to \\infty} (x_n + y_n) = A + B$.\n\n$Proof.$\n\nDenote $|A - x_n| = \\Delta(x_n)$, $|B - y_n| = \\Delta(y_n)$. Then we have\n\n$$\n|(A + B) - (x_n + y_n)| \\leq \\Delta(x_n) + \\Delta(y_n).\n$$\n\nSuppose $\\varepsilon > 0$ is given. Since $\\lim_{n \\to \\infty} x_n = A$, there exists $N'$ such that $\\Delta(x_n) < \\varepsilon / 2$ for all $n > N'$. Similarly, since $\\lim_{n \\to \\infty} y_n = B$, there exists $N''$ such that $\\Delta(y_n) < \\varepsilon / 2$ for all $n > N''$. Then for $n > \\max\\{N', N''\\}$ we shall have\n\n$$\n|(A + B) - (x_n + y_n)| < \\varepsilon,\n$$\n\nwhich, by definition of limit, gives us $\\lim_{n \\to \\infty} (x_n + y_n) = A + B$.\n\n$Qed.$",
        "structure": []
    },
    {
        "id": 7,
        "domain": "IMO",
        "informal": "## Goal\n\nLet $b, n > 1$ be integers. Suppose that for each $k > 1$ there exists an integer $a_k$ such that $b - a_k^n$ is divisible by $k$. Prove that $b = A^n$ for some integer $A$.\n\n## Proof\n\nLet the prime factorization of $b$ be $b = p_1^{\\alpha_1} \\cdots p_s^{\\alpha_s}$, where $p_1, \\ldots, p_s$ are distinct primes. Our goal is to show that all exponents $\\alpha_i$ are divisible by $n$, then we can set $A = p_1^{\\alpha_1/n} \\cdots p_s^{\\alpha_s/n}$.\n\nApply the condition for $k = b^2$. The number $b - a_k^n$ is divisible by $b^2$ and hence, for each $1 \\leq i \\leq s$, it is divisible by $p_i^{2\\alpha_i} > p_i^{\\alpha_i}$ as well. Therefore\n\n$$\na_k^n \\equiv b \\equiv 0 \\pmod{p_i^{\\alpha_i}}\n$$\n\nand\n\n$$\na_k^n \\not\\equiv b \\not\\equiv 0 \\pmod{p_i^{\\alpha_i + 1}},\n$$\n\nwhich implies that the largest power of $p_i$ dividing $a_k^n$ is $p_i^{\\alpha_i}$. Since $a_k^n$ is a complete $n$th power, this implies that $\\alpha_i$ is divisible by $n$.\n\n$Qed.$",
        "structure": []
    },
    {
        "id": 7,
        "domain": "Linear Algebra",
        "informal": "3.70 *dimension shows whether vector spaces are isomorphic*\n\nTwo finite-dimensional vector spaces over $\\mathbf{F}$ are isomorphic if and only if they have the same dimension.\n\n**Proof** First suppose $V$ and $W$ are isomorphic finite-dimensional vector spaces. Thus there exists an isomorphism $T$ from $V$ onto $W$. Because $T$ is invertible, we have null $T = \\{0\\}$ and range $T = W$. Thus\n\n$$\\dim \\operatorname{null} T = 0 \\quad \\text{and} \\quad \\dim \\operatorname{range} T = \\dim W.$$\n\nThe formula\n\n$$\\dim V = \\dim \\operatorname{null} T + \\dim \\operatorname{range} T$$\n\n(the fundamental theorem of linear maps, which is 3.21) thus becomes the equation $\\dim V = \\dim W$, completing the proof in one direction.\n\nTo prove the other direction, suppose $V$ and $W$ are finite-dimensional vector spaces of the same dimension. Let $v_1, ..., v_n$ be a basis of $V$ and $w_1, ..., w_n$ be a basis of $W$. Let $T \\in \\mathcal{L}(V,W)$ be defined by\n\n$$T(c_1 v_1 + \\cdots + c_n v_n) = c_1 w_1 + \\cdots + c_n w_n.$$\n\nThen $T$ is a well-defined linear map because $v_1, ..., v_n$ is a basis of $V$. Also, $T$ is surjective because $w_1, ..., w_n$ spans $W$. Furthermore, null $T = \\{0\\}$ because $w_1, ..., w_n$ is linearly independent. Thus $T$ is injective. Because $T$ is injective and surjective, it is an isomorphism (see 3.63). Hence $V$ and $W$ are isomorphic.\n\n",
        "structure": []
    },
    {
        "id": 7,
        "domain": "Logic",
        "informal": "**7.6 Lemma.** *Let $\\Phi$ be consistent and suppose $\\Phi$ allows representations. Then $\\Phi^\\vdash$ is not representable in $\\Phi$.*\n\n*Proof.* Suppose the assumptions of the lemma hold and let $\\chi(v_0)$ represent $\\Phi^\\vdash$ in $\\Phi$. By the consistency of $\\Phi$ we get for an arbitrary $\\alpha \\in L^{S_{ar}}_0$,  \n$$(1) \\quad \\Phi \\vdash \\neg \\chi(\\mathfrak{n}^\\alpha) \\quad \\text{iff} \\quad \\text{not } \\Phi \\vdash \\alpha.$$\n\nFor $\\psi := \\neg \\chi$ we choose, by 7.5, a \"fixed point\" $\\varphi \\in L^{S_{ar}}_0$ such that  \n$$(2) \\quad \\Phi \\vdash \\varphi \\leftrightarrow \\neg \\chi(\\mathfrak{n}^\\varphi).$$\n\n($\\varphi$ says intuitively \"I am not true.\") But then  \n$$\\Phi \\vdash \\varphi \\quad \\text{iff} \\quad \\Phi \\vdash \\neg \\chi(\\mathfrak{n}^\\varphi) \\quad (\\text{by } (2))$$  \n$$\\quad \\text{iff} \\quad \\text{not } \\Phi \\vdash \\varphi \\quad (\\text{by } (1)),$$  \na contradiction.  \n$\\square$\n",
        "structure": []
    },
    {
        "id": 7,
        "domain": "Measure Theory",
        "informal": "The next result guarantees that there is a smallest $\\sigma$-algebra on a set $X$ containing a given set $\\mathcal{A}$ of subsets of $X$.\n\n2.27  *smallest $\\sigma$-algebra containing a collection of subsets*\n\nSuppose $X$ is a set and $\\mathcal{A}$ is a set of subsets of $X$. Then the intersection of all $\\sigma$-algebras on $X$ that contain $\\mathcal{A}$ is a $\\sigma$-algebra on $X$.\n\n**Proof** There is at least one $\\sigma$-algebra on $X$ that contains $\\mathcal{A}$ because the $\\sigma$-algebra consisting of all subsets of $X$ contains $\\mathcal{A}$.\n\nLet $S$ be the intersection of all $\\sigma$-algebras on $X$ that contain $\\mathcal{A}$. Then $\\emptyset \\in S$ because $\\emptyset$ is an element of each $\\sigma$-algebra on $X$ that contains $\\mathcal{A}$.\n\nSuppose $E \\in S$. Thus $E$ is in every $\\sigma$-algebra on $X$ that contains $\\mathcal{A}$. Thus $X \\setminus E$ is in every $\\sigma$-algebra on $X$ that contains $\\mathcal{A}$. Hence $X \\setminus E \\in S$.\n\nSuppose $E_1, E_2, \\ldots$ is a sequence of elements of $S$. Thus each $E_k$ is in every $\\sigma$-algebra on $X$ that contains $\\mathcal{A}$. Thus $\\bigcup_{k=1}^\\infty E_k$ is in every $\\sigma$-algebra on $X$ that contains $\\mathcal{A}$. Hence $\\bigcup_{k=1}^\\infty E_k \\in S$, which completes the proof that $S$ is a $\\sigma$-algebra on $X$.\n",
        "structure": []
    },
    {
        "id": 7,
        "domain": "Probability Theory",
        "informal": "**Theorem 5.22** (Borel Zero-One Law). Let $X_1, X_2, \\ldots$ be a sequence of independent random variables. Then its tail field is trivial, that is, any set in the tail field has probability zero or one.\n\n**Proof.** Let $\\Lambda \\in \\mathcal{F}^*_\\infty$. Let $\\mathcal{H}$ be the class of all events $\\Gamma$ such that\n\n\\[\n(5.3) \\quad P\\{\\Gamma \\cap \\Lambda\\} = P\\{\\Gamma\\} P\\{\\Lambda\\}.\n\\]\n\nLet $\\mathcal{F}_n = \\sigma(X_1, \\ldots, X_n)$, and put $\\mathcal{F}_\\infty = \\sigma(X_1, X_2, \\ldots)$. Intuitively, $\\mathcal{F}_n$ contains all the information in the first $n$ random variables, and $\\mathcal{F}_\\infty$ contains the information in all of them. Note that $\\mathcal{F}_n$ and $\\mathcal{F}^*_n$ are generated by disjoint families of independent random variables, and are therefore independent. Moreover, $\\Lambda \\in \\mathcal{F}^*_n$, so any $\\Gamma \\in \\mathcal{F}_n$ satisfies (5.3). Thus $\\mathcal{F}_n \\subset \\mathcal{H}$. Consequently, $\\bigcup_n \\mathcal{F}_n \\subset \\mathcal{H}$. Now $\\bigcup_n \\mathcal{F}_n$ is a field, but it may not be a $\\sigma$-field. Let $\\mathcal{F}_\\infty = \\sigma(\\bigcup_n \\mathcal{F}_n)$. We claim that $\\mathcal{F}_\\infty \\subset \\mathcal{H}$. To see this, note that $\\mathcal{H}$ contains the field $\\bigcup_n \\mathcal{F}_n$. Then, by the Monotone Class Theorem (Theorem 1.5), it is sufficient to prove that $\\mathcal{H}$ is a monotone class.\n\nSuppose that $\\Gamma_1, \\Gamma_2, \\ldots$ are in $\\mathcal{H}$: (5.3) holds for each $\\Gamma_n$. If either $\\Gamma_1 \\subset \\Gamma_2 \\subset \\ldots$ or $\\Gamma_1 \\supset \\Gamma_2 \\supset \\ldots$, we can go to the limit on both sides to see that (5.3) holds for $\\Gamma = \\lim_n \\Gamma_n$. Thus $\\mathcal{H}$ is a monotone class, and therefore it contains $\\mathcal{F}_\\infty$. But certainly $\\Lambda \\in \\mathcal{F}_\\infty$, so we can take $\\Gamma = \\Lambda$: $P\\{\\Lambda\\} = P\\{\\Lambda\\} P\\{\\Lambda\\}$. That is, $\\Lambda$ is independent of itself! But there are only two solutions of $x = x^2$, zero and one. Therefore, $P\\{\\Lambda\\} = 0$ or $1$. Done!  □\n\nSo, any event in the tail field has probability either 0 or 1, and any tail random variable (i.e., $\\mathcal{F}^*_\\infty$-measurable) is constant a.e. It goes without saying that the hypothesis of independence is essential. For instance, if all the $X_n$ are equal, the tail field is $\\sigma(X_1)$.\n",
        "structure": []
    },
    {
        "id": 7,
        "domain": "Set Theory",
        "informal": "## Goal\n\nLet $R$ be a Equivalence Relation on $\\R$. Define $S_2= \\left\\{ ([a]_R, [b]_R) \\mid \\exists a' \\in [a]_R, b' \\in [b]_R, a' - b' = \\frac{1}{2} \\right\\} \\cup \\{([a]_R, [a]_R) \\mid a \\in \\mathbb{R} \\}$. Prove that $S_2$ is an Equivalence Relation.\n\n## Proof\n\n**Reflexivity:** For all $[a]_R$, $([a]_R, [a]_R) \\in S_2$ by definition.\n\n**Symmetry:** If $([a]_R, [b]_R) \\in S_2$, there exist $a' \\in [a]_R, b' \\in [b]_R$ with $a' - b' = \\frac{1}{2}$. Let $b'' = b' + 1 \\in [b]_R$. Then $b'' - a' = \\frac{1}{2}$, so $([b]_R, [a]_R) \\in S_2$.\n\n**Transitivity:** Suppose $([a]_R, [b]_R) \\in S_2$ and $([b]_R, [c]_R) \\in S_2$. Then:\n\n1. $\\exists a' \\in [a]_R, b_1 \\in [b]_R$ with $a' - b_1 = \\frac{1}{2}$.\n2. $\\exists b_2 \\in [b]_R, c' \\in [c]_R$ with $b_2 - c' = \\frac{1}{2}$.\n\nSince $b_1, b_2 \\in [b]_R$, $b_1 - b_2 = k \\in \\mathbb{Z}$. Let $c'' = c' + k \\in [c]_R$. Then:\n\n$$\na' - c'' = (a' - b_1) + (b_1 - c'') = \\frac{1}{2} + (b_1 - (c' + k)) = \\frac{1}{2} - \\frac{1}{2} = 0 \\implies [a]_R = [c]_R.\n$$\n\nThus, $([a]_R, [c]_R) \\in S_2$.\n",
        "structure": []
    },
    {
        "id": 7,
        "domain": "Topology Point Set",
        "informal": "**Theorem 18.3 (The pasting lemma).** Let $X = A \\cup B$, where $A$ and $B$ are closed in $X$. Let $f : A \\to Y$ and $g : B \\to Y$ be continuous. If $f(x) = g(x)$ for every $x \\in A \\cap B$, then $f$ and $g$ combine to give a continuous function $h : X \\to Y$, defined by setting $h(x) = f(x)$ if $x \\in A$, and $h(x) = g(x)$ if $x \\in B$.\n\n*Proof.* Let $C$ be a closed subset of $Y$. Now\n\n$$h^{-1}(C) = f^{-1}(C) \\cup g^{-1}(C),$$\n\nby elementary set theory. Since $f$ is continuous, $f^{-1}(C)$ is closed in $A$ and, therefore, closed in $X$. Similarly, $g^{-1}(C)$ is closed in $B$ and therefore closed in $X$. Their union $h^{-1}(C)$ is thus closed in $X$. ■\n",
        "structure": []
    },
    {
        "id": 8,
        "domain": "Abstract Algebra",
        "informal": "**1.4.6 Correspondence Theorem** If $N$ is a normal subgroup of $G$, then the map $\\psi : H \\to H / N$ sets up a one-to-one correspondence between subgroups of $G$ containing $N$ and subgroups of $G / N$. The inverse of $\\psi$ is the map $\\tau : Q \\to \\pi^{-1}(Q)$, where $\\pi$ is the canonical epimorphism of $G$ onto $G / N$. Furthermore,\n\n(i) $H_1 \\leq H_2$ if and only if $H_1 / N \\leq H_2 / N$, and in this case,\n\n$$[H_2 : H_1] = [H_2 / N : H_1 / N]$$\n\n(ii) $H$ is a normal subgroup of $G$ if and only if $H / N$ is a normal subgroup of $G / N$. More generally,  \n(iii) $H_1$ is a normal subgroup of $H_2$ if and only if $H_1 / N$ is a normal subgroup of $H_2 / N$, and in this case, $H_2 / H_1 \\cong (H_2 / N) / (H_1 / N)$.\n\n*Proof.* We have established that $\\psi$ is a bijection with inverse $\\tau$. If $H_1 \\leq H_2$, we have $H_1 / N \\leq H_2 / N$ immediately, and the converse follows from the above proof that $\\psi$ is injective. To prove the last statement of (i), let $\\eta$ map the left coset $aH_1$, $a \\in H_2$, to the left coset $(aN)(H_1 / N)$. Then $\\eta$ is a well-defined and injective map of\n\n$$aH_1 = bH_1 \\quad \\text{iff} \\quad a^{-1} b \\in H_1$$\n$$\\text{iff} \\quad (aN)^{-1}(bN) = a^{-1} b N \\in H_1 / N$$\n$$\\text{iff} \\quad (aN)(H_1 / N) = (bN)(H_1 / N);$$\n\n$\\eta$ is surjective because $a$ ranges over all of $H_2$.  \nTo prove (ii), assume that $H \\triangleleft G$; then for any $a \\in G$ we have\n\n$$(aN)(H / N)(aN)^{-1} = (aHa^{-1}) / N = H / N$$\n\nso that $H / N \\triangleleft G / N$. Conversely, suppose that $H / N$ is normal in $G / N$. Consider the homomorphism $a \\to (aN)(H / N)$, the composition of the canonical map of $G$ onto $G / N$ and the canonical map of $G / N$ onto $(G / N) / (H / N)$. The element $a$ will belong to the kernel of this map if and only if $(aN)(H / N) = H / N$, which happens if and only if $aN \\in H / N$, that is, $aN = hN$ for some $h \\in H$. But since $N$ is contained in $H$, this statement is equivalent to $a \\in H$. Thus $H$ is the kernel of a homomorphism, and is therefore a normal subgroup of $G$.  \n\nFinally, the proof of (ii) also establishes the first part of (iii); just replace $H$ by $H_1$ and $G$ by $H_2$. The second part of (iii) follows from the third isomorphism theorem (with the same replacement). ♣",
        "structure": []
    },
    {
        "id": 8,
        "domain": "Analysis",
        "informal": "$\\int {x}^{2}\\sqrt{{a}^{2} + {x}^{2}}\\mathrm{\\;d}x$ .\n\nSolution\n\n $\\int {x}^{2}\\sqrt{{a}^{2} + {x}^{2}}\\mathrm{\\;d}x = \\frac{1}{2}\\int x{\\left( {a}^{2} + {x}^{2}\\right) }^{\\frac{1}{2}}\\mathrm{\\;d}\\left( {{a}^{2} + {x}^{2}}\\right)  = \\frac{1}{3}\\int x\\mathrm{\\;d}\\left\\lbrack  {\\left( {a}^{2} + {x}^{2}\\right) }^{\\frac{3}{2}}\\right\\rbrack$\n$$\n= \\frac{1}{3}x{\\left( {a}^{2} + {x}^{2}\\right) }^{\\frac{3}{2}} - \\frac{1}{3}\\int {\\left( {a}^{2} + {x}^{2}\\right) }^{\\frac{3}{2}}\\mathrm{\\;d}x\n$$\n\n$$\n= \\frac{1}{3}x\\left( {{a}^{2} + {x}^{2}}\\right) \\sqrt{{a}^{2} + {x}^{2}} - \\frac{{a}^{2}}{3}\\int \\sqrt{{a}^{2} + {x}^{2}}\\mathrm{\\;d}x - \\frac{1}{3}\\int {x}^{2}\\sqrt{{a}^{2} + {x}^{2}}\\mathrm{\\;d}x.\n$$\n\nHence $\\int {x}^{2}\\sqrt{{a}^{2} + {x}^{2}}\\mathrm{\\;d}x = \\frac{3}{4}\\left\\lbrack  {\\frac{1}{3}x\\left( {{a}^{2} + {x}^{2}}\\right) \\sqrt{{a}^{2} + {x}^{2}} - \\frac{{a}^{2}}{3}\\int \\sqrt{{a}^{2} + {x}^{2}}\\mathrm{\\;d}x}\\right\\rbrack$\n\n$$\n= \\frac{1}{4}x\\left( {{a}^{2} + {x}^{2}}\\right) \\sqrt{{a}^{2} + {x}^{2}} - \\frac{{a}^{2}}{4}{\\left\\lbrack  \\frac{x}{2}\\sqrt{{a}^{2} + {x}^{2}} + \\frac{{a}^{2}}{2}\\ln \\left( x + \\sqrt{{a}^{2} + {x}^{2}}\\right) \\right\\rbrack  } + C\n$$\n\n$$\n= \\frac{x\\left( {2{x}^{2} + {a}^{2}}\\right) }{8}\\sqrt{{a}^{2} + {x}^{2}} - \\frac{{a}^{4}}{8}\\ln \\left( {x + \\sqrt{{x}^{2} + {a}^{2}}}\\right)  + C.\n$$",
        "structure": []
    },
    {
        "id": 8,
        "domain": "IMO",
        "informal": "## Goal\n\nLet $k$ be a positive integer. Prove that the number $(4k^2 - 1)^2$ has a positive divisor of the form $8kn - 1$ if and only if $k$ is even.\n\n## Proof\n\nThe statement follows from the following fact.\n\n$Lemma.$ For arbitrary positive integers $x$ and $y$, the number $4xy - 1$ divides $(4x^2 - 1)^2$ if and only if $x = y$.\n\n**Proof.** If $x = y$ then $4xy - 1 = 4x^2 - 1$ obviously divides $(4x^2 - 1)^2$ so it is sufficient to consider the opposite direction.\n\nCall a pair $(x, y)$ of positive integers **bad** if $4xy - 1$ divides $(4x^2 - 1)^2$ but $x \\neq y$. In order to prove that bad pairs do not exist, we present two properties of them which provide an infinite descent.\n\n**Property (i).** If $(x, y)$ is a bad pair and $x < y$ then there exists a positive integer $z < x$ such that $(x, z)$ is also bad.\n\nLet $r = \\frac{(4x^2 - 1)^2}{4xy - 1}$. Then\n\n$$\nr = -r \\cdot (-1) \\equiv -r(4xy - 1) = -(4x^2 - 1)^2 \\equiv -1 \\pmod{4x}\n$$\n\nand $r = 4xz - 1$ with some positive integer $z$. From $x < y$ we obtain that\n\n$$\n4xz - 1 = \\frac{(4x^2 - 1)^2}{4xy - 1} < 4x^2 - 1\n$$\n\nand therefore $z < x$. By the construction, the number $4xz - 1$ is a divisor of $(4x^2 - 1)^2$ so $(x, z)$ is a bad pair.\n\n**Property (ii).** If $(x, y)$ is a bad pair then $(y, x)$ is also bad.\n\nSince $1 = 1^2 \\equiv (4xy)^2 \\pmod{4xy - 1}$, we have\n\n$$\n(4y^2 - 1)^2 \\equiv (4y^2 - (4xy)^2)^2 = 16y^4(4x^2 - 1)^2 \\equiv 0 \\pmod{4xy - 1}.\n$$\n\nHence, the number $4xy - 1$ divides $(4y^2 - 1)^2$ as well.\n\nNow suppose that there exists at least one bad pair. Take a bad pair $(x, y)$ such that $2x + y$ attains its smallest possible value. If $x < y$ then property (i) provides a bad pair $(x, z)$ with $z < y$ and thus $2x + z < 2x + y$. Otherwise, if $y < x$, property (ii) yields that pair $(y, x)$ is also bad while $2y + x < 2x + y$. Both cases contradict the assumption that $2x + y$​ is minimal; the Lemma is proved.\n\n**Qed.**\n\nTo prove the problem statement, apply the Lemma for $x = k$ and $y = 2n$; the number $8kn - 1$ divides $(4k^2 - 1)^2$ if and only if $k = 2n$. Hence, there is no such $n$ if $k$ is odd and $n = k/2$ is the only solution if $k$ is even.\n\n$Qed.$",
        "structure": []
    },
    {
        "id": 8,
        "domain": "Linear Algebra",
        "informal": "The setting for the next result is the assumption that we have a basis $v_1, ..., v_n$ of $V$, along with its dual basis $\\varphi_1, ..., \\varphi_n$ of $V'$. We also have a basis $w_1, ..., w_m$ of $W$, along with its dual basis $\\psi_1, ..., \\psi_m$ of $W'$. Thus $\\mathcal{M}(T)$ is computed with respect to the bases just mentioned of $V$ and $W$, and $\\mathcal{M}(T')$ is computed with respect to the dual bases just mentioned of $W'$ and $V'$. Using these bases gives the following pretty result.\n\n3.132 *matrix of $T'$ is transpose of matrix of $T$*\n\nSuppose $V$ and $W$ are finite-dimensional and $T \\in \\mathcal{L}(V,W)$. Then\n\n$$\\mathcal{M}(T') = (\\mathcal{M}(T))^{t}.$$\n\n**Proof** Let $A = \\mathcal{M}(T)$ and $C = \\mathcal{M}(T')$. Suppose $1 \\leq j \\leq m$ and $1 \\leq k \\leq n$. From the definition of $\\mathcal{M}(T')$ we have\n\n$$T'(\\psi_j) = \\sum_{r=1}^n C_{r,j} \\varphi_r.$$\n\nThe left side of the equation above equals $\\psi_j \\circ T$. Thus applying both sides of the equation above to $v_k$ gives\n\n$$(\\psi_j \\circ T)(v_k) = \\sum_{r=1}^n C_{r,j} \\varphi_r(v_k) = C_{k,j}.$$\n\nWe also have\n\n$$(\\psi_j \\circ T)(v_k) = \\psi_j(T v_k) = \\psi_j \\left( \\sum_{r=1}^m A_{r,k} w_r \\right) = \\sum_{r=1}^m A_{r,k} \\psi_j(w_r) = A_{j,k}.$$\n\nComparing the last line of the last two sets of equations, we have $C_{k,j} = A_{j,k}$. Thus $C = A^{t}$. In other words, $\\mathcal{M}(T') = (\\mathcal{M}(T))^{t}$, as desired.\n\n\n",
        "structure": []
    },
    {
        "id": 8,
        "domain": "Measure Theory",
        "informal": "2.39  *condition for measurable function*\n\nSuppose $(X, S)$ is a measurable space and $f : X \\to \\mathbf{R}$ is a function such that\n\n$$\nf^{-1}((a, \\infty)) \\in S\n$$\n\nfor all $a \\in \\mathbf{R}$. Then $f$ is an $S$-measurable function.\n\n**Proof** Let\n\n$$\n\\mathcal{T} = \\{ A \\subset \\mathbf{R} : f^{-1}(A) \\in S \\}.\n$$\n\nWe want to show that every Borel subset of $\\mathbf{R}$ is in $\\mathcal{T}$. To do this, we will first show that $\\mathcal{T}$ is a $\\sigma$-algebra on $\\mathbf{R}$.\n\nCertainly $\\emptyset \\in \\mathcal{T}$, because $f^{-1}(\\emptyset) = \\emptyset \\in S$.\n\nIf $A \\in \\mathcal{T}$, then $f^{-1}(A) \\in S$; hence\n\n$$\nf^{-1}(\\mathbf{R} \\setminus A) = X \\setminus f^{-1}(A) \\in S\n$$\n\nby 2.33(a), and thus $\\mathbf{R} \\setminus A \\in \\mathcal{T}$. In other words, $\\mathcal{T}$ is closed under complementation.\n\nIf $A_1, A_2, \\ldots \\in \\mathcal{T}$, then $f^{-1}(A_1), f^{-1}(A_2), \\ldots \\in S$; hence\n\n$$\nf^{-1}\\left(\\bigcup_{k=1}^\\infty A_k \\right) = \\bigcup_{k=1}^\\infty f^{-1}(A_k) \\in S\n$$\n\nby 2.33(b), and thus $\\bigcup_{k=1}^\\infty A_k \\in \\mathcal{T}$. In other words, $\\mathcal{T}$ is closed under countable unions. Thus $\\mathcal{T}$ is a $\\sigma$-algebra on $\\mathbf{R}$.\n\nBy hypothesis, $\\mathcal{T}$ contains $\\{(a, \\infty) : a \\in \\mathbf{R}\\}$. Because $\\mathcal{T}$ is closed under complementation, $\\mathcal{T}$ also contains $\\{(-\\infty, b] : b \\in \\mathbf{R}\\}$. Because the $\\sigma$-algebra $\\mathcal{T}$ is closed under finite intersections (by 2.25), we see that $\\mathcal{T}$ contains $\\{(a,b) : a,b \\in \\mathbf{R}\\}$. Because $(a,b) = \\bigcup_{k=1}^\\infty (a, b-\\frac{1}{k}]$ and $(-\\infty,b) = \\bigcup_{k=1}^\\infty (-k, b-\\frac{1}{k}]$ and $\\mathcal{T}$ is closed under countable unions, we can conclude that $\\mathcal{T}$ contains every open subset of $\\mathbf{R}$.\n\nThus the $\\sigma$-algebra $\\mathcal{T}$ contains the smallest $\\sigma$-algebra on $\\mathbf{R}$ that contains all open subsets of $\\mathbf{R}$. In other words, $\\mathcal{T}$ contains every Borel subset of $\\mathbf{R}$. Thus $f$ is an $S$-measurable function.\n",
        "structure": []
    },
    {
        "id": 8,
        "domain": "Probability Theory",
        "informal": "**Theorem 9.7** (Doob Decomposition). Let $\\{X_n, \\mathcal{F}_n, n=0,1,2,\\ldots\\}$ be a submartingale. Then there exists a martingale $\\{M_n, \\mathcal{F}_n, n=0,1,2,\\ldots\\}$ and a process $\\{A_n, n=0,1,2,\\ldots\\}$ such that for $n=0,1,2,\\ldots$\n\n1. $A_n \\leq A_{n+1}$ a.e.,\n2. $A_0 = 0$ and $A_n$ is $\\mathcal{F}_{n-1}$-measurable, $n=1,2,3,\\ldots$,\n3. $X_n = M_n + A_n, n=0,1,2,\\ldots$.\n\n**Proof.** Let $d_n = E\\{X_{n+1} - X_n \\mid \\mathcal{F}_n\\}$. Then $d_n \\geq 0$ by the submartingale inequality, and $d_n$ is $\\mathcal{F}_n$-measurable. Define $A_0 = 0$, $M_0 = X_0$, and\n\n$$\nA_n \\overset{\\text{def}}{=} d_1 + \\cdots + d_{n-1}, \\quad M_n \\overset{\\text{def}}{=} X_n - A_n, \\quad n=1,2,3,\\ldots.\n$$\n\nIt is clear that (i), (ii) and (iii) hold. We must verify that $M_n$ is a martingale. It is adapted. Calculate\n\n$$\nE\\{M_{n+1} \\mid \\mathcal{F}_n\\} = E\\{X_{n+1} - A_{n+1} \\mid \\mathcal{F}_n\\} = E\\{X_{n+1} - X_n + \\underbrace{X_n - A_n}_{M_n} - (A_{n+1} - A_n) \\mid \\mathcal{F}_n\\}\n$$\n\nwhich is\n\n$$\n= d_n + M_n - \\underbrace{(A_{n+1} - A_n)}_{d_n} = M_n.\n$$\n\nThus, $M_n$ is indeed a martingale.  □\n",
        "structure": []
    },
    {
        "id": 8,
        "domain": "Set Theory",
        "informal": "To prove Theorem 10, which states that if $ R $ is an equivalence relation on $ A $ and $ a, b \\in A $, then $ aRb $ if and only if $[a]_R = [b]_R$, we need to show both directions of the implication.\n\n### Proof:\n\n1. **Assume $ aRb $. We need to show that $[a]_R = [b]_R$.**\n\n   - **Show $[a]_R \\subseteq [b]_R$:**\n     Let $ c \\in [a]_R $. By definition, $ aRc $. Since $ R $ is symmetric and $ aRb $, we have $ bRa $. By transitivity, $ bRa $ and $ aRc $ imply $ bRc $. Therefore, $ c \\in [b]_R $.\n\n   - **Show $[b]_R \\subseteq [a]_R$:**\n     Let $ c \\in [b]_R $. By definition, $ bRc $. Since $ aRb $ and $ R $ is transitive, $ aRb $ and $ bRc $ imply $ aRc $. Therefore, $ c \\in [a]_R $.\n\n   Since both $[a]_R \\subseteq [b]_R$ and $[b]_R \\subseteq [a]_R$ hold, we conclude that $[a]_R = [b]_R$.\n\n2. **Assume $[a]_R = [b]_R$. We need to show that $ aRb $.**\n\n   - Since $[a]_R = [b]_R$, and $ a \\in [a]_R $ (because $ R $ is reflexive, $ aRa $), it follows that $ a \\in [b]_R $. By definition of $[b]_R$, this means $ bRa $.\n\n   - Since $ R $ is symmetric, $ bRa $ implies $ aRb $.\n\nTherefore, we have shown both directions: $ aRb $ implies $[a]_R = [b]_R$, and $[a]_R = [b]_R$ implies $ aRb $. This completes the proof of Theorem 10.",
        "structure": []
    },
    {
        "id": 8,
        "domain": "Topology Point Set",
        "informal": "**Theorem 18.4 (Maps into products).** Let $f : A \\to X \\times Y$ be given by the equation\n\n$$f(a) = (f_1(a), f_2(a)).$$\n\nThen $f$ is continuous if and only if the functions\n\n$$f_1 : A \\to X \\quad \\text{and} \\quad f_2 : A \\to Y$$\n\nare continuous.\n\nThe maps $f_1$ and $f_2$ are called the *coordinate functions* of $f$.\n\n*Proof.* Let $\\pi_1 : X \\times Y \\to X$ and $\\pi_2 : X \\times Y \\to Y$ be projections onto the first and second factors, respectively. These maps are continuous. For $\\pi_1^{-1}(U) = U \\times Y$ and $\\pi_2^{-1}(V) = X \\times V$, and these sets are open if $U$ and $V$ are open. Note that for each $a \\in A$,\n\n$$f_1(a) = \\pi_1(f(a)) \\quad \\text{and} \\quad f_2(a) = \\pi_2(f(a)).$$\n\nIf the function $f$ is continuous, then $f_1$ and $f_2$ are composites of continuous functions and therefore continuous. Conversely, suppose that $f_1$ and $f_2$ are continuous. We show that for each basis element $U \\times V$ for the topology of $X \\times Y$, its inverse image $f^{-1}(U \\times V)$ is open. A point $a$ is in $f^{-1}(U \\times V)$ if and only if $f(a) \\in U \\times V$, that is, if and only if $f_1(a) \\in U$ and $f_2(a) \\in V$. Therefore,\n\n$$f^{-1}(U \\times V) = f_1^{-1}(U) \\cap f_2^{-1}(V).$$\n\nSince both of the sets $f_1^{-1}(U)$ and $f_2^{-1}(V)$ are open, so is their intersection. ■\n\n",
        "structure": []
    },
    {
        "id": 9,
        "domain": "Abstract Algebra",
        "informal": "**2.2.6 Proposition** If $f : R \\to S$ is a ring homomorphism and the only ideals of $R$ are $\\{0\\}$ and $R$, then $f$ is injective. (In particular, if $R$ is a division ring, then $R$ satisfies this hypothesis.)\n\n*Proof.* Let $I = \\ker f$, an ideal of $R$ (see (2.2.3)). If $I = R$ then $f$ is identically zero, and is therefore not a legal ring homomorphism since $f(1_R) = 1_S \\neq 0_S$. Thus $I = \\{0\\}$, so that $f$ is injective.\n\nIf $R$ is a division ring, then in fact $R$ has no nontrivial left or right ideals. For suppose that $I$ is a left ideal of $R$ and $a \\in I, a \\neq 0$. Since $R$ is a division ring, there is an element $b \\in R$ such that $ba = 1$, and since $I$ is a left ideal, we have $1 \\in I$, which implies that $I = R$. If $I$ is a right ideal, we choose the element $b$ such that $ab = 1$. ♣",
        "structure": []
    },
    {
        "id": 9,
        "domain": "Analysis",
        "informal": "## Goal\n\nLet $\\{x_n\\}$ and $\\{y_n\\}$ be numerical sequences. Prove that if $\\lim_{n \\to \\infty} x_n = A$ and $\\lim_{n \\to \\infty} y_n = B$, then $\\lim_{n \\to \\infty} (x_n \\cdot y_n) = A \\cdot B$.\n\n$Proof.$\n\nWe know that\n\n$$\n|(A \\cdot B) - (x_n \\cdot y_n)| \\leq |x_n| \\Delta(y_n) + |y_n| \\Delta(x_n) + \\Delta(x_n) \\cdot \\Delta(y_n).\n$$\n\nGiven $\\varepsilon > 0$ find numbers $N'$ and $N''$ such that\n\n$$\n\\forall n > N' \\left( \\Delta(x_n) < \\min \\left\\{ 1, \\frac{\\varepsilon}{3(|B| + 1)} \\right\\} \\right),\n$$\n\n$$\n\\forall n > N'' \\left( \\Delta(y_n) < \\min \\left\\{ 1, \\frac{\\varepsilon}{3(|A| + 1)} \\right\\} \\right).\n$$\n\nThen for $n > N = \\max\\{N', N''\\}$ we shall have\n\n$$\n|x_n| < |A| + \\Delta(x_n) < |A| + 1,\n$$\n\n$$\n|y_n| < |B| + \\Delta(y_n) < |B| + 1,\n$$\n\n$$\n\\Delta(x_n) \\cdot \\Delta(y_n) < \\min \\left\\{ 1, \\frac{\\varepsilon}{3} \\right\\} \\cdot \\min \\left\\{ 1, \\frac{\\varepsilon}{3} \\right\\} \\leq \\frac{\\varepsilon}{3}.\n$$\n\nHence for $n > N$ we have\n\n$$\n|x_n| \\Delta(y_n) < (|A| + 1) \\cdot \\frac{\\varepsilon}{3(|A| + 1)} < \\frac{\\varepsilon}{3},\n$$\n\n$$\n|y_n| \\Delta(x_n) < (|B| + 1) \\cdot \\frac{\\varepsilon}{3(|B| + 1)} < \\frac{\\varepsilon}{3},\n$$\n\n$$\n\\Delta(x_n) \\cdot \\Delta(y_n) < \\frac{\\varepsilon}{3}.\n$$\n\nand therefore $|AB - x_n y_n| < \\varepsilon$ for $n > N$.\n\n$Qed.$",
        "structure": []
    },
    {
        "id": 9,
        "domain": "IMO",
        "informal": "## Goal\n\nLet $a, b, c, d$ be positive real numbers such that\n\n$$\nabcd = 1 \\quad \\text{and} \\quad a + b + c + d > \\frac{a}{b} + \\frac{b}{c} + \\frac{c}{d} + \\frac{d}{a}.\n$$\n\nProve that\n\n$$\na + b + c + d < \\frac{b}{a} + \\frac{c}{b} + \\frac{d}{c} + \\frac{a}{d}.\n$$\n\n## Proof\n\nWe show that if $abcd = 1$, the sum $a + b + c + d$ cannot exceed a certain weighted mean of the expressions $\\frac{a}{b} + \\frac{b}{c} + \\frac{c}{d} + \\frac{d}{a}$ and $\\frac{b}{a} + \\frac{c}{b} + \\frac{d}{c} + \\frac{a}{d}$.\n\nBy applying the AM-GM inequality to the numbers $\\frac{a}{b}$, $\\frac{b}{c}$, $\\frac{c}{d}$, and $\\frac{d}{a}$, we obtain\n\n$$\na = \\sqrt[4]{\\frac{a^4}{abcd}} = \\sqrt[4]{\\frac{a}{b} \\cdot \\frac{b}{c} \\cdot \\frac{c}{d} \\cdot \\frac{d}{a}} \\leq \\frac{1}{4} \\left( \\frac{a}{b} + \\frac{b}{c} + \\frac{c}{d} + \\frac{d}{a} \\right).\n$$\n\nAnalogously,\n\n$$\nb \\leq \\frac{1}{4} \\left( \\frac{b}{c} + \\frac{c}{d} + \\frac{d}{a} + \\frac{a}{b} \\right), \\quad c \\leq \\frac{1}{4} \\left( \\frac{c}{d} + \\frac{d}{a} + \\frac{a}{b} + \\frac{b}{c} \\right), \\quad \\text{and} \\quad d \\leq \\frac{1}{4} \\left( \\frac{d}{a} + \\frac{a}{b} + \\frac{b}{c} + \\frac{c}{d} \\right).\n$$\n\nSumming up these estimates yields\n\n$$\na + b + c + d \\leq \\frac{3}{4} \\left( \\frac{a}{b} + \\frac{b}{c} + \\frac{c}{d} + \\frac{d}{a} \\right) + \\frac{1}{4} \\left( \\frac{b}{a} + \\frac{c}{b} + \\frac{d}{c} + \\frac{a}{d} \\right).\n$$\n\nIn particular, if $a + b + c + d > \\frac{a}{b} + \\frac{b}{c} + \\frac{c}{d} + \\frac{d}{a}$, then\n\n$$\na + b + c + d < \\frac{b}{a} + \\frac{c}{b} + \\frac{d}{c} + \\frac{a}{d}.\n$$\n\n$Qed.$",
        "structure": []
    },
    {
        "id": 9,
        "domain": "Linear Algebra",
        "informal": "The setting for the next result is the assumption that we have a basis $v_1, ..., v_n$ of $V$, along with its dual basis $\\varphi_1, ..., \\varphi_n$ of $V'$. We also have a basis $w_1, ..., w_m$ of $W$, along with its dual basis $\\psi_1, ..., \\psi_m$ of $W'$. Thus $\\mathcal{M}(T)$ is computed with respect to the bases just mentioned of $V$ and $W$, and $\\mathcal{M}(T')$ is computed with respect to the dual bases just mentioned of $W'$ and $V'$. Using these bases gives the following pretty result.\n\n3.132 *matrix of $T'$ is transpose of matrix of $T$*\n\nSuppose $V$ and $W$ are finite-dimensional and $T \\in \\mathcal{L}(V,W)$. Then\n\n$$\\mathcal{M}(T') = (\\mathcal{M}(T))^{t}.$$\n\n**Proof** Let $A = \\mathcal{M}(T)$ and $C = \\mathcal{M}(T')$. Suppose $1 \\leq j \\leq m$ and $1 \\leq k \\leq n$. From the definition of $\\mathcal{M}(T')$ we have\n\n$$T'(\\psi_j) = \\sum_{r=1}^n C_{r,j} \\varphi_r.$$\n\nThe left side of the equation above equals $\\psi_j \\circ T$. Thus applying both sides of the equation above to $v_k$ gives\n\n$$(\\psi_j \\circ T)(v_k) = \\sum_{r=1}^n C_{r,j} \\varphi_r(v_k) = C_{k,j}.$$\n\nWe also have\n\n$$(\\psi_j \\circ T)(v_k) = \\psi_j(T v_k) = \\psi_j \\left( \\sum_{r=1}^m A_{r,k} w_r \\right) = \\sum_{r=1}^m A_{r,k} \\psi_j(w_r) = A_{j,k}.$$\n\nComparing the last line of the last two sets of equations, we have $C_{k,j} = A_{j,k}$. Thus $C = A^{t}$. In other words, $\\mathcal{M}(T') = (\\mathcal{M}(T))^{t}$, as desired.\n\n\n",
        "structure": []
    },
    {
        "id": 9,
        "domain": "Measure Theory",
        "informal": "2.48  *limit of $S$-measurable functions*\n\nSuppose $(X, S)$ is a measurable space and $f_1, f_2, \\ldots$ is a sequence of $S$-measurable functions from $X$ to $\\mathbf{R}$. Suppose $\\lim_{k \\to \\infty} f_k(x)$ exists for each $x \\in X$. Define $f : X \\to \\mathbf{R}$ by\n\n$$\nf(x) = \\lim_{k \\to \\infty} f_k(x).\n$$\n\nThen $f$ is an $S$-measurable function.\n\n**Proof** Suppose $a \\in \\mathbf{R}$. We will show that\n\n2.49\n$$\nf^{-1}((a, \\infty)) = \\bigcup_{j=1}^\\infty \\bigcup_{m=1}^\\infty \\bigcap_{k=m}^\\infty f_k^{-1} \\left( \\left(a + \\frac{1}{j}, \\infty \\right) \\right),\n$$\n\nwhich implies that $f^{-1}((a, \\infty)) \\in S$.\n\nTo prove 2.49, first suppose $x \\in f^{-1}((a, \\infty))$. Thus there exists $j \\in \\mathbf{Z}^+$ such that $f(x) > a + \\frac{1}{j}$. The definition of limit now implies that there exists $m \\in \\mathbf{Z}^+$ such that $f_k(x) > a + \\frac{1}{j}$ for all $k \\geq m$. Thus $x$ is in the right side of 2.49, proving that the left side of 2.49 is contained in the right side.\n\nTo prove the inclusion in the other direction, suppose $x$ is in the right side of 2.49. Thus there exist $j, m \\in \\mathbf{Z}^+$ such that $f_k(x) > a + \\frac{1}{j}$ for all $k \\geq m$. Taking the limit as $k \\to \\infty$, we see that $f(x) \\geq a + \\frac{1}{j} > a$. Thus $x$ is in the left side of 2.49, completing the proof of 2.49. Thus $f$ is an $S$-measurable function.\n\n",
        "structure": []
    },
    {
        "id": 9,
        "domain": "Probability Theory",
        "informal": "**Theorem 9.8.** Let $\\{X_n, \\mathcal{F}_n, n=0,1,2,\\}$ be a submartingale and let $T$ be a stopping time. Then $\\{X_{n \\wedge T}, \\mathcal{F}_n, n=0,1,2,\\ldots\\}$ is a submartingale, as is $\\{X_{n \\wedge T}, \\mathcal{F}_{n \\wedge T}, n=0,1,2,\\ldots\\}$.\n\n**Proof.** The properties of stopping times and their associated $\\sigma$-fields were established in §7.4. We will use them freely.\n\nNote that $X_{n \\wedge T}$ is measurable with respect to $\\mathcal{F}_{n \\wedge T} \\subset \\mathcal{F}_T$, so $X_{n \\wedge T}$ is adapted to both filtrations $(\\mathcal{F}_n)$ and $(\\mathcal{F}_{n \\wedge T})$. Let $\\Lambda \\in \\mathcal{F}_n$.\n\n\\[\n\\int_{\\Lambda} X_{(n+1) \\wedge T} dP = \\int_{\\Lambda \\cap \\{T > n\\}} X_{n+1} dP + \\int_{\\Lambda \\cap \\{T \\leq n\\}} X_{n \\wedge T} dP,\n\\]\n\nfor $X_{(n+1) \\wedge T} = X_{n+1}$ if $T > n$. Now $\\Lambda \\cap \\{T > n\\} \\in \\mathcal{F}_n$ so, as $(X_n)$ is a submartingale,\n\n\\[\n\\int_{\\Lambda \\cap \\{T > n\\}} X_{n+1} dP \\geq \\int_{\\Lambda \\cap \\{T > n\\}} X_n dP = \\int_{\\Lambda \\cap \\{T > n\\}} X_{n \\wedge T} dP,\n\\]\n\nand the above is\n\n\\[\n\\geq \\int_{\\Lambda \\cap \\{T > n\\}} X_{n \\wedge T} dP + \\int_{\\Lambda \\cap \\{T \\leq n\\}} X_{n \\wedge T} dP = \\int_{\\Lambda} X_{n \\wedge T} dP,\n\\]\n\nshowing that $(X_{n \\wedge T})$ is a submartingale relative to the filtration $(\\mathcal{F}_n)$. Since $\\mathcal{F}_{n \\wedge T} \\subset \\mathcal{F}_n$, this also shows it is a submartingale with respect to the filtration $(\\mathcal{F}_{n \\wedge T})$.  □\n",
        "structure": []
    },
    {
        "id": 9,
        "domain": "Set Theory",
        "informal": "Prove there exists at least one “smallest inductive set” if there exists at least one inductive set. **Hint:** you can use the conclusions above.\n\nProof. Suppose there exists an inductive set $u$. Let\n\n$$\nN = \\bigcap_{\\substack{X \\text{ is inductive}}} X.\n$$\n\n$N$ is indeed a set because $N \\subseteq u$. We prove that $N$ is the smallest inductive set.  \nFor each inductive set $X$, $\\varnothing \\in X$. Thus $\\varnothing \\in N$. For each $x \\in N$, $x \\in X$ for all inductive set $X$, which implies that $x \\cup \\{x\\} \\in X$ for all inductive set $X$, which implies that $x \\cup \\{x\\} \\in N$. Hence $N$ is inductive. For any inductive set $X$, $X \\subseteq N$ by definition. Hence $N$ is *the* smallest inductive set.",
        "structure": []
    },
    {
        "id": 9,
        "domain": "Topology Point Set",
        "informal": "**Theorem 21.6 (Uniform limit theorem).** Let $f_n : X \\to Y$ be a sequence of continuous functions from the topological space $X$ to the metric space $Y$. If $(f_n)$ converges uniformly to $f$, then $f$ is continuous.\n\n*Proof.* Let $V$ be open in $Y$; let $x_0$ be a point of $f^{-1}(V)$. We wish to find a neighborhood $U$ of $x_0$ such that $f(U) \\subset V$.\n\nLet $y_0 = f(x_0)$. First choose $\\epsilon$ so that the $\\epsilon$-ball $B(y_0, \\epsilon)$ is contained in $V$. Then, using uniform convergence, choose $N$ so that for all $n \\geq N$ and all $x \\in X$,\n\n$$d(f_n(x), f(x)) < \\epsilon / 3.$$\n\nFinally, using continuity of $f_N$, choose a neighborhood $U$ of $x_0$ such that $f_N$ carries $U$ into the $\\epsilon / 3$ ball in $Y$ centered at $f_N(x_0)$.\n\nWe claim that $f$ carries $U$ into $B(y_0, \\epsilon)$ and hence into $V$, as desired. For this purpose, note that if $x \\in U$, then\n\n$$d(f(x), f_N(x)) < \\epsilon / 3 \\quad \\text{(by choice of $N$)},$$\n\n$$d(f_N(x), f_N(x_0)) < \\epsilon / 3 \\quad \\text{(by choice of $U$)},$$\n\n$$d(f_N(x_0), f(x_0)) < \\epsilon / 3 \\quad \\text{(by choice of $N$)}.$$\n\nAdding and using the triangle inequality, we see that $d(f(x), f(x_0)) < \\epsilon$, as desired. ■\n",
        "structure": []
    },
    {
        "id": 10,
        "domain": "Abstract Algebra",
        "informal": "**2.3.7 Chinese Remainder Theorem** Let $R$ be an arbitrary ring, and let $I_1, \\ldots, I_n$ be ideals in $R$ that are relatively prime in pairs, that is, $I_i + I_j = R$ for all $i \\neq j$.\n\n(1) If $a_1 = 1$ (the multiplicative identity of $R$) and $a_j = 0$ (the zero element of $R$) for $j = 2, \\ldots, n$, then there is an element $a \\in R$ such that $a \\equiv a_i \\mod I_i$ for all $i = 1, \\ldots, n$. More generally,\n\n(2) If $a_1, \\ldots, a_n$ are arbitrary elements of $R$, there is an element $a \\in R$ such that $a \\equiv a_i \\mod I_i$ for all $i = 1, \\ldots, n$.\n\n(3) If $b$ is another element of $R$ such that $b \\equiv a_i \\mod I_i$ for all $i = 1, \\ldots, n$, then $b \\equiv a \\mod I_1 \\cap I_2 \\cap \\ldots \\cap I_n$. Conversely, if $b \\equiv a \\mod \\cap_{i=1}^n I_i$, then $b \\equiv a_i \\mod I_i$ for all $i$.\n\n(4) $R / \\cap_{i=1}^n I_i$ is isomorphic to the direct product $\\prod_{i=1}^n R / I_i$.\n\n*Proof.*  \n(1) If $j > 1$ we have $I_1 + I_j = R$, so there exist elements $b_j \\in I_1$ and $c_j \\in I_j$ such that $b_j + c_j = 1$; thus\n\n$$\\prod_{j=2}^n (b_j + c_j) = 1.$$\n\nExpand the left side and observe that any product containing at least one $b_j$ belongs to $I_1$, while $c_2 \\cdots c_n$ belongs to $\\prod_{j=2}^n I_j$, the collection of all finite sums of products $x_2 \\cdots x_n$ with $x_j \\in I_j$. Thus we have elements $b \\in I_1$ and $a \\in \\prod_{j=2}^n I_j$ (a subset of each $I_j$) with $b + a = 1$. Consequently, $a \\equiv 1 \\mod I_1$ and $a \\equiv 0 \\mod I_j$ for $j > 1$, as desired.\n\n(2) By the argument of part (1), for each $i$ we can find $c_i$ with $c_i \\equiv 1 \\mod I_i$ and $c_i \\equiv 0 \\mod I_j, j \\neq i$. If $a = a_1 c_1 + \\cdots + a_n c_n$, then $a$ has the desired properties. To see this, write $a - a_i = a - a_i c_i + a_i (c_i - 1)$, and note that $a - a_i c_i$ is the sum of the $a_j c_j, j \\neq i$, and is therefore congruent to $0 \\mod I_i$.\n\n(3) We have $b \\equiv a_i \\mod I_i$ for all $i$ iff $b - a \\equiv 0 \\mod I_i$ for all $i$, that is, iff $b - a \\in \\cap_{i=1}^n I_i$, and the result follows.\n\n(4) Define $f : R \\to \\prod_{i=1}^n R / I_i$ by $f(a) = (a + I_1, \\ldots, a + I_n)$. If $a_1, \\ldots, a_n \\in R$, then by part (2) there is an element $a \\in R$ such that $a \\equiv a_i \\mod I_i$ for all $i$. But then $f(a) = (a_1 + I_1, \\ldots, a_n + I_n)$, proving that $f$ is surjective. Since the kernel of $f$ is the intersection of the ideals $I_j$, the result follows from the first isomorphism theorem for rings. ♣",
        "structure": []
    },
    {
        "id": 10,
        "domain": "Analysis",
        "informal": "## Goal\n\nLet $\\{x_n\\}$ and $\\{y_n\\}$ be numerical sequences. Prove that if $\\lim_{n \\to \\infty} x_n = A$ and $\\lim_{n \\to \\infty} y_n = B$, then $\\lim_{n \\to \\infty} \\frac{x_n}{y_n} = \\frac{A}{B}$, provided $y_n \\neq 0$ $(n = 1, 2, \\ldots)$ and $B \\neq 0$.\n\n$Proof.$\n\nWe use the estimate\n\n$$\n\\left| \\frac{A}{B} - \\frac{x_n}{y_n} \\right| \\leq \\frac{|x_n| \\Delta(y_n) + |y_n| \\Delta(x_n)}{y_n^2} \\cdot \\frac{1}{1 - \\delta(y_n)},\n$$\n\nwhere $\\delta(y_n) = \\frac{\\Delta(y_n)}{|y_n|}$.\n\nFor a given $\\varepsilon > 0$ we find numbers $N'$ and $N''$ such that\n\n$$\n\\forall n > N' \\left( \\Delta(x_n) < \\min \\left\\{ 1, \\frac{\\varepsilon |B|}{8} \\right\\} \\right),\n$$\n\n$$\n\\forall n > N'' \\left( \\Delta(y_n) < \\min \\left\\{ \\frac{|B|}{4}, \\frac{\\varepsilon \\cdot B^2}{16(|A| + 1)} \\right\\} \\right).\n$$\n\nThen for $n > \\max\\{N', N''\\}$ we shall have\n\n$$\n|x_n| < |A| + \\Delta(x_n) < |A| + 1,\n$$\n\n$$\n|y_n| > |B| - \\Delta(y_n) > |B| - \\frac{|B|}{4} > \\frac{|B|}{2},\n$$\n\n$$\n\\frac{1}{|y_n|} < \\frac{2}{|B|},\n$$\n\n$$\n0 < \\delta(y_n) = \\frac{\\Delta(y_n)}{|y_n|} < \\frac{|B| / 4}{|B| / 2} = \\frac{1}{2},\n$$\n\n$$\n1 - \\delta(y_n) > \\frac{1}{2},\n$$\n\nand therefore\n\n$$\n|x_n| \\cdot \\frac{1}{y_n^2} \\Delta(y_n) < (|A| + 1) \\cdot \\frac{4}{B^2} \\cdot \\frac{\\varepsilon \\cdot B^2}{16(|A| + 1)} = \\frac{\\varepsilon}{4},\n$$\n\n$$\n\\left| \\frac{1}{y_n} \\right| \\Delta(x_n) < \\frac{2}{|B|} \\cdot \\frac{\\varepsilon |B|}{8} = \\frac{\\varepsilon}{4},\n$$\n\n$$\n0 < \\frac{1}{1 - \\delta(y_n)} < 2,\n$$\n\nand consequently\n\n$$\n\\left| \\frac{A}{B} - \\frac{x_n}{y_n} \\right| < \\varepsilon \\text{ when } n > N.\n$$\n\n$Qed.$",
        "structure": []
    },
    {
        "id": 10,
        "domain": "IMO",
        "informal": "## Goal\n\nLet $a_0, a_1, a_2, \\ldots$ be a sequence of positive integers such that the greatest common divisor of any two consecutive terms is greater than the preceding term; in symbols, $\\gcd(a_i, a_{i+1}) > a_{i-1}$. Prove that $a_n \\geq 2^n$ for all $n \\geq 0$.\n\n## Prove\n\nSince $a_i \\geq \\gcd(a_i, a_{i+1}) > a_{i-1}$, the sequence is strictly increasing. In particular $a_0 \\geq 1$, $a_1 \\geq 2$. For each $i \\geq 1$ we also have $a_{i+1} - a_i \\geq \\gcd(a_i, a_{i+1}) > a_{i-1}$, and consequently $a_{i+1} \\geq a_i + a_{i-1} + 1$. Hence $a_2 \\geq 4$ and $a_3 \\geq 7$. The equality $a_3 = 7$ would force equalities in the previous estimates, leading to $\\gcd(a_2, a_3) = \\gcd(4, 7) > a_1 = 2$, which is false. Thus $a_3 \\geq 8$; the result is valid for $n = 0, 1, 2, 3$. These are the base cases for a proof by induction.\n\nTake any $n \\geq 3$ and assume that $a_i \\geq 2^i$ for $i = 0, 1, \\ldots, n$. We must show that $a_{n+1} \\geq 2^{n+1}$. Let $\\gcd(a_n, a_{n+1}) = d$. We know that $d > a_{n-1}$. The induction claim is reached immediately in the following cases:\n\n- if $a_{n+1} \\geq 4d$ then $a_{n+1} > 4a_{n-1} \\geq 4 \\cdot 2^{n-1} = 2^{n+1}$;\n- if $a_n \\geq 3d$ then $a_{n+1} \\geq a_n + d \\geq 4d > 4a_{n-1} \\geq 4 \\cdot 2^{n-1} = 2^{n+1}$;\n- if $a_n = d$ then $a_{n+1} \\geq a_n + d = 2a_n \\geq 2 \\cdot 2^n = 2^{n+1}$.\n\nThe only remaining possibility is that $a_n = 2d$ and $a_{n+1} = 3d$, which we assume for the sequel. So $a_{n+1} = \\frac{3}{2}a_n$.\n\nLet now $\\gcd(a_{n-1}, a_n) = d'$; then $d' > a_{n-2}$. Write $a_n = md'$ ($m$ an integer). Keeping in mind that $d' \\leq a_{n-1} < d$ and $a_n = 2d$, we get that $m \\geq 3$. Also $a_{n-1} < d = \\frac{1}{2}md'$, $a_{n+1} = \\frac{3}{2}md'$. Again we single out the cases which imply the induction claim immediately:\n\n- if $m \\geq 6$ then $a_{n+1} = \\frac{3}{2}md' \\geq 9d' > 9a_{n-2} \\geq 9 \\cdot 2^{n-2} = 2^{n+1}$;\n- if $3 \\leq m \\leq 4$ then $a_{n-1} < \\frac{1}{2} \\cdot 4d'$, and hence $a_{n-1} = d'$, $a_{n+1} = \\frac{3}{2}ma_{n-1} \\geq \\frac{3}{2} \\cdot 3a_{n-1} = \\frac{9}{2} \\cdot 2^{n-2} = 2^{n+1}$.\n\nSo we are left with the case $m = 5$, which means that $a_n = 5d'$, $a_{n+1} = \\frac{15}{2}d'$, and $a_{n-1} < d = \\frac{5}{2}d'$. The last relation implies that $a_{n-1}$ is either $d'$ or $2d'$. Anyway, $a_n \\geq 2d'$.\n\nThe same pattern repeats once more. We denote $\\gcd(a_{n-2}, a_{n-1}) = d''$, then $d'' > a_{n-3}$. Because $d''$ is a divisor of $a_{n-1}$, hence also of $2d'$, we may write $2d' = m'd''$ ($m'$ an integer). Since $d'' < a_{n-2} < d'$, we get $m' \\geq 3$. Also, $a_{n-2} < d' = \\frac{1}{2}m'd''$, $a_{n+1} = \\frac{15}{4}m'd''$. As before, we consider the cases:\n\n- if $m' \\geq 5$ then $a_{n+1} = \\frac{15}{4}m'd'' \\geq \\frac{75}{4}d'' > \\frac{75}{4}a_{n-3} \\geq \\frac{75}{4} \\cdot 2^{n-3} > 2^{n+1}$;\n- if $3 \\leq m' \\leq 4$ then $a_{n-2} < \\frac{1}{2} \\cdot 4d''$, and hence $a_{n-2} = d''$, $a_{n+1} = \\frac{15}{4}m'a_{n-2} = \\frac{15}{4} \\cdot 3a_{n-2} \\geq \\frac{45}{4} \\cdot 2^{n-2} > 2^{n+1}$.\n\nBoth of them have produced the induction claim. But now there are no cases left. Induction is complete; the inequality $a_n \\geq 2^n$ holds for all $n$.\n\n$Qed.$\n",
        "structure": []
    },
    {
        "id": 10,
        "domain": "Linear Algebra",
        "informal": "5.22 *existence, uniqueness, and degree of minimal polynomial*\n\nSuppose $V$ is finite-dimensional and $T \\in \\mathcal{L}(V)$. Then there is a unique monic polynomial $p \\in \\mathcal{P}(\\mathbf{F})$ of smallest degree such that $p(T) = 0$. Furthermore, $\\deg p \\leq \\dim V$.\n\n**Proof** If $\\dim V = 0$, then $I$ is the zero operator on $V$ and thus we take $p$ to be the constant polynomial 1.\n\nNow use induction on $\\dim V$. Thus assume that $\\dim V > 0$ and that the desired result is true for all operators on all complex vector spaces of smaller dimension. Let $v \\in V$ be such that $v \\neq 0$. The list $v, Tv, ..., T^{\\dim V} v$ has length $1 + \\dim V$ and thus is linearly dependent. By the linear dependence lemma (2.19), there is a smallest positive integer $m \\leq \\dim V$ such that $T^m v$ is a linear combination of $v, Tv, ..., T^{m-1} v$. Thus there exist scalars $c_0, c_1, c_2, ..., c_{m-1} \\in \\mathbf{F}$ such that\n\n5.23\n$$c_0 v + c_1 T v + \\cdots + c_{m-1} T^{m-1} v + T^m v = 0.$$\n\nDefine a monic polynomial $q \\in \\mathcal{P}_m(\\mathbf{F})$ by\n\n$$q(z) = c_0 + c_1 z + \\cdots + c_{m-1} z^{m-1} + z^m.$$\n\nThen 5.23 implies that $q(T) v = 0$.\n\nIf $k$ is a nonnegative integer, then\n\n$$q(T)(T^k v) = T^k (q(T) v) = T^k(0) = 0.$$\n\nThe linear dependence lemma (2.19) shows that $v, Tv, ..., T^{m-1} v$ is linearly independent. Thus the equation above implies that $\\dim \\operatorname{null} q(T) \\geq m$. Hence\n\n$$\\dim \\operatorname{range} q(T) = \\dim V - \\dim \\operatorname{null} q(T) \\leq \\dim V - m.$$\n\nBecause $\\operatorname{range} q(T)$ is invariant under $T$ (by 5.18), we can apply our induction hypothesis to the operator $T|_{\\operatorname{range} q(T)}$ on the vector space $\\operatorname{range} q(T)$. Thus there is a monic polynomial $s \\in \\mathcal{P}(\\mathbf{F})$ with\n\n$$\\deg s \\leq \\dim V - m \\quad \\text{and} \\quad s(T|_{\\operatorname{range} q(T)}) = 0.$$\n\nHence for all $v \\in V$ we have\n\n$$(sq)(T)(v) = s(T)(q(T) v) = 0$$\n\nbecause $q(T) v \\in \\operatorname{range} q(T)$ and $s(T)|_{\\operatorname{range} q(T)} = s(T|_{\\operatorname{range} q(T)}) = 0$. Thus $sq$ is a monic polynomial such that $\\deg sq \\leq \\dim V$ and $(sq)(T) = 0$.\n\nThe paragraph above shows that there is a monic polynomial of degree at most $\\dim V$ that when applied to $T$ gives the 0 operator. Thus there is a monic polynomial of smallest degree with this property, completing the existence part of this result.\n\nLet $p \\in \\mathcal{P}(\\mathbf{F})$ be a monic polynomial of smallest degree such that $p(T) = 0$. To prove the uniqueness part of the result, suppose $r \\in \\mathcal{P}(\\mathbf{F})$ is a monic polynomial of the same degree as $p$ and $r(T) = 0$. Then $(p - r)(T) = 0$ and also $\\deg(p - r) < \\deg p$. If $p - r$ were not equal to 0, then we could divide $p - r$ by the coefficient of the highest-order term in $p - r$ to get a monic polynomial (of smaller degree than $p$) that when applied to $T$ gives the 0 operator. Thus $p - r = 0$, as desired.\n",
        "structure": []
    },
    {
        "id": 10,
        "domain": "Measure Theory",
        "informal": "2.60  *measure of a decreasing intersection*\n\nSuppose $(X, S, \\mu)$ is a measure space and $E_1 \\supset E_2 \\supset \\cdots$ is a decreasing sequence of sets in $S$, with $\\mu(E_1) < \\infty$. Then\n\n$$\n\\mu \\left( \\bigcap_{k=1}^\\infty E_k \\right) = \\lim_{k \\to \\infty} \\mu(E_k).\n$$\n\n**Proof** One of De Morgan’s Laws tells us that\n\n$$\nE_1 \\setminus \\bigcap_{k=1}^\\infty E_k = \\bigcup_{k=1}^\\infty (E_1 \\setminus E_k).\n$$\n\nNow $E_1 \\setminus E_1 \\subset E_1 \\setminus E_2 \\subset E_1 \\setminus E_3 \\subset \\cdots$ is an increasing sequence of sets in $S$. Thus 2.59, applied to the equation above, implies that\n\n$$\n\\mu \\left( E_1 \\setminus \\bigcap_{k=1}^\\infty E_k \\right) = \\lim_{k \\to \\infty} \\mu(E_1 \\setminus E_k).\n$$\n\nUse 2.57(b) to rewrite the equation above as\n\n$$\n\\mu(E_1) - \\mu \\left( \\bigcap_{k=1}^\\infty E_k \\right) = \\mu(E_1) - \\lim_{k \\to \\infty} \\mu(E_k),\n$$\n\nwhich implies our desired result.\n",
        "structure": []
    },
    {
        "id": 10,
        "domain": "Probability Theory",
        "informal": "**Theorem 9.23** (Martingale Convergence Theorem). Let $\\{X_n, \\mathcal{F}_n, n=0,1,2,\\ldots\\}$ be a submartingale. Suppose that $\\sup_n E\\{|X_n|\\} < \\infty$. Then there exists a finite random variable $X_\\infty$ such that\n\n$$\n\\lim_{n \\to \\infty} X_n = X_\\infty \\quad a.s.\n$$\n\n**Proof.** First let us show that $\\liminf_{n \\to \\infty} X_n = \\limsup_{n \\to \\infty} X_n$ a.s. If this is not true, then $\\liminf_n X_n < \\limsup_n X_n$ with positive probability. Fix $\\omega$ for which $\\liminf_n X_n(\\omega) < \\limsup_n X_n(\\omega)$. (Once we have fixed $\\omega$, we are just dealing with a sequence of real numbers.) Then there exist rational $a < b$ for which $\\liminf_n X_n(\\omega) < a < b < \\limsup_n X_n(\\omega)$. Then there exist infinitely many $n$ for which $X_n < a$, and infinitely many $n$ for which $X_n > b$. But for this to happen, the process must cross $[a,b]$ infinitely often. Thus, $\\nu_\\infty(a,b)(\\omega) = \\infty$.\n\nBy the upcrossing inequality,\n\n$$\nE\\{\\nu_\\infty(a,b)\\} \\leq \\frac{\\sup_N E\\{(X_N - a)^+\\}}{b - a} \\leq \\frac{\\sup_N E\\{|X_N|\\} + |a|}{b - a},\n$$\n\nand this is finite by hypothesis. Thus, $P\\{\\nu_\\infty(a,b) = \\infty\\} = 0$. This is true simultaneously for each of the countable number of rational pairs $a < b$, and it follows that the set of $\\omega$ for which $\\liminf_n X_n(\\omega) < \\limsup_n X_n(\\omega)$ has probability zero. In short, $P\\{\\liminf_n X_n = \\limsup_n X_n\\} = 1$, so the limit exists almost surely. Thus, $X_\\infty \\overset{\\text{def}}{=} \\lim_n X_n = \\liminf_n X_n = \\limsup_n X_n$ exists. A priori it might be infinite. However, by Fatou's Lemma,\n\n$$\nE\\{|X_\\infty|\\} \\leq \\liminf_{n \\to \\infty} E\\{|X_n|\\} < \\infty.\n$$\n\nThus, $X_\\infty$ is finite a.s.  □\n",
        "structure": []
    },
    {
        "id": 10,
        "domain": "Topology Point Set",
        "informal": "**Theorem 20.1.** Let $X$ be a metric space with metric $d$. Define $\\bar{d} : X \\times X \\to \\mathbb{R}$ by the equation\n\n$$\\bar{d}(x,y) = \\min\\{d(x,y), 1\\}.$$\n\nThen $\\bar{d}$ is a metric that induces the same topology as $d$.\n\nThe metric $\\bar{d}$ is called the *standard bounded metric* corresponding to $d$.\n\n*Proof.* Checking the first two conditions for a metric is trivial. Let us check the triangle inequality:\n\n$$\\bar{d}(x,z) \\leq \\bar{d}(x,y) + \\bar{d}(y,z).$$\n\nNow if either $d(x,y) \\geq 1$ or $d(y,z) \\geq 1$, then the right side of this inequality is at least 1; since the left side is (by definition) at most 1, the inequality holds. It remains to consider the case in which $d(x,y) < 1$ and $d(y,z) < 1$. In this case, we have\n\n$$d(x,z) \\leq d(x,y) + d(y,z) = \\bar{d}(x,y) + \\bar{d}(y,z).$$\n\nSince $\\bar{d}(x,z) \\leq d(x,z)$ by definition, the triangle inequality holds for $\\bar{d}$.\n\nNow we note that in any metric space, the collection of $\\epsilon$-balls with $\\epsilon < 1$ forms a basis for the metric topology, for every basis element containing $x$ contains such an $\\epsilon$-ball centered at $x$. It follows that $d$ and $\\bar{d}$ induce the same topology on $X$, because the collections of $\\epsilon$-balls with $\\epsilon < 1$ under these two metrics are the same collection. ■\n",
        "structure": []
    },
    {
        "id": 11,
        "domain": "Abstract Algebra",
        "informal": "**2.4.3 Theorem** Let $M$ be an ideal in the commutative ring $R$. Then $M$ is a maximal ideal if and only if $R/M$ is a field.\n\n*Proof.* Suppose $M$ is maximal. We know that $R/M$ is a ring (see (2.2.4)); we need to find the multiplicative inverse of the element $a + M$ of $R/M$, where $a + M$ is not the zero element, i.e., $a \\notin M$. Since $M$ is maximal, the ideal $Ra + M$, which contains $a$ and is therefore strictly larger than $M$, must be the ring $R$ itself. Consequently, the identity element $1$ belongs to $Ra + M$. If $1 = ra + m$ where $r \\in R$ and $m \\in M$, then\n\n$$(r + M)(a + M) = ra + M = (1 - m) + M = 1 + M \\text{ since } m \\in M,$$\n\nproving that $r + M$ is the multiplicative inverse of $a + M$.\n\nConversely, if $R/M$ is a field, then $M$ must be a proper ideal. If not, then $M = R$, so that $R/M$ contains only one element, contradicting the requirement that $1 \\neq 0$ in $R/M$ (see (7) of (2.1.1)). By (2.2.6), the only ideals of $R/M$ are $\\{0\\}$ and $R/M$, so by the correspondence theorem (2.3.5), there are no ideals properly between $M$ and $R$. Therefore $M$ is a maximal ideal. ♣",
        "structure": []
    },
    {
        "id": 11,
        "domain": "Analysis",
        "informal": "## Goal\n\nLet $\\{x_n\\}$ and $\\{y_n\\}$ be two convergent sequences with $\\lim_{n \\to \\infty} x_n = A$ and $\\lim_{n \\to \\infty} y_n = B$. Prove that if $A < B$, then there exists an index $N \\in \\mathbb{N}$ such that $x_n < y_n$ for all $n > N$.\n\n$Proof.$\n\nChoose a number $C$ such that $A < C < B$. By definition of limit, we can find numbers $N'$ and $N''$ such that $|x_n - A| < C - A$ for all $n > N'$ and $|y_n - B| < B - C$ for all $n > N''$. Then for $n > N = \\max\\{N', N''\\}$ we shall have $x_n < A + C - A = C = B - (B - C) < y_n$.\n\n$Qed.$\n\n",
        "structure": []
    },
    {
        "id": 11,
        "domain": "IMO",
        "informal": "## Goal\n\nProve that there exist infinitely many positive integers $n$ such that $n^2 + 1$ has a prime divisor greater than $2n + \\sqrt{10n}$.\n\n## Proof\n\nLet $p \\equiv 1 \\pmod{8}$ be a prime. The congruence $x^2 \\equiv -1 \\pmod{p}$ has two solutions in $[1, p-1]$ whose sum is $p$. If $n$ is the smaller one of them then $p$ divides $n^2 + 1$ and $n \\leq (p-1)/2$. We show that $p > 2n + \\sqrt{10n}$.\n\nLet $n = (p-1)/2 - \\ell$ where $\\ell \\geq 0$. Then $n^2 \\equiv -1 \\pmod{p}$ gives\n\n$$\n\\left(\\frac{p-1}{2} - \\ell\\right)^2 \\equiv -1 \\pmod{p}\n$$\n\nor\n\n$$\n(2\\ell + 1)^2 + 4 \\equiv 0 \\pmod{p}.\n$$\n\nThus $(2\\ell + 1)^2 + 4 = rp$ for some $r \\geq 0$. As $(2\\ell + 1)^2 \\equiv 1 \\pmod{8} \\equiv p \\pmod{8}$, we have $r \\equiv 5 \\pmod{8}$, so that $r \\geq 5$. Hence $(2\\ell + 1)^2 + 4 \\geq 5p$, implying $\\ell \\geq (\\sqrt{5p - 4} - 1)/2$. Set $\\sqrt{5p - 4} = u$ for clarity; then $\\ell \\geq (u - 1)/2$. Therefore\n\n$$\nn = \\frac{p-1}{2} - \\ell \\leq \\frac{1}{2} (p - u).\n$$\n\nCombined with $p = (u^2 + 4)/5$, this leads to $u^2 - 5u - 10n + 4 \\geq 0$. Solving this quadratic inequality with respect to $u \\geq 0$ gives $u \\geq (5 + \\sqrt{40n + 9})/2$. So the estimate $n \\leq (p - u)/2$ leads to\n\n$$\np \\geq 2n + u \\geq 2n + \\frac{1}{2}(5 + \\sqrt{40n + 9}) > 2n + \\sqrt{10n}.\n$$\n\nSince there are infinitely many primes of the form $8k + 1$, it follows easily that there are also infinitely many $n$ with the stated property.",
        "structure": []
    },
    {
        "id": 12,
        "domain": "Abstract Algebra",
        "informal": "**2.6.8 Theorem** Every principal ideal domain is a unique factorization domain. For short, PID implies UFD.\n\n*Proof.* If $<a_1> \\subseteq <a_2> \\subseteq \\ldots$, let $I = \\cup_i <a_i>$. Then $I$ is an ideal, necessarily principal by hypothesis. If $I = <b>$ then $b$ belongs to some $<a_n>$, so $I \\subseteq <a_n>$. Thus if $i \\geq n$ we have $<a_i> \\subseteq I \\subseteq <a_n> \\subseteq <a_i>$. Therefore $<a_i> = <a_n>$ for all $i \\geq n$, so that $R$ satisfies the acc on principal ideals.\n\nNow suppose that $a$ is irreducible. Then $<a>$ is a proper ideal, for if $<a> = R$ then $1 \\in <a>$, so that $a$ is a unit. By the acc on principal ideals, $<a>$ is contained in a maximal ideal $I$. (Note that we need not appeal to the general result (2.4.2), which uses Zorn’s lemma.) If $I = <b>$ then $b$ divides the irreducible element $a$, and $b$ is not a unit since $I$ is proper. Thus $a$ and $b$ are associates, so $<a> = <b> = I$. But $I$, a maximal ideal, is prime by (2.4.6), hence $a$ is prime. The result follows from (2.6.6). ♣",
        "structure": []
    },
    {
        "id": 12,
        "domain": "Analysis",
        "informal": "## Goal\n\nSuppose the sequences $\\{x_n\\}$, $\\{y_n\\}$, and $\\{z_n\\}$ are such that $x_n \\leq y_n \\leq z_n$ for all $n > N \\in \\mathbb{N}$. Prove that if the sequences $\\{x_n\\}$ and $\\{z_n\\}$ both converge to the same limit, then the sequence $\\{y_n\\}$ also converges to that limit.\n\n$Proof.$\n\nSuppose $\\lim_{n \\to \\infty} x_n = \\lim_{n \\to \\infty} z_n = A$. Given $\\varepsilon > 0$ choose $N'$ and $N''$ such that $A - \\varepsilon < x_n$ for all $n > N'$ and $z_n < A + \\varepsilon$ for all $n > N''$. Then for $n > N = \\max\\{N', N''\\}$ we shall have $A - \\varepsilon < x_n \\leq y_n \\leq z_n < A + \\varepsilon$, which says $|y_n - A| < \\varepsilon$, that is $A = \\lim_{n \\to \\infty} y_n$.\n\n$Qed.$",
        "structure": []
    },
    {
        "id": 12,
        "domain": "IMO",
        "informal": "## Goal\n\nLet $a, b, c$ be positive real numbers such that $\\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} = a + b + c$. Prove that\n\n$$\n\\frac{1}{(2a + b + c)^2} + \\frac{1}{(2b + c + a)^2} + \\frac{1}{(2c + a + b)^2} \\leq \\frac{3}{16}.\n$$\n\n## Proof\n\nFor positive real numbers $x, y, z$, from the arithmetic-geometric-mean inequality,\n\n$$\n2x + y + z = (x + y) + (x + z) \\geq 2\\sqrt{(x + y)(x + z)},\n$$\n\nwe obtain\n\n$$\n\\frac{1}{(2x + y + z)^2} \\leq \\frac{1}{4(x + y)(x + z)}.\n$$\n\nApplying this to the left-hand side terms of the inequality to prove, we get\n$$\n\\frac{1}{(2a + b + c)^2} + \\frac{1}{(2b + c + a)^2} + \\frac{1}{(2c + a + b)^2}\n$$\n\n$$\n\\leq \\frac{1}{4(a + b)(a + c)} + \\frac{1}{4(b + c)(b + a)} + \\frac{1}{4(c + a)(c + b)}\n$$\n\n$$\n= \\frac{(b + c) + (c + a) + (a + b)}{4(a + b)(b + c)(c + a)} = \\frac{a + b + c}{2(a + b)(b + c)(c + a)}\\tag{1}\n$$\n\nA second application of the inequality of the arithmetic-geometric mean yields\n\n$$\na^2b + a^2c + b^2a + b^2c + c^2a + c^2b \\geq 6abc,\n$$\n\nor, equivalently,\n\n$$\n9(a + b)(b + c)(c + a) \\geq 8(a + b + c)(ab + bc + ca). \\tag{2}\n$$\n\nThe supposition $\\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} = a + b + c$ can be written as\n$$\nab + bc + ca = abc(a + b + c). \\tag{3}\n$$\n\nApplying the arithmetic-geometric-mean inequality $x^2y^2 + x^2z^2 \\geq 2x^2yz$ thrice, we get\n\n$$\na^2b^2 + b^2c^2 + c^2a^2 \\geq a^2bc + ab^2c + abc^2,\n$$\n\nwhich is equivalent to\n\n$$\n(ab + bc + ca)^2 \\geq 3abc(a + b + c). \\tag{4}\n$$\n\nCombining (1), (2), (3), and (4), we will finish the proof:\n$$\n\\frac{a + b + c}{2(a + b)(b + c)(c + a)} = \\frac{(a + b + c)(ab + bc + ca)}{2(a + b)(b + c)(c + a)} \\cdot \\frac{ab + bc + ca}{abc(a + b + c)} = \\frac{abc(a + b + c)}{(ab + bc + ca)^2}\n$$\n\n$$\n\\leq \\frac{9}{2} \\cdot \\frac{1}{8} \\cdot \\frac{1}{3} = \\frac{3}{16}.\n$$\n",
        "structure": []
    },
    {
        "id": 13,
        "domain": "Abstract Algebra",
        "informal": "**3.1.3 Theorem** Let $f$ be a nonconstant polynomial over the field $F$. Then there is an extension $E/F$ and an element $\\alpha \\in E$ such that $f(\\alpha) = 0$.\n\n*Proof.* Since $f$ can be factored into irreducibles, we may assume without loss of generality that $f$ itself is irreducible. The ideal $I = <f(X)>$ in $F[X]$ is prime (see (2.6.1)), in fact maximal (see (2.6.9)). Thus $E = F[X]/I$ is a field by (2.4.3). We have a problem at this point because $F$ need not be a subset of $E$, but we can place an isomorphic copy of $F$ inside $E$ via the homomorphism $h : a \\to a + I$; by (3.1.2), $h$ is a monomorphism, so we may identify $F$ with a subfield of $E$. Now let $\\alpha = X + I$; if $f(X) = a_0 + a_1 X + \\cdots + a_n X^n$, then\n\n$$\n\\begin{aligned}\nf(\\alpha) &= (a_0 + I) + a_1 (X + I) + \\cdots + a_n (X + I)^n \\\\\n&= (a_0 + a_1 X + \\cdots + a_n X^n) + I \\\\\n&= f(X) + I\n\\end{aligned}\n$$\n\nwhich is zero in $E$. ♣",
        "structure": []
    },
    {
        "id": 13,
        "domain": "Analysis",
        "informal": "## Goal\n\n$\\lim_{n \\to \\infty} \\frac{n}{q^n} = 0$ if $q > 1$.\n\n## Proof\n\nIndeed, if $x_n = \\frac{n}{q^n}$, then $x_{n+1} = \\frac{n+1}{nq} x_n$ for $n \\in \\mathbb{N}$. Since $\\lim_{n \\to \\infty} \\frac{n+1}{nq} = \\lim_{n \\to \\infty} \\left( \\frac{1 + \\frac{1}{n}}{q} \\right) = \\lim_{n \\to \\infty} \\left( 1 + \\frac{1}{n} \\right) \\cdot \\lim_{n \\to \\infty} \\frac{1}{q} = 1 \\cdot \\frac{1}{q} = \\frac{1}{q} < 1$, there exists an index $N$ such that $\\frac{n+1}{nq} < 1$ for $n > N$. Thus we shall have $x_{n+1} < x_n$ for $n > N$, so that the sequence will be monotonically decreasing from index $N$ on. As one can see from the definition of a limit, a finite set of terms of a sequence has no effect on the convergence of a sequence or its limit, so that it now suffices to find the limit of the sequence $x_{N+1} > x_{N+2} > \\cdots$.\n\nThe terms of this sequence are positive, that is, the sequence is bounded below. Therefore it has a limit.\n\nLet $x = \\lim_{n \\to \\infty} x_n$. It now follows from the relation $x_{n+1} = \\frac{n+1}{nq} x_n$ that\n\n$$\nx = \\lim_{n \\to \\infty} (x_{n+1}) = \\lim_{n \\to \\infty} \\left( \\frac{n+1}{nq} x_n \\right) = \\lim_{n \\to \\infty} \\frac{n+1}{nq} \\cdot \\lim_{n \\to \\infty} x_n = \\frac{1}{q} x,\n$$\n\nfrom which we find $\\left( 1 - \\frac{1}{q} \\right) x = 0$, and so $x = 0$.",
        "structure": []
    },
    {
        "id": 13,
        "domain": "IMO",
        "informal": "## Goal\n\nLet $a, b, c$ be positive real numbers such that $ab + bc + ca \\leq 3abc$. Prove that\n$$\n\\sqrt{\\frac{a^2 + b^2}{a + b}} + \\sqrt{\\frac{b^2 + c^2}{b + c}} + \\sqrt{\\frac{c^2 + a^2}{c + a}} + 3 \\leq \\sqrt{2} \\left( \\sqrt{a + b} + \\sqrt{b + c} + \\sqrt{c + a} \\right).\n$$\n\n## Proof\n\nStarting with the terms of the right-hand side, the quadratic-arithmetic-mean inequality yields\n$$\n\\sqrt{2} \\sqrt{a + b} = 2 \\sqrt{\\frac{ab}{a + b}} \\sqrt{\\frac{1}{2} \\left( 2 + \\frac{a^2 + b^2}{ab} \\right)}\n$$\n\n$$\n\\geq 2 \\sqrt{\\frac{ab}{a + b}} \\cdot \\frac{1}{2} \\left( \\sqrt{2} + \\sqrt{\\frac{a^2 + b^2}{ab}} \\right) = \\sqrt{\\frac{2ab}{a + b}} + \\sqrt{\\frac{a^2 + b^2}{a + b}}.\n$$\n\nand, analogously,\n\n$$\n\\sqrt{2} \\sqrt{b + c} \\geq \\sqrt{\\frac{2bc}{b + c}} + \\sqrt{\\frac{b^2 + c^2}{b + c}},\n$$\n\n$$\n\\sqrt{2} \\sqrt{c + a} \\geq \\sqrt{\\frac{2ca}{c + a}} + \\sqrt{\\frac{c^2 + a^2}{c + a}}.\n$$\n\nApplying the inequality between the arithmetic mean and the squared harmonic mean will finish the proof:\n\n$$\n\\sqrt{\\frac{2ab}{a + b}} + \\sqrt{\\frac{2bc}{b + c}} + \\sqrt{\\frac{2ca}{c + a}} \\geq 3 \\cdot \\sqrt{\\frac{3}{\\frac{a + b}{2ab} + \\frac{b + c}{2bc} + \\frac{c + a}{2ca}}} = 3 \\cdot \\sqrt{\\frac{3ab}{ab + bc + ca}} \\geq 3.\n$$\n\n",
        "structure": []
    },
    {
        "id": 14,
        "domain": "Abstract Algebra",
        "informal": "**3.1.7 Theorem** If $\\alpha \\in E$ is algebraic over $F$ and the minimal polynomial $m(X)$ of $\\alpha$ over $F$ has degree $n$, then $F(\\alpha) = F[\\alpha]$, the set of polynomials in $\\alpha$ with coefficients in $F$. In fact, $F[\\alpha]$ is the set $F_{n-1}[\\alpha]$ of all polynomials of degree at most $n-1$ with coefficients in $F$, and $1, \\alpha, \\ldots, \\alpha^{n-1}$ form a basis for the vector space $F[\\alpha]$ over the field $F$. Consequently, $[F(\\alpha) : F] = n$.\n\n*Proof.* Let $f(X)$ be any nonzero polynomial over $F$ of degree $n-1$ or less. Then since $m(X)$ is irreducible and $\\deg f < \\deg m$, $f(X)$ and $m(X)$ are relatively prime, and there are polynomials $a(X)$ and $b(X)$ over $F$ such that $a(X)f(X) + b(X)m(X) = 1$. But then $a(\\alpha)f(\\alpha) = 1$, so that any nonzero element of $F_{n-1}[\\alpha]$ has a multiplicative inverse. It follows that $F_{n-1}[\\alpha]$ is a field. (This may not be obvious, since the product of two polynomials of degree $n-1$ or less can have degree greater than $n-1$, but if $\\deg g > n-1$, then divide $g$ by $m$ to get $g(X) = q(X)m(X) + r(X)$ where $\\deg r(X) < \\deg m(X) = n$. Replace $X$ by $\\alpha$ to get $g(\\alpha) = r(\\alpha) \\in F_{n-1}[\\alpha]$. Less abstractly, if $m(\\alpha) = \\alpha^3 + \\alpha + 1 = 0$, then $\\alpha^3 = -\\alpha - 1$, $\\alpha^4 = -\\alpha^2 - \\alpha$, and so on.)\n\nNow any field containing $F$ and $\\alpha$ must contain all polynomials in $\\alpha$, in particular all polynomials of degree at most $n-1$. Therefore $F_{n-1}[\\alpha] \\subseteq F[\\alpha] \\subseteq F(\\alpha)$. But $F(\\alpha)$ is the smallest field containing $F$ and $\\alpha$, so $F(\\alpha) \\subseteq F_{n-1}[\\alpha]$, and we conclude that $F(\\alpha) = F[\\alpha] = F_{n-1}[\\alpha]$. Finally, the elements $1, \\alpha, \\ldots, \\alpha^{n-1}$ certainly span $F_{n-1}[\\alpha]$, and they are linearly independent because if a nontrivial linear combination of these elements were zero, we would have a nonzero polynomial of degree less than that of $m(X)$ with $\\alpha$ as a root, contradicting (2) of (3.1.6). ♣",
        "structure": []
    },
    {
        "id": 14,
        "domain": "Analysis",
        "informal": "## Goal\n\n$$\n\\lim_{n \\to \\infty} \\sqrt[n]{n} = 1.\n$$\n\n**Proof.** By what was just proved, for a given $\\epsilon > 0$ there exists $N \\in \\mathbb{N}$ such that $1 \\leq n < (1 + \\epsilon)^n$ for all $n > N$. Then for $n > N$ we obtain $1 \\leq \\sqrt[n]{n} < 1 + \\epsilon$ and hence $\\lim_{n \\to \\infty} \\sqrt[n]{n} = 1$.",
        "structure": []
    },
    {
        "id": 14,
        "domain": "IMO",
        "informal": "## Goal\n\nA sequence $x_1, x_2, \\ldots$ is defined by $x_1 = 1$ and $x_{2k} = -x_k, x_{2k-1} = (-1)^{k+1} x_k$ for all $k \\geq 1$. Prove that $x_1 + x_2 + \\cdots + x_n \\geq 0$ for all $n \\geq 1$.\n\n## Proof\n\nWe start with some observations. First, from the definition of $x_i$ it follows that for each positive integer $k$ we have\n$$\nx_{4k-3} = x_{2k-1} = -x_{4k-2} \\quad \\text{and} \\quad x_{4k-1} = x_{4k} = -x_{2k} = x_k.\n$$\n\nHence, denoting $S_n = \\sum_{i=1}^n x_i$, we have\n\n$$\nS_{4k} = \\sum_{i=1}^k ((x_{4k-3} + x_{4k-2}) + (x_{4k-1} + x_{4k})) = \\sum_{i=1}^k (0 + 2x_k) = 2S_k,\n$$\n\n$$\nS_{4k+2} = S_{4k} + (x_{4k+1} + x_{4k+2}) = S_{4k}.\n$$\n\nObserve also that $S_n = \\sum_{i=1}^n x_i \\equiv \\sum_{i=1}^n 1 = n \\ (\\text{mod} \\ 2)$.\n\nNow we prove by induction on $k$ that $S_i \\geq 0$ for all $i \\leq 4k$. The base case is valid since $x_1 = x_3 = x_4 = 1, x_2 = -1$. For the induction step, assume that $S_i \\geq 0$ for all $i \\leq 4k$. Using the relations (1)-(3), we obtain\n\n$$\nS_{4k+4} = 2S_{k+1} \\geq 0, \\quad S_{4k+2} = S_{4k} \\geq 0, \\quad S_{4k+3} = S_{4k+2} + x_{4k+3} = \\frac{S_{4k+2} + S_{4k+4}}{2} \\geq 0.\n$$\n\nSo, we are left to prove that $S_{4k+1} \\geq 0$. If $k$ is odd, then $S_{4k} = 2S_k \\geq 0$; since $k$ is odd, $S_k$ is odd as well, so we have $S_{4k} \\geq 2$ and hence $S_{4k+1} = S_{4k} + x_{4k+1} \\geq 1$.\n\nConversely, if $k$ is even, then we have $x_{4k+1} = x_{2k+1} = x_{k+1}$, hence $S_{4k+1} = S_{4k} + x_{4k+1} = 2S_k + x_{k+1} = S_k + S_{k+1} \\geq 0$. The step is proved.",
        "structure": []
    },
    {
        "id": 15,
        "domain": "Abstract Algebra",
        "informal": "**3.5.5 Theorem** The finite extension $E/F$ is normal if and only if every $F$-monomorphism of $E$ into an algebraic closure $C$ is actually an $F$-automorphism of $E$.\n\n*Proof.* If $E/F$ is normal, then as in (3.5.1), an $F$-monomorphism $\\tau$ of $E$ into $C$ must map each element of $E$ to one of its conjugates. Thus by hypothesis, $\\tau(E) \\subseteq E$. But $\\tau(E)$ is an isomorphic copy of $E$, so it must have the same degree as $E$ over $F$. Since the degree is assumed finite, we have $\\tau(E) = E$. (All we are saying here is that an $m$-dimensional subspace of an $m$-dimensional vector space is the entire space.) Conversely, let $\\alpha \\in E$, and let $\\beta$ be any conjugate of $\\alpha$ over $F$. As in the proof of (3.5.2), there is an $F$-monomorphism of $E$ into $C$ that carries $\\alpha$ to $\\beta$. If all such embeddings are $F$-automorphisms of $E$, we must have $\\beta \\in E$, and we conclude that $E$ is normal over $F$. ♣",
        "structure": []
    },
    {
        "id": 15,
        "domain": "Analysis",
        "informal": "## Goal\n\nIf there exists $ r > 0 $ such that for $ 0 < |x - x_0| < r $, we have $ g(x) \\leq f(x) \\leq h(x) $, and $ \\lim_{x \\to x_0} g(x) = \\lim_{x \\to x_0} h(x) = A $, then $ \\lim_{x \\to x_0} f(x) = A $.\n\n**Proof**: For any $ \\epsilon > 0 $, since $ \\lim_{x \\to x_0} h(x) = A $, there exists $ \\delta_1 > 0 $ such that for all $ x $ satisfying $ 0 < |x - x_0| < \\delta_1 $,\n\n$$\n|h(x) - A| < \\epsilon \\implies h(x) < A + \\epsilon.\n$$\n\nSince $ \\lim_{x \\to x_0} g(x) = A $, there exists $ \\delta_2 > 0 $ such that for all $ x $ satisfying $ 0 < |x - x_0| < \\delta_2 $,\n\n$$\n|g(x) - A| < \\epsilon \\implies A - \\epsilon < g(x).\n$$\n\nLet $ \\delta = \\min \\{ \\delta_1, \\delta_2, r \\} $. Then for $ 0 < |x - x_0| < \\delta $,\n\n$$\nA - \\epsilon < g(x) \\leq f(x) \\leq h(x) < A + \\epsilon.\n$$\n\nThus, $ \\lim_{x \\to x_0} f(x) = A $.\n\n**Q.E.D.**",
        "structure": []
    },
    {
        "id": 15,
        "domain": "IMO",
        "informal": "## Goal\n\nLet the real numbers $a, b, c, d$ satisfy the relations $a + b + c + d = 6$ and $a^2 + b^2 + c^2 + d^2 = 12$. Prove that\n\n$$\n36 \\leq 4(a^3 + b^3 + c^3 + d^3) - (a^4 + b^4 + c^4 + d^4) \\leq 48.\n$$\n\n## Proof\n\nObserve that\n\n$$\n4(a^3 + b^3 + c^3 + d^3) - (a^4 + b^4 + c^4 + d^4) = -((a - 1)^4 + (b - 1)^4 + (c - 1)^4 + (d - 1)^4) + 6(a^2 + b^2 + c^2 + d^2) - 4(a + b + c + d) + 4\n$$\n\n$$\n= -((a - 1)^4 + (b - 1)^4 + (c - 1)^4 + (d - 1)^4) + 52.\n$$\n\nNow, introducing $x = a - 1, y = b - 1, z = c - 1, t = d - 1$, we need to prove the inequalities\n\n$$\n16 \\geq x^4 + y^4 + z^4 + t^4 \\geq 4,\n$$\n\nunder the constraint\n\n$$\nx^2 + y^2 + z^2 + t^2 = (a^2 + b^2 + c^2 + d^2) - 2(a + b + c + d) + 4 = 4\n$$\n\n(we will not use the value of $x + y + z + t$ though it can be found).\n\nNow the rightmost inequality in (1) follows from the power mean inequality:\n\n$$\nx^4 + y^4 + z^4 + t^4 \\geq \\frac{(x^2 + y^2 + z^2 + t^2)^2}{4} = 4.\n$$\n\nFor the other one, expanding the brackets we note that\n\n$$\n(x^2 + y^2 + z^2 + t^2)^2 = (x^4 + y^4 + z^4 + t^4) + q,\n$$\n\nwhere $q$ is a nonnegative number, so\n\n$$\nx^4 + y^4 + z^4 + t^4 \\leq (x^2 + y^2 + z^2 + t^2)^2 = 16,\n$$\n\nand we are done.",
        "structure": []
    },
    {
        "id": 16,
        "domain": "Analysis",
        "informal": "## Goal\n\nEvery bounded sequence of real numbers contains a convergent subsequence.\n\n**Proof.** Let $E$ be the set of values of the bounded sequence $\\{x_n\\}$. If $E$ is finite, there exists a point $x \\in E$ and a sequence $n_1 < n_2 < \\cdots$ of indices such that $x_{n_1} = x_{n_2} = \\cdots = x$. The subsequence $\\{x_{n_k}\\}$ is constant and hence converges.\n\nIf $E$ is infinite, then by the Bolzano–Weierstrass principle it has a limit point $x$. Since $x$ is a limit point of $E$, one can choose $n_1 \\in \\mathbb{N}$ such that $|x_{n_1} - x| < 1$. If $n_k \\in \\mathbb{N}$ have been chosen so that $|x_{n_k} - x| < \\frac{1}{k}$, then, because $x$ is a limit point of $E$, there exists $n_{k+1} \\in \\mathbb{N}$ such that $n_k < n_{k+1}$ and $|x_{n_{k+1}} - x| < \\frac{1}{k+1}$.\n\nSince $\\lim_{k \\to \\infty} \\frac{1}{k} = 0$, the sequence $x_{n_1}, x_{n_2}, \\ldots, x_{n_k}, \\ldots$ so constructed converges to $x$.",
        "structure": []
    },
    {
        "id": 16,
        "domain": "IMO",
        "informal": "## Goal\n\nGiven six positive numbers $a, b, c, d, e, f$ such that $a < b < c < d < e < f$. Let $a + c + e = S$ and $b + d + f = T$. Prove that\n\n$$\n2ST > \\sqrt{3(S + T)} (S(bd + bf + df) + T(ac + ae + ce)).\n$$\n\n## Proof\n\nWe define also $\\sigma = ac + ce + ae, \\tau = bd + bf + df$. The idea of the solution is to interpret (1) as a natural inequality on the roots of an appropriate polynomial.\n\nActually, consider the polynomial\n\n$$\nP(x) = (b + d + f)(x - a)(x - c)(x - e) + (a + c + e)(x - b)(x - d)(x - f)\n$$\n\n$$\n= T(x^3 - Sx^2 + \\sigma x - ace) + S(x^3 - Tx^2 + \\tau x - bdf).\n$$\n\nSurely, $P$ is cubic with leading coefficient $S + T > 0$. Moreover, we have\n\n$$\nP(a) = S(a - b)(a - d)(a - f) < 0, \\quad P(c) = S(c - b)(c - d)(c - f) > 0,\n$$\n\n$$\nP(e) = S(e - b)(e - d)(e - f) < 0, \\quad P(f) = T(f - a)(f - c)(f - e) > 0.\n$$\n\nHence, each of the intervals $(a, c), (c, e), (e, f)$ contains at least one root of $P(x)$. Since there are at most three roots at all, we obtain that there is exactly one root in each interval (denote them by $\\alpha \\in (a, c), \\beta \\in (c, e), \\gamma \\in (e, f)$). Moreover, the polynomial $P$ can be factorized as\n\n$$\nP(x) = (T + S)(x - \\alpha)(x - \\beta)(x - \\gamma).\n$$\n\nEquating the coefficients in the two representations (2) and (3) of $P(x)$ provides\n\n$$\n\\alpha + \\beta + \\gamma = \\frac{2TS}{T + S}, \\quad \\alpha \\beta + \\alpha \\gamma + \\beta \\gamma = \\frac{S \\tau + T \\sigma}{T + S}.\n$$\n\nNow, since the numbers $\\alpha, \\beta, \\gamma$ are distinct, we have\n\n$$\n0 < (\\alpha - \\beta)^2 + (\\alpha - \\gamma)^2 + (\\beta - \\gamma)^2 = 2(\\alpha + \\beta + \\gamma)^2 - 6(\\alpha \\beta + \\alpha \\gamma + \\beta \\gamma),\n$$\n\nwhich implies\n\n$$\n\\frac{4S^2 T^2}{(T + S)^2} = (\\alpha + \\beta + \\gamma)^2 > 3(\\alpha \\beta + \\alpha \\gamma + \\beta \\gamma) = \\frac{3(S \\tau + T \\sigma)}{T + S},\n$$\n\nor\n\n$$\n4S^2 T^2 > 3(T + S)(T \\sigma + S \\tau),\n$$\n\nwhich is exactly what we need.",
        "structure": []
    },
    {
        "id": 17,
        "domain": "Analysis",
        "informal": "## Goal\n\nProve that the function $ f(x) = \\sin x $ is continuous and uniformly continuous on $(-\\infty, +\\infty)$.\n\n**Proof**: By the inequality\n$$\n| \\sin x' - \\sin x'' | = 2 \\left| \\cos \\frac{x' + x''}{2} \\sin \\frac{x' - x''}{2} \\right| \\leq | x' - x'' |,\n$$\n\nfor any given $ \\epsilon > 0 $, take $ \\delta = \\epsilon $. Then for any two points $ x', x'' \\in (-\\infty, +\\infty) $, as long as $ | x' - x'' | < \\delta $, it follows that\n\n$$\n| \\sin x' - \\sin x'' | \\leq | x' - x'' | < \\delta = \\epsilon.\n$$\n\nBy definition, $ \\sin x $ is uniformly continuous on $(-\\infty, +\\infty)$.\n\n**Q.E.D.**",
        "structure": []
    },
    {
        "id": 18,
        "domain": "Analysis",
        "informal": "## Goal\n\nIf $ u = g(x) $ is continuous at $ x_0 $ and $ g(x_0) = u_0 $, and $ y = f(u) $ is continuous at $ u_0 $, then the composite function $ y = f(g(x)) $ is continuous at $ x_0 $.\n\n**Proof**: For any given $ \\epsilon > 0 $, since $ \\lim_{u \\to u_0} f(u) = f(u_0) $, there exists $ \\eta > 0 $ such that when $ |u - u_0| < \\eta $, we have\n\n$$\n|f(u) - f(u_0)| < \\epsilon.\n$$\n\nFor this $ \\eta > 0 $, since $ \\lim_{x \\to x_0} g(x) = g(x_0) = u_0 $, there exists $ \\delta > 0 $ such that when $ |x - x_0| < \\delta $, we have\n\n$$\n|g(x) - u_0| < \\eta.\n$$\n\nTherefore, when $ |x - x_0| < \\delta $, we have\n\n$$\n|f(g(x)) - f(g(x_0))| = |f(g(x)) - f(u_0)| < \\epsilon,\n$$\n\nwhich means\n\n$$\n\\lim_{x \\to x_0} f(g(x)) = f(g(x_0)).\n$$\n\n**Q.E.D.**",
        "structure": []
    },
    {
        "id": 19,
        "domain": "Analysis",
        "informal": "## Goal\n\nThe function $ f(x) = x^2 $ is not uniformly continuous on $[0, +\\infty)$, but it is uniformly continuous on $[0, A]$ for any finite positive number $ A $.\n\n**Proof**: Let $ x_n' = \\sqrt{n+1} $ and $ x_n'' = \\sqrt{n} $ (where $ n = 1, 2, 3, \\ldots $). Then\n\n$$\n\\lim_{n \\to \\infty} (x_n' - x_n'') = \\lim_{n \\to \\infty} (\\sqrt{n+1} - \\sqrt{n}) = 0,\n$$\n\nbut\n\n$$\n\\lim_{n \\to \\infty} (f(x_n') - f(x_n'')) = 1.\n$$\n\nBy Example 3.4.5, we know that $ f(x) $ is not uniformly continuous on $[0, +\\infty)$.\n\nWhen the interval is restricted to $[0, A]$, we have\n\n$$\n|x'^2 - x''^2| = |(x' + x'')(x' - x'')| \\leq 2A |x' - x''|.\n$$\n\nFor any given $ \\epsilon > 0 $, we can take $ \\delta = \\frac{\\epsilon}{2A} > 0 $. For any $ x', x'' \\in [0, A] $, as long as $ |x' - x''| < \\delta $, it follows that\n\n$$\n|x'^2 - x''^2| < \\epsilon.\n$$\n\nTherefore, $ f(x) = x^2 $ is uniformly continuous on $[0, A]$.\n\n**Q.E.D.**",
        "structure": []
    },
    {
        "id": 20,
        "domain": "Analysis",
        "informal": "## Goal\n\nIf a function $ f(x) $ is continuous on the closed interval $[a, b]$, then it takes on any value between its maximum value $ M = \\max \\{ f(x) \\mid x \\in [a, b] \\} $ and its minimum value $ m = \\min \\{ f(x) \\mid x \\in [a, b] \\} $.\n\n**Proof**: By the Extreme Value Theorem, there exist $ \\xi, \\eta \\in [a, b] $ such that\n$$\nf(\\xi) = m, \\quad f(\\eta) = M.\n$$\n\nWithout loss of generality, assume $ \\xi < \\eta $. For any intermediate value $ C $ such that $ m < C < M $, consider the auxiliary function\n\n$$\n\\varphi(x) = f(x) - C.\n$$\n\nSince $ \\varphi(x) $ is continuous on the interval $[ \\xi, \\eta ]$, $ \\varphi(\\xi) = f(\\xi) - C < 0 $ and $ \\varphi(\\eta) = f(\\eta) - C > 0 $. By the Intermediate Value Theorem, there must exist a $ \\zeta \\in (\\xi, \\eta) $ such that\n\n$$\n\\varphi(\\zeta) = 0,\n$$\n\nwhich means\n\n$$\nf(\\zeta) = C.\n$$\n\n**Q.E.D.**",
        "structure": []
    },
    {
        "id": 21,
        "domain": "Analysis",
        "informal": "## Goal\n\nThe function $ f(x) = \\sin x $ is continuous on $(-\\infty, +\\infty)$.\n\n**Proof**: Let $ x_0 \\in (-\\infty, +\\infty) $ be an arbitrary point. Since\n\n$$\n|\\sin x - \\sin x_0| = 2 \\left| \\cos \\frac{x + x_0}{2} \\sin \\frac{x - x_0}{2} \\right| \\leq |x - x_0|,\n$$\n\nfor any given $ \\epsilon > 0 $, take $ \\delta = \\epsilon $. When $ |x - x_0| < \\delta $, we have\n\n$$\n|\\sin x - \\sin x_0| \\leq |x - x_0| < \\epsilon.\n$$\n\nTherefore, $ f(x) = \\sin x $ is continuous on $(-\\infty, +\\infty)$.\n\n**Q.E.D.**",
        "structure": []
    },
    {
        "id": 22,
        "domain": "Analysis",
        "informal": "## Goal\n\n$ f(x) = \\sqrt{x(1 - x)} $ is continuous on the closed interval $[0, 1]$.\n\n**Proof**: Let $ x_0 \\in (0, 1) $ be any point, and let $ \\eta = \\min \\{x_0, 1 - x_0\\} > 0 $. When $ |x - x_0| < \\eta $ and $ x \\in (0, 1) $, we have\n$$\n| \\sqrt{x(1 - x)} - \\sqrt{x_0(1 - x_0)} | = \\frac{|(1 - x) - (1 - x_0)| |x - x_0|}{\\sqrt{x(1 - x)} + \\sqrt{x_0(1 - x_0)}} < \\frac{1}{\\sqrt{x_0(1 - x_0)}} |x - x_0|.\n$$\n\nFor any given $ \\epsilon > 0 $, let $ \\delta = \\min \\{ \\eta, \\sqrt{x_0(1 - x_0)} \\epsilon \\} $. When $ |x - x_0| < \\delta $, we have\n\n$$\n| \\sqrt{x(1 - x)} - \\sqrt{x_0(1 - x_0)} | < \\frac{1}{\\sqrt{x_0(1 - x_0)}} |x - x_0| < \\epsilon.\n$$\n\nTherefore, $ f(x) = \\sqrt{x(1 - x)} $ is continuous on $ (0, 1) $.\n\nNow consider the endpoints. For any given $ \\epsilon > 0 $, let $ \\delta = \\epsilon^2 $. When $ 0 \\leq x < \\delta $,\n\n$$\n| f(x) - f(0) | \\leq \\sqrt{x} < \\epsilon.\n$$\n\nAnd when $ 1 - \\delta < x \\leq 1 $,\n\n$$\n| f(x) - f(1) | \\leq \\sqrt{1 - x} < \\epsilon.\n$$\n\nThis shows that $ f(x) $ is continuous at $ x = 0 $ from the right and at $ x = 1 $ from the left.\n\nThus, $ f(x) = \\sqrt{x(1 - x)} $ is continuous on the closed interval $[0, 1]$.\n\n**Q.E.D.**",
        "structure": []
    },
    {
        "id": 23,
        "domain": "Analysis",
        "informal": "## Goal\n\nLet $ T = \\{ x \\mid x \\in \\mathbb{Q} \\text{ and } x > 0, x^2 < 2 \\} $. Prove that $ T $ has no least upper bound in $ \\mathbb{Q} $.\n\n**Proof**: By contradiction, suppose $ T $ has a least upper bound in $ \\mathbb{Q} $. Let $ \\sup T = \\frac{n}{m} $ (where $ m, n \\in \\mathbb{N}^* $ and $ m, n $ are coprime). Then it is obvious that\n\n$$\n1 < \\left( \\frac{n}{m} \\right)^2 < 3.\n$$\n\nSince the square of a rational number cannot be 2, there are only two possible cases:\n\n1. $ 1 < \\left( \\frac{n}{m} \\right)^2 < 2 $.\n\n    Let $ \\frac{n^2}{m^2} = 2 - t $, where $ 0 < t < 1 $. Let $ r = \\frac{n}{6m} $, then it is obvious that $ n + r > 0 $, $ \\frac{n + r}{m} \\in \\mathbb{Q} $. Since\n\n    $$\n    \\frac{n^2}{3m^2} t < \\frac{2}{3}, \\quad \\frac{2n}{m} - r < \\frac{n}{18} t,\n    $$\n\n    we get\n\n    $$\n    \\left( \\frac{n + r}{m} \\right)^2 = \\frac{n^2}{m^2} + \\frac{2n}{m} r + r^2 = 2 - t + \\frac{2n}{m} r + r^2 < 2.\n    $$\n\n    This implies $ \\frac{n + r}{m} \\in T $, contradicting the assumption that $ \\frac{n}{m} $ is the least upper bound of $ T $.\n\n2. $ 2 < \\left( \\frac{n}{m} \\right)^2 < 3 $.\n\n    Let $ \\frac{n^2}{m^2} = 2 + t $, where $ 0 < t < 1 $. Let $ r = \\frac{n}{6m} $, then it is obvious that $ n - r > 0 $, $ \\frac{n - r}{m} \\in \\mathbb{Q} $. Since\n\n    $$\n    \\frac{2n}{m} - r < \\frac{n^2}{3m^2} t < 1,\n    $$\n\n    we get\n\n    $$\n    \\left( \\frac{n - r}{m} \\right)^2 = \\frac{n^2}{m^2} - \\frac{2n}{m} r + r^2 = 2 + t - \\frac{2n}{m} r + r^2 > 2.\n    $$\n\n    This implies $ \\frac{n - r}{m} \\in T $, contradicting the assumption that $ \\frac{n}{m} $ is the least upper bound of $ T $.\n\nTherefore, we conclude that $ T $ has no least upper bound in $ \\mathbb{Q} $.\n\n**Q.E.D.**",
        "structure": []
    },
    {
        "id": 24,
        "domain": "Analysis",
        "informal": "## Goal\n\nProve that the limit of the sequence $ \\left\\{ \\frac{n}{n+3} \\right\\} $ is 1.\n\n**Proof**: For any given $ \\epsilon > 0 $, we need to show that\n$$\n\\left| \\frac{n}{n+3} - 1 \\right| < \\epsilon.\n$$\n\nWe have\n\n$$\n\\left| \\frac{n}{n+3} - 1 \\right| = \\left| \\frac{n - (n+3)}{n+3} \\right| = \\left| \\frac{-3}{n+3} \\right| = \\frac{3}{n+3}.\n$$\n\nTo make $ \\frac{3}{n+3} < \\epsilon $, we need $ n > \\frac{3}{\\epsilon} - 3 $. We can choose any integer $ N $ greater than $ \\frac{3}{\\epsilon} - 3 $, for example, $ N = \\left\\lceil \\frac{3}{\\epsilon} \\right\\rceil - 3 $, where $ \\left\\lceil x \\right\\rceil $ denotes the ceiling of $ x $.\n\nTherefore, for $ n > N $, we have $ n > \\frac{3}{\\epsilon} - 3 $, and thus\n\n$$\n\\left| \\frac{n}{n+3} - 1 \\right| = \\frac{3}{n+3} < \\epsilon.\n$$\n\nHence, the limit of the sequence $ \\left\\{ \\frac{n}{n+3} \\right\\} $ is 1.\n\n**Q.E.D.**",
        "structure": []
    },
    {
        "id": 25,
        "domain": "Analysis",
        "informal": "## Goal\n\nIf $ \\lim_{x \\to x_0} f(x) = A $, $ \\lim_{x \\to x_0} g(x) = B $, and $ A > B $, then there exists $ \\delta > 0 $ such that for $ 0 < |x - x_0| < \\delta $, we have $ f(x) > g(x) $.\n\n**Proof**: Let $ \\epsilon_0 = \\frac{A - B}{2} > 0 $. Since $ \\lim_{x \\to x_0} f(x) = A $, there exists $ \\delta_1 > 0 $ such that for all $ x $ satisfying $ 0 < |x - x_0| < \\delta_1 $,\n\n$$\n|f(x) - A| < \\epsilon_0 \\implies f(x) > \\frac{A + B}{2}.\n$$\n\nSince $ \\lim_{x \\to x_0} g(x) = B $, there exists $ \\delta_2 > 0 $ such that for all $ x $ satisfying $ 0 < |x - x_0| < \\delta_2 $,\n\n$$\n|g(x) - B| < \\epsilon_0 \\implies g(x) < \\frac{A + B}{2}.\n$$\n\nLet $ \\delta = \\min \\{ \\delta_1, \\delta_2 \\} $. Then for $ 0 < |x - x_0| < \\delta $,\n\n$$\ng(x) < \\frac{A + B}{2} < f(x).\n$$\n\nTherefore, $ f(x) > g(x) $ for $ 0 < |x - x_0| < \\delta $.\n\n**Q.E.D.**",
        "structure": []
    },
    {
        "id": 26,
        "domain": "Analysis",
        "informal": "**Example 1** Find the general solution of the differential equation $\\frac{dy}{dx} = 2xy$.\n\n**Solution** The equation is separable. Separating the variables gives $\\frac{dy}{y} = 2x dx.$\n\nIntegrating both sides $\\int \\frac{dy}{y} = \\int 2x dx,$\n\nwe get $\\ln |y| = x^2 + C_1,$\n\nso that $y = \\pm e^{x^2 + C_1} = \\pm e^{C_1} e^{x^2}.$\n\nSince $\\pm e^{C_1}$ is an arbitrary nonzero constant, and $y=0$ is also a solution, we obtain the general solution $y = C e^{x^2}.$",
        "structure": []
    }
]