import os
import sys
import shutil
import argparse
import time
from dotenv import load_dotenv
from openai import OpenAI

# å¯¼å…¥æˆ‘ä»¬çš„è¾…åŠ©æ¨¡å—
from check_coverage import get_next_cursor
from sketch_revise import run_auto_fix, extract_code_block

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
API_KEY = os.getenv("OPENAI_API_KEY")
BASE_URL = os.getenv("OPENAI_BASE_URL")
MODEL_NAME = os.getenv("MODEL_NAME")

client = OpenAI(api_key=API_KEY, base_url=BASE_URL)

# é…ç½®å‚æ•°
MAX_OUTER_LOOPS = 15  # æœ€å¤§å¤–éƒ¨ç”Ÿæˆè¿­ä»£æ¬¡æ•°
GEN_PROMPT_PATH = "prompts/structure_to_lean_sketch_single_round_prompt.md"
WORKING_DIR = "lean_project/LeanProject"
TEMP_OUTPUT_FILE = "llm_output.lean"   # LLM ç”Ÿæˆçš„ä¸´æ—¶æ–‡ä»¶ (ç”¨äº revise)
SAVE_FILE = "llm_save.lean"            # é€šè¿‡æ£€æŸ¥çš„ç´¯ç§¯æ–‡ä»¶
LOG_DIR = "logs_sketch"

def ensure_dir(path):
    if not os.path.exists(path):
        os.makedirs(path)

def read_file(path):
    if os.path.exists(path):
        with open(path, 'r', encoding='utf-8') as f:
            return f.read()
    return ""

def write_file(path, content):
    with open(path, 'w', encoding='utf-8') as f:
        f.write(content)

def call_gen_llm(json_content, previous_sketch, cursor):
    """
    è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆä¸‹ä¸€æ­¥çš„ Sketch
    """
    print(f"\n[Gen] Calling LLM to extend sketch at Cursor {cursor}...")
    
    prompt_template = read_file(GEN_PROMPT_PATH)
    if not prompt_template:
        print(f"Error: Prompt template not found at {GEN_PROMPT_PATH}")
        sys.exit(1)

    # æ›¿æ¢ Prompt ä¸­çš„å ä½ç¬¦
    prompt = prompt_template.replace("{Structure_Input}", json_content)
    
    if not previous_sketch.strip():
        previous_sketch_input = "" # Empty for Cursor 1
    else:
        previous_sketch_input = previous_sketch

    prompt = prompt.replace("{Previous_Sketch_Input}", previous_sketch_input)
    prompt = prompt.replace("{Cursor_Input}", str(cursor))

    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"Error calling LLM: {e}")
        return ""

def main():
    parser = argparse.ArgumentParser(description="Iteratively generate Lean sketch from JSON structure.")
    parser.add_argument("json_path", help="Path to the JSON structure file")
    parser.add_argument("output_lean_path", help="Final output path for the Lean file")
    
    args = parser.parse_args()
    
    json_path = args.json_path
    final_output_path = args.output_lean_path

    # 1. åˆå§‹åŒ–è·¯å¾„
    ensure_dir(LOG_DIR)
    
    llm_save_path = os.path.join(WORKING_DIR, SAVE_FILE)
    llm_output_path = os.path.join(WORKING_DIR, TEMP_OUTPUT_FILE)
    revise_log_path = os.path.join(LOG_DIR, "latest_revise_error.md")

    # 2. æ¸…ç©º llm_save.lean (åˆå§‹åŒ–)
    print(f"Initializing {llm_save_path}...")
    ensure_dir(os.path.dirname(llm_save_path))
    write_file(llm_save_path, "") 

    # è¯»å– JSON å†…å®¹
    json_content = read_file(json_path)
    if not json_content:
        print(f"Error: JSON file empty or not found at {json_path}")
        sys.exit(1)

    # ================= ä¸»è¿­ä»£å¾ªç¯ =================
    for iteration in range(1, MAX_OUTER_LOOPS + 1):
        print(f"\n========================================")
        print(f"  Outer Loop Iteration {iteration}/{MAX_OUTER_LOOPS}")
        print(f"========================================")

        # 3. è¯»å–å½“å‰å·²æœ‰çš„ Sketch
        current_sketch = read_file(llm_save_path)
        
        # 4. è®¡ç®— Cursor
        cursor = get_next_cursor(json_path, current_sketch)
        
        if cursor is None:
            print(f"ğŸ‰ Generation Complete! All JSON IDs covered.")
            break
        
        print(f"â„¹ï¸ Current Cursor: {cursor}")

        # 5. è°ƒç”¨ LLM ç”Ÿæˆ/ç»­å†™
        llm_raw_resp = call_gen_llm(json_content, current_sketch, cursor)
        if not llm_raw_resp:
            print("Error: LLM returned empty response. Stopping.")
            break

        new_sketch_code = extract_code_block(llm_raw_resp)
        if not new_sketch_code:
            print("Warning: No code block found in LLM response.")
            new_sketch_code = llm_raw_resp

        # å°†ç”Ÿæˆçš„ä»£ç å†™å…¥ llm_output.lean ä¾› sketch_revise æ£€æŸ¥
        write_file(llm_output_path, new_sketch_code)
        
        # ä¿å­˜åŸå§‹ç”Ÿæˆä»£ç  log
        log_gen_path = os.path.join(LOG_DIR, f"iter_{iteration}_cursor_{cursor}_gen_raw.lean")
        write_file(log_gen_path, new_sketch_code)

        # 6. è°ƒç”¨ sketch_revise è¿›è¡Œæ£€æŸ¥å’Œè‡ªåŠ¨ä¿®å¤
        print(f"[Process] Verifying and Revising...")
        revise_success = run_auto_fix(llm_output_path, revise_log_path)

        if not revise_success:
            print(f"âŒ Iteration {iteration} Failed: Revision could not fix the code.")
            print("Stopping iteration.")
            break
        
        print(f"âœ… Iteration {iteration} Success: Code verified and saved.")
        
        # è¯»å–åˆšåˆšä¿å­˜çš„ llm_save.lean è¿›è¡Œæ—¥å¿—è®°å½•
        saved_code = read_file(llm_save_path)
        log_save_path = os.path.join(LOG_DIR, f"iter_{iteration}_cursor_{cursor}_verified.lean")
        write_file(log_save_path, saved_code)

    # ================= ç»“æŸå¤„ç† =================
    print(f"\n========================================")
    
    if os.path.exists(llm_save_path):
        print(f"Copying result to final output: {final_output_path}")
        shutil.copy2(llm_save_path, final_output_path)
    else:
        print("Error: No result file generated.")

if __name__ == "__main__":
    main()